\input{head}



\title{Современное программирование \\ 
Конспект по алгебре, 2019-2020}
\date{}


\begin{document}

\tableofcontents

\chapter{Введение}




\section{Уравнения и числа}

Современные задачи алгебры очень разнообразны, однако корни всех алгебраических задач стоит искать в исследовании различных уравнений.


Попробуем проиллюстрировать этот тезис на примере простейшего уравнения в целых числах $a=bx$.

\dfn[Делимость] Пусть $a$ и $b$ -- два целых числа. Будем говорить, что $a$ делится на $b$, если уравнение $a=bx$ разрешимо в целых числах. Будем обозначать это как $a\di b$ 
\edfn

Как узнать, делится ли одно число на другое? Для этого стоит поделить с остатком.

\thrm[О делении с остатком] Пусть $a$ и $b$ --- два целых числа, причём $b\neq 0$. Тогда существуют единственные целые
числа $q$ и $r$, такие что
$$a=bq+r \text{ и } 0\leq r< |b|.$$
\ethrm
\proof Докажем существование. Будем считать, что $a\geq 0$. Индукция по $a$. Если $a<|b|$  то возьмём $q=0$, $r=a$. Шаг. Вычтем из $a$ модуль $b$. Обозначим разность $a'=a-|b|$. По предположению $a'=bq'+r$. Тогда $a=b(q'\pm 1)+r$, где $\pm$ ставится в зависимости от знака $b$. 

Для отрицательных $a$. $|a|=bq+r$. Если $r=0$, то $a=b(-q)$.  Иначе $a=b(-q)+(-r)=b(-q\pm 1)+(|b|-r)$.

Докажем единственность. Пусть $a=bq+r=bq'+r'$. Тогда $0=b(q-q')+(r-r')$. Заметим, что $|r-r'|<|b|$. Тогда равенство нулю возможно только если $q-q'=0$. Но тогда $r-r'=0$.
\endproof




\dfn[Общий делитель и наибольший общий делитель] Пусть $a$ и $b$ -- два целых числа. Тогда число $d$ называется общим делителем $a$ и $b$, если $a\di  d $ и $b \di d$.
Общий делитель $d$ называется наибольшим общим делителем, если $d\geq 0$  и для любого другого делителя $d'$ верно, что $d \di d'$
\edfn

\rm Если два натуральных числа делятся друг на друга, то это значит, что они равны. Из этого следует, что если наибольший общий делитель есть, то он единственен.
\erm

\lm Пусть $a$ и $b$ -- два целых числа. Тогда существует такое натуральное число $d$, что множество чисел вида $ax+by$ совпадает с множеством чисел, кратных $d$ (будем обозначать последнее как $d\mb Z$). Более того, $d$ является наибольшим общим делителем чисел $a$ и $b$.
\elm 
\proof  Пусть $a$ и $b$ отличны от нуля. Рассмотрим множество $(a,b)=\{z\in \mb Z\,|\, \exists\, x,y \in \mb Z, \text{ что } z=ax+by\}$ элементов вида $ax+by$. Или менее формально $(a,b)=\{ax+by\,|\, x,y \in \mb Z\}$. Для всякого $d$ так же определим множество $(d)=d\mb Z=\{dx\,|\,x\in \mb Z\}$ -- множество элементов, кратных $d$. Мы хотим найти такой $d$, что $(d)=(a,b)$. 

В случае $a=0=b$ подходит $d=0$ и ничего другого. Иначе, рассмотрим наименьший положительный $d$ (такой теперь есть), который лежит в $A$. Покажем, что тогда все элементы $A$ делятся на $d$. Действительно, пусть $ax+by=c\in A$. Поделим $c$ на $d$ с остатком
$$c=dq+r.$$
Заметим, что тогда элемент $r$ так же лежит в $A$, но его модуль меньше $d$. Так как $d$ наименьший положительный элемент $A$ это значит, что $r$ равен нулю. Что и требовалось. 


Обратно, любой элемент, делящийся на $d=ax+by$ сам имеет такой вид.
Осталось показать, что $d$ наибольший общий делитель. Для этого заметим, что элементы $a$ и $b$ сами лежат в $A$ и, следовательно, делятся на $d$. С другой стороны, так как $d=ax+by$, то любой общий делитель  $a$  и $b$ есть делитель $d$. 
\endproof

\crl Для любых двух целых чисел $a,b$ существует $\Nod(a,b)$. $\Nod(a,b)$ единственен и является наименьшим положительным элементом в множестве $(a,b)$ и, таким образом, однозначно этим множеством определяется. Поэтому мы будем вместо $\Nod(a,b)$ писать просто $(a,b)$, если это не вызовет путаницы.
\ecrl



\crl[Линейное разложение НОД-а] Пусть $a$ и $b$ --- два целых числа. Пусть $d$ --- наибольший общий делитель $a$ и $b$. Тогда существуют такие целые числа $x$ и $y$, что
$d=ax+by$.
\ecrl

Думаю, что вам всем хорошо известно следующее определение:

\dfn Два числа $n,m$ называются взаимно простыми, если $(n,m)=1$.
\edfn

Покажем полезное для нас свойство взаимно простых чисел.

\lm Пусть $n$ -- некоторое целое число, $a$ и $b$ -- взаимно простые целые числа. Тогда если $na \di b$, то $n\di b$.
\elm
\proof Наибольший общий делитель $a$  и $b$ равен $1$. Рассмотрим его линейное разложение 
$$1=ax+by.$$
Домножим это разложение на $n$. Получим $$n=nax+nby.$$
Заметим, что оба слагаемых делятся на $b$ -- первое по предположению теоремы, второе -- просто содержит $b$. Значит $n \di b$, что и требовалось.
\endproof

Разберёмся подробнее с решением уравнения $ax+by=c$. Первое: как искать какое-то решение? Второе: как выглядят все остальные решения?\\
1) Очевидно, что если $(d)=(a,b)$, то для того, чтобы было решение необходимо и достаточно $c\di d$. А именно если есть разложение $d=ax+by$. Тогда есть решение $c= \frac{c}{d}(ax+by)$. Опишем, как выглядят все остальные решения.\\
2) Пусть есть два решения уравнения $ax+by=c$ и $ax'+by'=c$. Возьмём разность $a(x-x')+b(y-y')=0$, то есть 
$$\tfrac{a}{d}(x-x')=-\tfrac{b}{d}(y-y').$$
Тогда по привычному нам свойству $(x-x')\di \frac{b}{d}$. Пусть $(x-x')=t\frac{b}{d}$. Тогда $(y-y')=-t\frac{a}{d}$. Таким образом, пододвинуть первую координату мы можем только на кратное $\frac{b}{d}$ и это определяет однозначно вторую координату. Понятно, что новые $x',y'$ обязательно решения.\\






Единственный не рассмотренный подробно вопрос, касающийся решения  линейного диофантового уравнения $ax+by=c$ состоит в том, как вычислить наибольший общий делитель двух чисел и его линейное разложение.

Рассмотрим пару чисел $a,b\in \mb Z$. Поделим $a$ на $b$ с остатком $a=bq_1+r_1$. Тогда $$(d)=(a,b)=(a-bq,b)=(b,r_1).$$
продолжая так далее имеем соотношение
$$r_{i-1}=r_iq_{i+1}+r_{i+1}  \text{ и $(a,b)=(r_i,r_{i+1})$}.$$
Так как начиная со второго номера все $r_i$ положительные и всегда убывают по модулю, то в конце концов получаем, что $r_{k+1}=0$.
В этот момент  $\Nod(a,b)=\Nod(r_k,r_{k+1})=r_k$. Как вычислить линейное разложение? В самом начале имеем $a=1\cdot a+0\cdot b$ и $b=0\cdot a+ 1\cdot b$. По индукции пусть есть разложение для $r_{i-1}=ax+by$ и $r_i=ax'+by'$. Тогда $$r_{i+1}=r_{i-1}-q_ir_{i}=a(x-q_ix')+b(y-q_iy').$$
Это называется расширенным алгоритмом Евклида.

Сколько делений с остатком может потребоваться для того, чтобы вычислить наибольший общий делитель?
Понятно, что чем больше неполные частные в делениях с остатком, тем меньше делений с остатком понадобится. Кандидатом на наихудшее число операций в алгоритме Евклида, таким образом, являются числа Фибоначчи:
$$f_{n+1}=f_n+f_{n-1}, \text{ и $f_0=0$, $f_1=1$}.$$

\lm Пусть для пары целых чисел $a>b>0$ в алгоритме Евклида происходит $k$ делений с остатком. Тогда $a\geq f_{k+2}$, а $b\geq f_{k+1}$? 
\elm
\proof Пусть $r_{i-2}=q_ir_{i-1}+r_i$, $a=r_{-1}$, $b=r_0$ и $r_k=0$. Покажем, что $f_{i+1}\leq r_{k-i}$. По индукции. $i=1,2$ верно. Далее
$$r_{k-i}=q_{k-i+2}r_{k-i+1}+r_{k-i+2}\geq r_{k-i+1}+r_{k-i+2}\geq f_i+f_{i-1}=f_{i+1}.$$
\endproof

Оценим рост чисел Фибоначчи:
\lm При $n>1$ имеет место неравенство $f_n\geq \varphi^{n-2}$, где $\varphi=\frac{1+\sqrt{5}}{2}$ является корнем уравнения $x^2-x-1=0$. 
\elm
\proof При $n=1,2$ --- верно. Для $n+1$ имеем неравенство
 $$f_{n+1}=f_n+f_{n-1}\geq \varphi^{n-2}+\varphi^{n-3}=\ffi^{n-3}(\ffi+1)=\ffi^{n-1}.$$
\endproof

\thrm Пусть $b<a\leq N$ --- натуральные числа. Тогда число делений с остатком в алгоритме Евклида при нахождении $\Nod(a,b)$  не превосходит $\left\lfloor \log_{\ffi} N\right\rfloor$. 
\ethrm
\proof Пусть, как и раньше, $k$ -- это количество делений с остатком для пары $a,b$. Тогда
$$N\geq a=r_{-1}\geq f_{k+2} \geq \ffi^k.$$ 
Получаем $k\leq \left\lfloor \log_{\ffi} N\right\rfloor.$ 
\endproof

\rm Можно улучшить алгоритм допуская отрицательные остатки.
\erm

\fct Битовая сложность умножения двух чисел такая же как и у операции деления с остатком (естественно, с точностью до мультипликативной константы).
\efct

\rm В этом случае, битовая сложность алгоритма Евклида для чисел длины $n$ бит можно оценить как $ C n^3$, если использовать обычный алгоритм умножения и $C n^2\log n \log \log n$ при использовании алгоритма умножения Шёнхаге-Штрассена. Если присмотреться, то можно увидеть, что это не оптимальная оценка, так как в случае маленьких неполных частных деление с остатком стоит не так много. Это приводит к оценке $C n (\log n)^2\log\log n$.
\erm




\section{Сравнения по модулю и их свойства} 

Рассмотрим уравнение $x^2+251x+1203$. Имеет ли оно решение в целых числах? Если действовать в лоб, то надо потратить много усилий. Однако, можно заметить, что если $x$ чётное, то результат его подстановки будет числом нечётным и, следовательно, не может быть равным нулю. Аналогично, если $x$ -- нечётное число. Таким образом, получаем, что целочисленных решений это уравнение не имеет. 

Смотреть на чётность -- это всё равно что смотреть на остаток от деления на $2$. Наша задача для произвольного числа $n$ извлечь информацию про целочисленное уравнение посмотрев на  остатки его коэффициентов по модулю $n$.

\dfn Пусть $a,b,n$ -- целые числа . Будем говорить, что число числа $a$ и $b$ сравнимы по модулю $n$, если $a-b \di n$. Будем записывать это как
$$a\equiv b \mod n \text{ или для краткости } a\equiv b \,\,(n)$$  
\edfn

Сформулируем основные свойства сравнений. Пусть $n, a,b,c, d$ --  некоторые целые числа. Тогда:\\
0) $a\equiv a \mod n$, если $a\equiv b \mod n$, то $b \equiv a \mod n$\\
1) $a\equiv b \mod n$ $b\equiv c \mod n$ тогда $a\equiv c$.\\
2) Если $a\equiv b \mod n$, и $c\equiv d \mod n$, то $a+c \equiv b+d \mod n$ и $ac\equiv bd \mod n$.\\
3) В частности, если $a\equiv b \mod  n$, то $ac\equiv bc \mod n$.\\
4) $a\equiv b \mod n$ тогда и только тогда, когда их остатки от деления на $n$ одинаковы.\\
5) Если $a\equiv b \mod n$, то $a\equiv b \mod (-n)$. Таким образом, всегда можно считать, что $n$ -- неотрицательное число. Более того, при $n=0,1$ понятие сравнения становится очень простым. При $n=0$ два числа сравнимы тогда и только тогда, когда они равны, а при $n=1$ любые два целых числа сравнимы.\\


На языке сравнений удобно формулировать разные ограничительные условия для целых чисел. Например, линейным сравнением относительно $x$ называется условие вида 
$$ax\equiv b \mod n .$$
Здесь $a,b,n$ -- целые числа. Решить такое сравнение означает найти все целые $x$ ему удовлетворяющие. Посмотрим как это можно сделать. Для этого распишем условие $ax\equiv b \mod n$. Такое сравнение выполнено, если существует $y$, что $ax=b+ny$, то есть если $ax-ny=b$. Таким образом, число $x$ должно быть первой компонентой решения линейного диофантового уравнения. 

Отсюда сразу сделаем вывод: решение линейного сравнения $ax\equiv b \mod n$ существует тогда и только тогда, когда $b \di (a,n)$. В частности, оно всегда разрешимо, если $a,n$ -- взаимно простые числа. 

Далее, если $x_0$ -- некоторое решение сравнения, то все его решения имеют вид $x=x_0+t\frac{n}{d}$. Осталось заметить, что само это условие удобно записывается на языке сравнений
$$x\equiv x_0 \mod \frac{n}{d}.$$ 
Мы будем говорить, что решение линейного сравнения единственно по модулю $\frac{n}{d}$. В частности, это означает, что у сравнения есть единственное решение в промежутке от $0$ до $\frac{n}{d}-1$.



\section{Немного теории множеств}

Нам понадобилось понятие отображения. Для этого нам необходимо понятие декартового произведения множеств.

\dfn Пусть $A$ и $B$ -- два множества. Тогда их декартовым произведением $A\times B$ называется множество всех пар $(a,b)$, где первая координата из $A$, а вторая из $B$. Это можно записать так:
$$ A \times B = \{ (a,b) \, | \, a \in A \text{ и } b \in B \}.$$
\edfn

Что такое отображение между множествами $A$  и $B$? Это некоторое правило соответствия по элементу $A$ дающее элемент $B$. Но как это выразить на самом языке теории множеств? Для этого заметим, что отображение $f\colon A \to B$ должно кодироваться его графиком, то есть множеством пар вида $(x,f(x))$ $x\in A$ и $f(x)\in B$, то есть подмножеством декартового произведения. Это приводит нас к формальному определению.


\dfn Отображение $f \colon A \to B$ из множества $A$ в множество $B$ это тройка данных $(A,B,\Gamma_f)$, где $\Gamma_f \subseteq A \times B$ обладает свойством, что для всех $a \in A$ существует единственное $b \in B$, что $(a,b) \in \Gamma_f$ (в дальнейшем будем просто писать $b=f(a)$). 
\edfn

Конечно для задания отображений мы будем использовать формулы, а не графики. Например, $f(x)=x^2$ задаст нам отображение $\mb R \to \mb R$. Однако, отдельное задание множеств $A$ и $B$ -- области определения и прибытия, очень важно. Одна и та же формула $f(x)=x^2$ может задавать совершенно разные отображения. Например, из $\mb N$ в $\mb N$, или из $\mb Z \to \mb R$.

Вот примеры интересных нам свойств, которые очень чувствительны к тому, какие $A$ и $B$ выбраны:

\dfn Отображение $f \colon A \to B$ называется инъективным, если $\forall x_1,x_2 \in A$ таких, что $f(x_1)=f(x_2)$ верно, что $x_1=x_2$.
\edfn

Отображение $f(x)=x^2$ инъективно, если посмотреть на него, как на отображение $\mb N \to \mb N$, но не инъективно как отображение $\mb Z \to \mb R$.

\dfn Отображение $f \colon A \to B$ называется сюръективным, если $\forall y\in B$ существует $x \in A$, что $f(x)=y$. Такой элемент $x\in B$ называется прообразом элемента $y$.
\edfn

Отображение $f(x)=x^2$ -- сюръективно как отображение $\mb R \to \mb R_{\geq 0}$, но не сюръективно как отображение $\mb R \to \mb R$.

\dfn Отображение $f \colon A \to B$ называется биективным, если оно инъективно и сюръективно.
\edfn



Из любого множества в себя есть биективное отображение -- тождественное.



\dfn Пусть $A$ множество. Определим отображение $id_A \colon A \to A$ по правилу $id_A(x)=x$. Такое отображение называется тождественным.
\edfn

Ещё пример биективного отображения: $f \colon \mb R \to \mb R$ вида $f(x)=2x$, ну или $f(x)=x^3$.


В дальнейшем нам понадобится определения обратного отображения и, следовательно, определение композиции отображений.

\dfn Пусть есть три множества $A,B,C$ и два отображения между ними -- $f\colon A \to B$ и $g\colon B \to C$. Определим отображение $g \circ f \colon A \to C$, по правилу $g\circ f(x)= g(f(x))$ для всех $x \in A$. 
\edfn

\dfn Пусть $f \colon A \to B$. Тогда обратным отображением к $f$ называется такое $f^{-1}\colon B \to A$, что $f \circ f^{-1}= id_B$ и $f^{-1}\circ f= id_A$. 
\edfn



\thrm Отображение $f \colon A \to B$ биективно тогда и только тогда, когда оно обратимо. Если обратное отображение есть, то оно единственно.
\ethrm

\crl Если $f \colon A \to B$ биекция, то $f^{-1}\colon B \to A$ тоже биекция.
\ecrl

\thrm[Принцип Дирихле] Пусть $X$ и $Y$ -- два конечных множества с одинаковым числом элементов. Тогда отображение  $f\colon X \to Y$ инъективно тогда и только тогда, когда оно сюръективно.
\ethrm




\section{Бинарные отношения}

Часто встречается такая ситуация, когда некоторое свойство $R(x,y)$ зависит от $x,y$ -- элементов одинакового типа (одного множества $X$). В такой ситуации принято говорить, что на $X$ задано бинарное отношение $R$. Если свойство $R$ истинно на паре $x,y$, то пишут $x R y$. Но что же такое это свойство $R$ с точки зрения теории множеств? Вот формальный ответ:

\dfn Бинарное отношение на множестве $X$ это подмножество $R \subseteq X\times X$.
\edfn
Действительно, если взять пару $x,y$, то $x R y$ истинно, если $(x,y)\in R$. Понятно, что таким образом кодируется абсолютно любое свойство $R$.

\exm \\
1) Отношение сравнимости $a\equiv b \mod n$ на $\mb Z$ по фиксированному модулю $n$.\\
2) Отношение дружить друг с другом на множестве всех людей.\\
3) Отношение делимости на множестве натуральных чисел.\\


Посмотрим на другое определение:

\dfn Ориентированным графом (возможно с петлями) $G$ называется пара множеств $V$, $E$, где $E \subseteq V \times V$. Множество $V$ называется множеством вершин, а множество $E$ -- множеством рёбер.
\edfn



Видно, что оба эти определения идентичны, точнее задать отношение на множестве $X$ это тоже самое, что задать ориентированный граф с множеством вершин $X$. Это замечание позволяет связать с каждым отношением $R$ на $X$ картинку -- элементы множества $X$ обозначаются точками, и две точки $x$ и $y$ соединяются стрелкой $x \to y$, если $x R y$.



\dfn Пусть  $R$ -- отношение на $X$. Тогда\\
1) Если $\forall a \in X$ верно $a R a$, то $R$ называется рефлексивным. Рефлексивность означает, что на картинке у каждой вершины будет нарисована петелька. Следовательно для заведомо рефлексивных отношений петельки можно не рисовать.\\
2) Если $\forall a,b \in X$ верно, что если $a R b$, то $b R a$, то отношение $R$ называется симметричным. На картинке это означает, что все стрелочки двусторонние. В этом случае вместо стрелок рисуют просто отрезки.\\
3) Если $\forall a,b,c \in X$ верно, что в случае $aR b $ и $b R c$ выполнено, что $a R c$, то отношение $R$ называется транзитивным.\\
4) Если $\forall a,b \in X$, где $a\neq b$, верно, что если $a R b$, то $b \notin R a$, то отношение $R$ называется антисимметричным. На картинке это означает, что все стрелочки направлены в одну сторону.\\
\edfn

Это довольно Теперь сформулируем два наиболее осмысленных типа отношений, которые будут встречаться нам в дальнейшем. Первое из них отвечает интуитивному пониманию, что значит упорядочить элементы множества



\dfn Пусть $X$ --- множество, $\leq$ --- бинарное отношение на нём. Будем говорить, что $\leq$ --- отношение порядка, если выполнены свойства\\
1)  $\forall a \in X$ верно $a \leq a$. Это свойство называется рефлексивностью.\\
2)  $\forall a,b,c \in X$ верно, что в случае $a \leq b $ и $b \leq c$ выполнено, что $a \leq c$. Это свойство называется транзитивностью.\\
3)  $\forall a,b \in X$, если одновременно $a\leq b$ и $a \leq b$, то $a=b$. Это свойство называется антисимметричностью.\\
Множество $X$ вместе с таким отношением $\leq$ называется частично упорядоченным множеством.
\edfn

\exm \\
1) Множества $\mb Z$, $\mb Q$, $\mb R$ относительно обычного порядка.\\
2) Множество натуральных чисел, относительно делимости $a | b$.\\
3) Множество яблок, относительно такого порядка: яблоко $x$ меньше или равно яблока $y$, если площадь поверхности яблока $x$ занятая красным цветом, $redarea(x)$ строго меньше $redarea(y)$ или, если  $x=y$. Нельзя поставить здесь вместе $<$ отношение $\leq$, так как тогда нарушится симметричность. Но тогда для рефлексивности надо добавить условие про равенство.\\
4) Если $X$ -- частично упорядоченное множество, а $A \subseteq X$, то на $A$ можно ограничить порядок, говоря, что $a\leq b$, если это отношение было выполнено для них, как для элементов $X$.\\
5) Множество $2^X=\{A\,|\, A \subseteq X\}$ всех подмножеств множества $X$ относительно включения $\subseteq$.\\
6) Множество $\mb N \times \mb N$ относительно порядка $(a,b) \leq (c,d)$, если $a\leq c$ и $b\leq d$.\\
7) Множество $\mb N \times \mb N$ относительно порядка $(a,b) \leq_{lex} (c,d)$, если $a<c$ или $a=c$ и при этом $b\leq d$\\



Вот пример полезного определения (которого не было на лекции) определяемого через отношение порядка.


\dfn[Наибольший и наименьший элемент] Пусть $X$ -- частично упорядоченное множество. Элемент $x\in X$ называется наименьшим, если $\forall y \in X$  выполнено, что $x\leq y$. Аналогично определяется наибольший  элемент.
\edfn









Часто бывают ситуации, когда с объектами обладающими одним и тем же свойством хочется работать как с одним. Например, если некоторая конструкция применима для всех зданий с одинаковой высотой, то стоит объявить такие здания эквивалентными, если речь идёт об этой конструкции. Аксиоматизируем, что бы мы хотели от такого отношения:

\dfn Пусть $X$ --- множество, $\sim$ --- бинарное отношение на нём. Будем говорить, что $\sim$ --- отношение эквивалентности, если\\
1) $\forall a \in X$ верно $a \sim a$.\\
2) $\forall a,b \in X$, если $a\sim b$ то и $b \sim a$. Это свойство отношения называется симметричностью.
3)  $\forall a,b,c \in X$ верно, что в случае $a \sim b $ и $b \sim c$ выполнено, что $a \sim c$.
\edfn




\exm \\
1) Отношение сравнимости по модулю $n$, то есть $a\equiv b \mod n $, является отношением эквивалентности на $\mb Z$.\\
2) Множество пар вещественных чисел $\mb R \times \mb R$ обладает, например, таким отношением эквивалентности $(x,y)\sim (u,v)$, если $x-y=u-v$.\\
3) Множество прямых на плоскости и отношение быть параллельным.\\
4) Пусть $G=(V,E)$ -- ориентированный граф с петлями. Тогда можно задать отношение эквивалентности на множестве вершин -- быть соединённым путём из неориентированных рёбер $G$. По-другому это называют "лежать в одной компоненте связности". Так же на это определение можно посмотреть так: мы научились по каждому бинарному отношению строить отношение эквивалентности. Более того -- это наименьшее отношение эквивалентности, которое содержит исходное.\\
5) Множество домов и отношение быть одинаковой высоты.\\
6) А вот отношение "Дружить" не является отношением эквивалентности. Если на симметричность ещё можно надеяться, то с транзитивностью часто бывают проблемы. \\





\dfn Пусть $\sim $ отношение эквивалентности на $X$. Тогда классом эквивалентности элемента $x\in X$ называется множество $\ovl{x}=\{y \in X\,|y\sim x\}$. 
\edfn

\rm Все элементы в классе $\ovl{x}$ эквивалентны друг другу.
\erm 

\fct Всякий элемент $x\in X$ лежит в каком-то классе эквивалентности.  Иными словами, объединение всех классов эквивалентности есть всё множество $X$.  Два класса эквивалентности либо не пересекаются либо совпадают. В такой ситуации будем говорить, что множество $X$ раскладывается в дизъюнктное объединение, в данном случае, классов эквивалентности.
\efct
\proof Первое утверждение тривиально -- всякий элемент лежит в своём классе эквивалентности. Покажем, что если два класс $\ovl{x}$  и $\ovl{y}$ пересекаются, то они совпадают. Для этого покажем, например, включение $\ovl{x}\subseteq \ovl{y}$. Пусть $z\in \ovl{x}\cap \ovl{y}$. Тогда $z\sim x$ и $z\sim y$. Если теперь $x' \sim x$, то $x'\sim x\sim z\sim y$, откуда по транзитивности получаем, что $x'\sim y$, что  и требовалось.
\endproof

\dfn Обозначим за $X/\!\sim$ множество всех классов эквивалентности по отношению $\sim$. Это множество называется фактормножеством $X$ по отношению $\sim$. Есть естественное сюръективное отображение $X\to X/\!\sim$. Класс элемента $x\in X$ я буду обозначать как $\ovl{x}$ или просто как $x$, если из контекста ясно, что нужно взять именно элемент фактормножества.
\edfn

\exm\\
1) Для отношения сравнимости по модулю $n$ классы эквивалентности выглядят как $\{a+kn\,|\, n \in \mb Z\}$. Количество различных классов эквивалентности равно $n$. Они однозначно соответствуют числам от $0$ до $n-1$ -- различным остаткам от деления.\\
2) Если $G$ -- ориентированный граф возможно с петлями, то фактор множества вершин по отношению быть соединённым путём по неориентированным рёбрам в $G$ есть множество компонент связности.\\
3) Множество классов эквивалентности для отношения параллельности для прямых континуально. Каждому такому классу однозначно соответствует угол $0\leq \ffi<\pi $ между каждой прямой и 


Введём специальное обозначение для нашего главного героя.

\dfn Пусть $n\in \mb N$. Тогда отношение фактор множества $\mb Z$ по отношению сравнимости по модулю $n$ обозначается как $\mb Z/n\mb Z$ или просто $\mb Z/n$ и называется множеством классов вычетов по модулю $n$.
\edfn

Даже если в определении некоторого понятия участвует понятие фактормножества, при конкретных вычислениях никто не представляет классы эквивалентности. Вместо этого часто для каждого класса выбирают какого-то его канонического представителя и вместо класса целиком проводят все определения и действия с этими самыми представителями. 

\dfn Пусть $X$ -- множество, $\sim$ -- отношение эквивалентности. Тогда $T\subseteq X$ называется полной системой представителей или трансверсалью для отношения $\sim$, если для каждого $x \in X$ существует единственный $c\in T$, что $x\sim c$. Иными словами, в каждом классе эквивалентности лежит ровно один элемент из $T$.
\edfn

Например, при вычислении в кольце остатков по модулю $n$ думают не про классы эквивалентности, а про числа от $0$ до $n-1$ ровно по той причине, что в каждом классе эквивалентности есть ровно одно число от $0$ до $n-1$.

Выбор представителя в классе эквивалентности --- дело вкуса и удобства, однако есть несколько советов, как можно сделать этот выбор. Например, если множество $X$ было упорядочено. Тогда в классе эквивалентности можно выбрать в качестве представителя наименьший элемент (если он, конечно есть) или наименьший с каким-нибудь свойством. Так для отношения сравнимости по модулю $n$ на целых числах канонические представители выбираются как наименьшие неотрицательные числа в классе эквивалентности.


Однако в теоретических вопросах работать с классами эквивалентности довольно удобно. Покажем это на примере $\mb Z/n$. А именно, введём на этом множестве аналогичные сложению и умножению операции. Однако прежде всего дадим определение.

\dfn Бинарной операцией на множестве $X$ называется отображение $f \colon X \times X \to X$.
\edfn

Введём на множестве $\mb Z/n$ две бинарные операции $+$ и $\cdot$ по следующему правилу:\\
$$ \ovl{a}+\ovl{b}=\ovl{a+b} \text{ и } \ovl{a}\cdot \ovl{b}= \ovl{a\cdot b}.$$
Здесь есть небольшая проблема. Представим себе, что мы хотим посчитать эти операции два двух классов $A$ и $B$. Заметим, что в качестве представителей этих классов по определению можно взять  разные элементы $a\in A$ и $b\in B$. Сумма $a+b$ и произведение $ab$ зависят от этого выбора, так что априори и классы $\ovl{a+b}$  и $\ovl{ab}$ могут от такого выбора зависеть. Наша задача показать, что это определение корректно, то есть, что результат не зависит от выбора представителей.

Действительно, пусть $c,d$ -- другие представители классов $\ovl{a}$ и $\ovl{b}$, то есть $a\equiv c$ и $b\equiv d$. Тогда как мы знаем $a+b\equiv c+d \mod n$ и $ab\equiv cd \mod n$. Но это и означает, что мы получили элементы из тех же классов эквивалентности. 

Какие свойства наследуют эти операции от операций над целыми числами? Практически все базовые свойства <<раскрытия скобочек>>:\\
1) Ассоциативность: для всех $a,b,c \in \mb Z/n$ верно $(a+b)+c=a+(b+c)$.\\
2) Существование нейтрального: существует $0\in \mb Z/n$, такой что $\forall a \in \mb Z/n$ верно $a+0=0+a=a$. Таким свойством обладает класс нуля  $\ovl{0}$.\\
3) Существование обратного: $\forall a \in \mb Z/n$ существует $-a \in \mb Z/n$, что $a+(-a)=(-a)+a=0$.\\
4) Коммутативность сложения: $\forall a,b \in \mb Z/n$ верно $a+b=b+a$.\\
5) Дистрибутивность: $a(b+c)=ab+ac$ и $(b+c)a=ba+ca$.\\
6) Ассоциативность умножения: $(ab)c=a(bc)$\\
7) Существование нейтрального относительно умножения: $\exists 1 \in \mb Z/n$, что $\forall a \in \mb Z/n$ выполнено $1\cdot a= a\cdot 1=a$.\\
8) Коммутативность умножения: $ab=ba$.\\

\rm Если бы мы вводили операции не на классах, а на их представителях -- числах от $0$ до $n-1$, то сложение определялось бы так 
$$a+_n b = \begin{cases}a+b, \text{ если } a+b<n\\
a+b-n, \text{ если } a+b\geq n
\end{cases}$$
Попробуйте проверить ассоциативность сложения по такому определению!
\erm

Для каждого числа $n$ мы получили множество с двумя бинарными операциями. Это вынуждает нас ввести общее определение для того, чтобы мы могли исследовать все эти множества и другие аналогичные объекты одновременно, не разбираясь с каждым по отдельности.






\chapter{Базовые алгебраические структуры}

Из всех свойств операций для остатков и целых чисел только свойство дистрибутивности задействует две операции сразу, все остальные свойства относятся только к одной операции. 
\rm Если операция обозначается как $\cdot$ или похожим образом, то её обычно называют умножением. Обозначение операции $\cdot$ часто опускают и вместо $a\cdot b$ пишут просто $ab$. Если же операция обозначается похожим на $+$ способом, то она называется сложением. По умолчанию используется обозначение умножения $\cdot$.
\erm
Пусть $\cdot\colon X \times X \to X$ -- бинарная операция.
Тогда:\\
{\bf\noindent 1)} Такая операция  называется ассоциативной, если $\forall a,b,c\in X$ выполнено $(a\cdot b) \cdot c=a\cdot (b \cdot c)$.
\rm
Благодаря ассоциативности операции в произведении нескольких элементов можно не расставлять скобки.
\erm
{\bf\noindent 2)} Операция умножения коммутативна, если $\forall a,b \in X$ выполнено $a\cdot b= b\cdot a$

{\bf\noindent 3)} Относительно умножения существует нейтральный, если $\exists e \in X$ такой, что $\forall a\in G$ верно $a\cdot e= e\cdot a= a$.
\rm Если нейтральный элемент существует, то он единственен. Действительно, пусть есть два нейтральных элемента $e_1$ и $e_2$. Тогда имеем $e_1=e_1\cdot e_2=e_2$. Для получения левого равенства мы воспользовались нейтральностью $e_2$, а для получения правого -- нейтральностью $e_1$.
\erm
{\bf\noindent 4)} Если относительно операции умножения на $X$ есть нейтральный элемент $e$, то будем говорить, что элемент $x\in X$ называется обратимым относительно этой операции, если $\exists x^{-1} \in X$, такой что $x\cdot x^{-1}=x^{-1}\cdot x=e$. Такой элемент $x^{-1}$ называется обратным. 

\rm Если обратный элемент относительно операции умножения есть, то он единственен. Действительно, пусть   есть два элемента $y_1$ и $y_2$ из $X$, являющиеся обратными к $x\in X$. Тогда
$$y_1= y_1\cdot 1=y_1(xy_2)=(y_1x)y_2=y_2.$$
\erm

\rm Если есть два обратимых элемента, то их произведение тоже обратимо. Действительно, покажем, что $(xy)^{-1}= y^{-1}x^{-1}$. 
В самом деле 
$$xyy^{-1}x^{-1}= x\cdot 1\cdot x^{-1}=e.$$
Аналогично для произведения с другой стороны.
\erm



\section{Группы}

\dfn[Группа] Пусть $G$ множество на котором задана бинарная операция $\cdot \colon G \times G \to G$. Тогда $G$ вместе с этой операцией называется группой если \\
1) $\cdot$ ассоциативна.\\
2) Существует нейтральный относительно $\cdot$.\\
3) Любой элемент $x\in G$ обратим.
\edfn

Часто говорят, что $G$ -- группа, опуская информацию об операции, если это не приводит к коллизии. Коллизия может произойти, когда на одном и том же множестве заданы две разные операции, относительно которых это множество является группой. Но  такое бывает крайне редко.

\dfn[Абелевы группы] Пусть $G$ -- группа. $G$ называется абелевой группой если \\
4) Для всех $a,b \in G$ выполнено $ab=ba$. То есть операция коммутативна.
\edfn

Операция в абелевых группах часто называется суммой и обозначается как $+$ вместо $\cdot$. В свою очередь нейтральный элемент обозначается как $0$ (вместо $1$ или $e$), а обратный к $a$ как $-a$ (вместо $a^{-1}$).

\exm\\
а) На одноэлементном множестве $G=\{e\}$ можно единственным образом ввести операцию и она-таки удовлетворяет всем свойствам. Это тривиальный пример\\
б) Группа целых чисел $\mb Z$, рациональных $\mb Q$ и вещественных $\mb R$ относительно сложения.\\
в) Пусть $X$ -- множество. Рассмотрим множество $S_X=\{f\colon X\to X\,|\, f \text{ биекция }\}$. Заметим, что композиция биекций -- снова биекция. Действительно $(f\circ g)^{-1}=g^{-1}\circ f^{-1}$. Таким образом, на $S_X$ определена операция $(f,g)\to f \circ g$. Заметим, что  композиция отображений ассоциативна. Нейтральным элементом является тождественное отображение, а обратным относительно композиции -- обратное отображение. Таким образом, $S_X$ образует группу относительно композиции.



Основной лазейкой, при помощи которой группы попадают в этот мир является понятие подгруппы.

\dfn[Подгруппа] Пусть $G$ -- группа, а $H$ -- её подмножество. Тогда $H$ называется подгруппой если\\
1) $\forall a,b \in H$ выполнено, что $ab \in H$. Это свойство называется замкнутостью относительно операции.\\
2) Нейтральный элемент группы $G$ лежит в $H$.\\
3) $\forall a \in H$ выполнено, что $a^{-1} \in H$.
\edfn

\rm Первое свойство позволяет ограничить умножение с $G$ на $H$. Два других свойства гарантируют существование нейтрального и обратного. Ассоциативность наследуется автоматически.
\erm

\noindent г) Рассмотрим множество изометрий плоскости $Isom_{\mb R^2}=\{f\colon \mb R^2\to \mb R^2\,|\, f \text{ изометрия (движение) } \}$. Последнее означает, что $f$ -- биекция и $\forall x,y \in \mb R^2$ верно, что $\dist(x,y)=\dist(f(x),f(y))$, где $\dist$ -- обычное расстояние между точками плоскости. Это подгруппа в $S_{\mb R^2}$. Прежде всего легко заметить, что композиция таких отображений тоже сохраняет расстояние и тождественное отображение сохраняет расстояние. Дальше заметим, что обратное отображение сохраняет расстояние. Действительно $$\dist(f^{-1}(x),f^{-1}(y))=\dist(f(f^{-1}(x)),f(f^{-1}(y)))=\dist(x,y).$$
\noindent д) Рассмотрим множество изометрий сохраняющих фиксированную точку $x_0$ на плоскости. Это подгруппа в $Isom_{\mb R^2}$.

\dfn[Произведение групп] Пусть $G$ и $H$ две группы. Тогда введём на  их декартовом произведении $G\times H$ структуру группы определив умножение покомпонентно 
$$(g_1,h_1)\cdot(g_2,h_2)=(g_1g_2, h_1h_2).$$
\edfn


\rm Единицей этой группы будет элемент $(e,e)$. Обратным к элементу $(g,h)$ будет $(g^{-1},h^{-1})$.
\erm

Если мы хотим сравнить два множества между собой, то мы строим отображение между ними. Что делать, если у нас не просто множества а группы?

\dfn[Гомоморфизм] Пусть $G$ и $H$ -- группы. Отображение $f\colon G\to H$ называется гомоморфизмом групп, если $f(ab)=f(a)f(b)$. 
\edfn

\lm[Свойства] Пусть $f\colon G \to H$ -- гомоморфизм групп. Тогда\\
1) $f(e)=e$\\
2) $f(x^{-1})=(f(x))^{-1}$\\
3) $f(x^n)=f(x)^n$ для всех натуральных $n$.
\proof $f(e)=f(e\cdot e)=f(e)f(e)$. Осталось домножить на $f(e)^{-1}$.
Далее $e=f(e)=f(xx^{-1})=f(x)f(x^{-1})$. Аналогично про произведение $f(x^{-1})f(x)$. Третье очевидно
\endproof
\elm

\dfn Пусть $x \in G$ -- элемент группы, а $n\in \mb Z$. Тогда определим $$x^n=\begin{cases}
x\cdot \dots \cdot x, \text{ $n$ раз } n>0\\
e, n=0\\
(x^{-1})^{|n|}, n<0
\end{cases}$$
\edfn

\upr Проверьте, что $x^{n+m}=x^nx^m$. Покажите, что при гомоморфизме $f(x^n)=f(x)^n$ для всех целых $n$.
\eupr

\dfn[Изоморфизм] Гомоморфизм $G\to H$ называется 1) мономорфизмом 2) эпиморфизмом 3) изоморфизмом, если он 1) инъективен, 2) сюръективен, 3) биективен соответственно.
\edfn

\rm Обратное отображение к изоморфизму -- тоже изоморфизм.
\erm

\exm\\
а) Отображение $\mb Z \to\mb Z/n$, заданное правилом $x\to \ovl x$ является эпиморфизмом.\\
б) Отображение $\mb Z/nm \to \mb Z/n$, заданное правилом $\ovl{x} \to \ovl{x}$ корректно и является эпиморфизмом.\\
в) Отображение $\mb Z/n \to \mb Z/nm$, заданное правилом $\ovl{x}\to \ovl{mx}$ корректно и является мономорфизмом.\\
г)  Отображение $\mb Z \to \mb Z$, заданное правилом $x \to ax$ является гомоморфизмом при любом $a\in \mb Z$, мономорфизмом при $a\neq 0$ и изоморфизмом при $a=\pm 1$.\\
д)  Отображение $\mb R \to \mb R$, заданное правилом $x \to ax$ является гомоморфизмом при любом $a\in \mb R$,  и изоморфизмом при $a\neq 0$.\\
е) Отображение $\mb Z/nm \to\mb Z/n\times \mb Z/m$ заданное правилом $\ovl{x}\to (\ovl{x},\ovl{x})$ является изоморфизмом групп если $(n,m)=1$ по китайской теореме об остатках.\\





\section{Кольца}

Нашей текущей задачей будет введение понятия кольца, 

\dfn[Кольцо] Кольцом называется множество $R$ вместе со введёнными на нём операциями сложения $+\colon R\times R \to R$ и умножения $\cdot \colon R \times R \to R$ со свойствами:\\
1) $(R,+)$ – абелева группа.\\
2) $\forall a,b,c \in R$ верно, что $(a+b)\cdot c= a\cdot c+ b\cdot c$ и  $c\cdot (a+b)= c\cdot a+ c\cdot b$. Это свойство дистрибутивности.
\edfn

Редко кто рассматривает теорию для произвольных колец.

\dfn[Коммутативное ассоциативное кольцо с единицей] Пусть $R$ --- кольцо. Мы будем обращать внимание на следующие свойства:\\
а) Существование нейтрального (единицы) относительно умножения. Кольцо с таким элементом называется кольцом с единицей.\\
б) Ассоциативность умножения. Кольцо в котором умножение ассоциативно называется ассоциативным. \\
в) Коммутативность умножения. Кольцо с таким свойством называется коммутативным.\\
Если выполнены все три свойства, то кольцо $R$ называется ассоциативным коммутативным кольцом с единицей.
\edfn





\lm[Общие свойства] Пусть $R$ — кольцо. Тогда \\
1) $\forall b \in R$  $0\cdot b = b\cdot 0 = 0$;\\
2) $\forall a,b \in R$  $a\cdot(-b) = (-a)\cdot b = -ab$.
3) Если $R$ -- кольцо с единицей то $\forall a \in R$  $(-1)\cdot a = a\cdot (-1) = -a$.
\elm





\exm \\
Примерами колец являются:\\
а) целые числа $\mb Z$;\\
б) рациональные $\mb Q$;\\
в) вещественные $\mb R$;\\
г) Пусть $X$ – некоторое множество, $R$ – кольцо. Рассмотрим множество всех функций $R^X=\{f\colon X \to R\}$. Это кольцо относительно поточечных операций сложения и умножения. Точнее определим 
$$(f+g)(x)=f(x)+g(x) \text{ и } f\cdot g(x)=f(x)g(x).$$
е) Кольца остатков $\mb Z/n$.\\
ё) Рассмотрим пример некоммутативного кольца. Пусть $R$ -- кольцо. Рассмотрим множество троек $R\times R\times R$ и определим на нём структуру кольца по следующему правилу:
$$(a,b,c) + (a',b',c')=(a+a',b+b',c+c') \text{ и } (a,b,c) \cdot (a',b',c')=(a\cdot a',ab'+bc',c\cdot c').$$
Заметим, что произведение элементов $(1,0,0)$ и $(0,1,1)$ разное в зависимости от порядка.\\
ж) Определим произведение колец
\dfn Пусть $R_1, R_2$ -- два кольца. Тогда введём на  их декартовом произведении $R_1\times R_2$ структуру кольца определив операции покомпонентно 
$$(r,u)+(s,v)=(r+s,u+v) \text{ и } (r,u)\cdot(s,v)=(rs, uv).$$
\edfn

\rm
Если $R_1,R_2$ -- 1) ассоциативные, 2) коммутативные, 3) с единицей, то и $R_1\times R_2$  тоже 1) ассоциативное, 2) коммутативное, 3) с единицей, соответственно.
\erm

\dfn[Подкольцо] Пусть $R$ - кольцо. Подкольцом в $R$ называется подмножество $R'$, на которое можно
ограничить операции. Тонкость состоит в том, что если $R$ было кольцом с единицей, то от $R'$ так же требуют, чтобы оно содержало ту же единицу. Морально стоит считать, что взятие единицы так же является операцией (только от нуля аргументов).
Формально это означает, что\\
а) $R'$ -- подгруппа $R$ по сложению\\
б) $\forall a,b\in R'$ должно быть верно $a\cdot b \in R'$ \\
В случае колец с единицей отдельно требуют\\
в) $1\in R'$.
\edfn



\rm Например, множество вещественных чисел вида 
$$\mb Z[\sqrt{2}]=\{a+b\sqrt{2}\,|\, a,b \in \mb Z\} $$
является подкольцом (с единицей) в $\mb R$. Это обозначение читают так: << $\mb Z$ от корня из двух >> или << $\mb Z$ с добавленным корнем из двух>>. 
\erm




Приведём ещё два примера колец, которые понадобятся нам в дальнейшем, а именно кольцо многочленов и кольцо
формальных степенных рядов.



\dfn[Кольцо многочленов] Пусть $R$ – некоторое кольцо. Определим кольцо многочленов $R[x]$ от
одной переменной как кольцо состоящее из выражений вида
$$a_0+a_1x+a_2x^2+\dots+a_nx^n,$$
где все коэффициенты $a_i\in R$. Сложение двух таких выражений происходит покоэффициентно, а умножение однозначно определяется правилом раскрытия скобок и равенством $x^n\cdot x^m=x^{n+m}.$ Точнее для двух многочленов $f(x)=a_0+\dots+a_nx^n$ и $g(x)=b_0+\dots+b_mx^m$ выполнено
$$f(x)g(x)= (a_0b_0)+ \dots+\sum_{i+j=k}a_ib_j x^k+\dots+a_nb_mx^{n+m}$$
\edfn

\dfn[Кольцо формальных степенных рядов] Аналогично определим кольцо формальных степенных рядов как кольцо выражений
$$a_0+a_1x+a_2x^2+\dots+a_nx^n+\dots,$$
где операции определены тем же способом, что и в кольце многочленов.
\edfn

Однако, в таком определении чувствуется нестрогость. Поэтому дадим формальные определения, выписав явно формулу для коэффициентов суммы и произведения двух таких выражений.

\dfn[Кольцо формальных степенных рядов и кольцо многочленов, дубль два] Пусть $R$ -- кольцо. Кольцом формальных степенных рядов над $R$ назовём множество последовательностей $\{(a_0,\dots,a_n, \dots)\,|\, a_i\in R\}$ вместе с операциями:
$$ (a+b)_n=a_n+b_n, \, \text{ и } \, (a\cdot b)_n=\sum_{i+j=n}a_ib_j.$$
Кольцо формальных степенных рядов обозначается $R[[x]]$. Кольцом многочленов над $R$ назовём подкольцо в $R[[x]]$,
состоящее из элементов $a=(a_0,\dots,a_n,\dots)$ таких, что все кроме конечного числа $a_i$ равны $0$.
\edfn

Дав формальное определение я должен бы был проверить, что выполнены все требуемые аксиомы (свойства сложения, дистрибутивность) и то, что $R[x]$ -- подкольцо. Это упражнение на раскрытие скобок, которое я предпочитаю опустить для того, чтобы сохранить ясность происходящего. Так же

\rm Если $R$ -- 1) ассоциативное, 2) коммутативное, 3) с единицей, то $R[x]$ и $R[[x]]$ тоже 1) ассоциативное, 2) коммутативное, 3) с единицей, соответственно.
Единицей в $R[x]$ и $R[[x]]$ будет элемент вида $1(x)=1=1+0\cdot x+0\cdot x^2+\dots$.
\erm




\section{Группа обратимых элементов}

\dfn[Обратимый элемент] Пусть $R$ — ассоциативное кольцо с единицей. Элемент $a\in R$ называется обратимым, если он обратим относительно умножения.
\edfn

\lm[Группа обратимых элементов] Пусть $R$ --- ассоциативное кольцо с единицей. Множество всех обратимых элементов $R^*$ образует группу относительно умножения.
\elm
\proof Ассоциативность наследуется из ассоциативности умножения на всём кольце, единица есть, т.к. обратна себе. Обратный есть, так как обратный есть в $R$ и лежит в $R^*$ (так как ${x^{-1}}^{-1}=x$).
\endproof

\dfn[Поле] Пусть $R$ -- ассоциативное коммутативное кольцо с единицей и $R\neq \{0\}$. $R$ называется полем, если   любой элемент $ a\in R\setminus\{0\}$ обратим. По другому это означает что множество $R\setminus\{0\}$ есть группа относительно умножения.
\edfn



\noindent Посмотрим на примеры описания групп обратимых элементов\\
{\bf Примеры:}\\
а) $\mb Z^* = \{\pm 1\}\cong \mb Z/2$.\\
б) $\mb Q$ -- поле, то $\mb Q^* = \mb Q\setminus\{0\}$, большего особенно не скажешь.\\
в) Пусть $K=\mb R$. Все вещественные числа не равные нулю либо положительны, либо отрицательны. Кроме того, любое положительное число есть экспонента от единственного вещественного числа. Тогда вещественному не нулевому числу $y$ можно сопоставить пару $(\eps(y), \ln|y|)$. Здесь $\eps(y)=\ovl{0}$, если $y>0$ и $\eps(y)=\ovl{1}$, если $y<0$. Получился изоморфизм групп $$\mb R^*\to \mb Z/2 \times \mb R.$$
Обратный к нему -- это гомоморфизм
$$(\eps,x) \to (-1)^{\eps}\, e^x.$$
\noindent г) C кольцом формальных степенных рядов тоже можно разобраться. 
\lm Пусть $R$ -- ассоциативное коммутативное кольцо с 1. Ряд $f(x) \in R[[x]]$ обратим тогда и только тогда, когда его свободный член обратим.
\elm
\proof Пусть дан ряд $f(x)=a_0+a_1x+\dots$. Для того, чтобы ряд был обратим необходимо, чтобы $a_0$ было обратимо. Действительно, условие $fg=1$, где $g=b_0+b_1x+\dots$, для нулевого коэффициента даёт равенство $1=a_0b_0$.  Теперь покажем в обратную сторону, что если $a_0$ обратим, то и весь ряд обратим. Условие $fg=1$ даёт уравнения на коэффициенты $g$. Первое из них мы уже рассмотрели. Пойдём дальше 
$$0=a_0b_n+a_1b_{n-1}+\dots+a_nb_0.$$
Видно, что если определены $b_0, \dots, b_{n-1}$, то однозначно определено и $b_n=\frac{-1}{a_0}(a_1b_{n-1}+\dots+a_nb_0)$.
\endproof
\noindent д) Элементы $\mb Z/n^*$ однозначно соответствуют остаткам, взаимно простым с $n$. Таким образом, в $\mb Z/n^*$ элементов столько же, сколько в  $\{0 < x< n\,|\, (x,n) = 1\}$. Детальное описание будет позже.\\




\section{Комплексные числа}

\dfn Пусть $L$ -- поле, а $K\subseteq L$ -- подкольцо в $L$, являющееся полем относительно ограничения операций с $L$. Тогда $K$ называется подполем $L$, а $L$ расширением $K$.
\edfn



Покажем, что у многочлена $x^2+1$ есть корни в некотором расширении $\mb R$. Представим себе на секунду, что в некотором расширении вещественных чисел $L$ есть корень $i\in L$ у многочлена $x^2+1$. Тогда в поле $L$ должны лежать и все элементы вида $a+bi$, где $a,b\in \mb R$. Более того, если мы захотим перемножить два числа такого вида, то обязаны получить $$(a+bi)(c+di)=ac+bdi^2+(ad+bc)i=(ac-bd)+(ad+bc)i.$$
Это приводит нас к следующей конструкции:

\dfn[Комплексные числа] Множество $\mb R \times \mb R$ вместе с двумя операциями $$(a,b)+(c,d)=(a+c,b+d)\,\,(a,b)\cdot(c,d)=(ac-bd,ad+bc)$$
называется полем комплексных чисел. Будем обозначать это  поле как $\mb C$.
\edfn

\upr Поле комплексных чисел действительно поле.
\eupr

Исходя из мотивации нашей конструкции обозначим за $i$ пару $(0,1)$. Вещественное число $a\in \mb R$ отождествляется с парой $(a,0)$. Элемент $i$ по самому определению $\mb C$ удовлетворяет соотношению $i^2=-1$. Любой элемент $z$ в $\mb C$ однозначно записывается в виде суммы $z=a+bi$, где $a,b\in \mb R$.  Такая форма записи для комплексного числа будет для нас стандартной. Число $a$ называется вещественной частью числа $z$ и обозначается $a=\re z$, число $b$ --- мнимой частью и обозначается $\im z$.

Единственное, что нам понадобится разобрать из упражнения -- это формула для обратного  к комплексному числу. Для удобства введём понятие модуля комплексного числа и понятие сопряжённого к комплексному числу.

\dfn[Модуль комплексного числа] Рассмотрим комплексное число $z=a+bi\in \mb C$. Тогда модулем комплексного числа $z$ назовём выражение
$$|z|=\sqrt{a^2+b^2}.$$
\edfn

\dfn[Комплексное сопряжение] Пусть $z=a+bi\in \mb C$. Тогда сопряжённым к $z$ числом называется $a-bi=\ovl{z}$.
\edfn

\lm[Формула для обратного] Пусть $z\in \mb C$, $z\neq 0$. Тогда
$$ z^{-1} =\frac{\ovl{z}}{|z|^2}= \frac{a}{a^2+b^2}-i\frac{b}{a^2+b^2}.$$
\elm

Оказывается, что отображение комплексного сопряжения обладает замечательными свойствами.


\dfn[Гомоморфизм колец] Пусть $R$ и $S$ --- два кольца. Гомоморфизмом из $R$ в $S$ называется отображение $f\colon R \to S$, что
$\forall a,b \in R$ выполнено 
$$f(a+b)=f(a)+f(b)\text{  и  }f(a\cdot b)=f(a) \cdot f(b).$$
Если кольца $R$ и $S$ с единицей, то естественно дополнительно потребовать $f(1) = 1$. Биективные гомоморфизмы называются  изоморфизмами.
\edfn


\utv Комплексное сопряжение является автоморфизмом $\mb C$. Более того это единственный нетривиальный автоморфизм, который оставляет на месте $\mb R$.
\eutv
\proof То, что это автоморфизм проверяется непосредственно. Покажем единственность. Для начала докажем простую, но показательную лемму.
\lm[Решение переходит в решение]  Пусть имеются два кольца $R$ и $S$ и гомоморфизм колец $\psi\colon R \to S$. Пусть $a_0+a_1x+\dots+a_nx^n=g(x)\in R[x]$ и $\psi(g(x))=\psi(a_0)+\psi(a_1)x+\dots+\psi(a_n)x^n \in S[x]$.  Если $\lambda$ корень $g(x)$, то $\psi(\lambda)$ -- корень $\psi(g(x))$. 
\elm
\proof Имеем $0=f(\lambda)=a_0+a_1\lambda+\dots+a_n\lambda^n$. Подействуем $\psi$ и раскроем скобки. 
$$0=\psi(0)=\psi(f(\lambda)=\psi(a_0)+\psi(a_1)\psi(\lambda)+\dots+\psi(a_n)\psi(\lambda)^n.$$
\endproof

Пусть теперь дан гомоморфизм $\ffi\colon \mb C \to \mb C$, такой что $\ffi(a)=a$ для всех $a\in \mb R$. Тогда $\ffi(a+bi)=a+b\ffi(i)$. Всё определяется образом $\ffi(i)$ и нас интересует какие бывают варианты. Заметим, что $i$ -- это корень многочлена $x^2+1$. Тогда $\ffi(i)$ должен быть корнем $\ffi(x^2+1)=x^2+1$, то есть того же многочлена.  Но $\mb C$ -- поле и у этого многочлена уже есть два корня -- $\pm i$. Других быть не может. Если мы $i$ отображаем в $i$, то получаем тождественное отображение. Иначе $i\to -i$ и элемент $a+bi$ переходит в $a-bi$. То есть, в случае нетождественного отображения, мы обязаны получить комплексное сопряжение.
\endproof


\rm Модуль комплексного числа может быть выражен через комплексное сопряжение: $|z|=\sqrt{z\ovl{z}}$.
\erm

Такое представление удобно. Например, докажем при его помощи мультипликативность модуля комплексного числа

\utv Пусть $z_1, z_2\in \mb C$. Тогда $|z_1z_2| = |z_1||z_2|$.
\eutv
\proof Правая и левая часть вещественные положительные числа. Достаточно доказать равенство их квадратов. Имеем
$$|z_1z_2|^2=z_1z_2\ovl{z_1z_2}= z_1\ovl{z_1}z_2\ovl{z_2}=|z_1|^2|z_2|^2.$$
\endproof


Комплексное число не определяется своим модулем. Геометрически ясно, что недостающим параметром является угол (ориентированный) между данным комплексным числом и лучом $OX$. Это приводит к новой форме записи комплексного числа.

\dfn[Тригонометрическая форма записи] Рассмотрим ненулевое комплексное число $z= a+bi\in\mb C$. Поделим число $z$ на его модуль. Число
$\frac{z}{|z|}$ лежит на окружности $|z| = 1$. Точки такой окружности имеют вид $\cos\varphi +i\sin\varphi$ для единственного $0 \leq \varphi < 2\pi$. Обозначим выражение $\cos\varphi+i\sin\varphi$ за $e^{i\varphi}$. Тогда $z=re^{i\varphi}$, где $0<r=|z|$.
Такая запись единственна и называется тригонометрической записью комплексного числа. Угол $\varphi$ обозначают $\arg z$ и называют аргументом комплексного числа. На самом деле, нам будет удобно считать, что аргумент комплексного числа определён с точностью до $2\pi$. Иными словами, что это элемент группы $\mb R/2\pi\mb Z$. Аргумент нуля не определён.
\edfn


Почему $e^{i\varphi}$ естественно определить как $\cos\varphi+i\sin \varphi$ ? Основная мотивация для этого есть тождество для рядов  (это эвристическое рассуждение, но его можно сделать строгим).
$$\exp(ix) = \sum_{n=0}^{\infty} \frac{(ix)^n}{n!}=\sum_{n=0}^{\infty} \frac{(-1)^n x^{2n}}{2n!} + i\sum_{n=0}^{\infty} \frac{(-1)^n x^{2n+1}}{(2n+1)!}= \cos(x) + i\sin(x).$$
Возможность подставлять в ряды различные значения вместо формальной переменной вы будете долго обсуждать в рамках курса математического анализа. В частности, во все указанные ряды можно будет подставить любое комплексное число. Так же, если для рядов было выполнено некоторое тождество, то оно будет верно и для функций, которые они задают.
Нас на текущий момент гораздо больше  интересует то обстоятельство, что экспонента сумму переводит в произведение. Так как никаких особенных средств матанализа у нас в распоряжении нет, то постараемся показать это свойство в нашем конкретном случае руками, а заодно посмотреть геометрически, что же происходит.


Вопрос: что происходит с аргументом при домножении на другое комплексное число, в частности на число вида $\cos \varphi + i \sin \varphi$? 

\thrm Пусть $z_1$ и $z_2$ -- два комплексных числа, отличных от нуля. Тогда $\arg z_1z_2= \arg z_1+ \arg z_2$.
\ethrm
Дадим два доказательства этой теоремы \proof[Первое доказательство.] Можно считать, что $|z_1|=|z_2|=1$. Тогда $z_1=\cos \alpha+i\sin \alpha$, а $z_2=\cos \beta + i\sin \beta$. Перемножив получаем:
$$z_1z_2=\cos \alpha \cos\beta - \sin\alpha \sin \beta+ i(\cos \alpha \sin\beta + \sin\alpha \cos \beta)= \cos (\alpha+\beta)+i\sin(\alpha+\beta).$$
Что и требовалось.
\endproof

Второе доказательство хорошо тем, что не использует тригонометрических формул, а лишь использует понятие движения и некоторые базовые свойства.


\dfn[Расстояние] Расстоянием между комплексными числами $z_1$ и $z_2$ положим равным $|z_1-z_2|$. Это просто обычное расстояние на плоскости.
\edfn

Отображение плоскости, сохраняющее расстояние называется движением или изометрией плоскости. Множество всех изометрий плоскости образует группу относительно композиции. Верен следующий:
\begin{fact} Поворот вокруг точки на некоторый угол есть изометрия плоскости. Изометрия плоскости, имеющая ровно
одну неподвижную точку является поворотом вокруг этой точки.
\end{fact}

Докажем лемму:

\lm[Геометрическая интерпретация] Пусть $z\in \mb C$ число по модулю равное единице. Тогда отображение $\mb C \to \mb C$  заданное правилом $ x\to zx$   является поворотом вокруг точки 0 на угол $\arg z$.
\elm
\proof Пусть $z\neq 1$ (случай $z=1$ соответствует тождественному отображению). Проверим, что домножение на $z$ имеет ровно одну неподвижную точку 0. Действительно, пусть $zz_1=z_1$. Тогда, если $z\neq 1$, то обязательно $z_1= 0$. Очевидно, домножение на $z$ сохраняет расстояние $$|zz_1-zz_2|=|z(z_1-z_2)|=|z||(z_1-z_2)|=|(z_1-z_2)|.$$
Получается, что домножение на $|z|$ есть поворот. Как узнать угол? Для этого достаточно посмотреть, куда переходит какая-нибудь точка. Например 1. Единица переходит в $z$, то есть точку под углом $\arg z$ к исходной. Значит это поворот на $\arg z$.
\endproof

\proof[Второе доказательство теоремы]
Пусть как и раньше $|z_1|=|z_2|=1$. Тогда по лемме $z_1z_2$ есть $z_2$ повёрнутое на $\arg z_1$, то есть имеет аргумент $\arg z_2+ \arg z_1$.
\endproof

\crl
Домножение на число вида $re^{i\ffi}$, это растяжение в $r$ раз и поворот на угол $\ffi$.
\ecrl

\crl В тригонометрической записи тождество для аргумента смотрится очень естественно 
$$r_1e^{i\ffi_1}r_2e^{i\ffi_2}=r_1r_2e^{i(\ffi_1+\ffi_2)}.$$
\ecrl

\crl[Формула Муавра] Пусть $z\in \mb C$ имеет тригонометрическую запись $z=re^{i\varphi}$. Тогда
$$z^{n}=r^{n}(\cos \varphi +i\sin \varphi )^{n}=r^{n}(\cos n\varphi +i\sin n\varphi ).$$
Эту формулу можно воспринимать, как выражение для косинусов и синусов кратного угла.
\ecrl

То, что тригонометрическая запись числа существует и ведёт себя предсказуемым образом при произведении, позволяет нам более или менее явно построить решения для уравнений некоторого специального вида.

\thrm[Извлечение корней] Пусть $z\in \mb C$, $z=re^{i\varphi}$, $r>0$ . Тогда у уравнения $x^n=z$ есть ровно $n$ различных
решений в $\mb C$, которые задаются формулой
$$ x_k =\sqrt[n]{r} e^{i\frac{\varphi + 2\pi k}{n}} ,\,\, k\in \ovl{0,n-1}.$$
\ethrm
\proof
Прямой проверкой можно установить, что указанные $x_k$ являются корнями уравнения. Необходимо доказать, что они различные. Рассмотрим частное $x_k$ и $x_l$. Это $e^{i\frac{ 2\pi k- 2\pi l}{n}}$. Так как $\frac{k-l}{n}$ не есть целое число, то аргумент $x_k/x_l$ не равен нулю и, значит, это отношение не единица.
\endproof

\dfn Пусть $R$ -- кольцо. Тогда элемент $x\in R$, такой что $x^n=1$ называется корнем степени $n$ из единицы.
\edfn

\rm Числа вида $e^{i\frac{2\pi k}{n}}$ являются корнями степени $n$ из единицы в $\mb C$.
\erm

\dfn Пусть $K$ -- поле. Тогда $\xi\in K$ -- корень степени $n$ из единицы  называется первообразным корнем степени $n$ если его порядок в $ K^*$ ровно $n$. (Для колец более сложное определение).
\edfn

\rm Элементы $e^{i\frac{2\pi k}{n}}$, где $(k,n)=1$ являются первообразными корнями из единицы в $\mb C$.
\erm



Иногда в жизни бывает необходимо посчитать какую-то странную сумму. Часто это невозможно сделать, но иногда компактный ответ можно найти. Приведём пример, как комплексные числа могут помочь в подсчёте сумм.
Рассмотрим сумму $1+ \cos x + \cos 2x + \dots + \cos nx$, где $x > 1$. Вопрос состоит в том, чему она равна в зависимости от $n$. Основной трюк здесь --- заменить непонятные вещественные числа, на их улучшенную комплексную версию. Например, $$\cos x = \frac{e^{ix}+e^{-ix}}{2}, \sin x = \frac{e^{ix}-e^{-ix}}{2i}.$$
В данном случае, проще заметить, что $\cos x=\re e^{ix}$. Тогда
$$1 + \cos x+ \dots + \cos nx = \re(1+ e^{ix} + \dots + e^{inx}) = \re \frac{e^{i(n+1)x}-1}{e^{ix}-1}$$
Теперь необходимо привести выражение к виду, не содержащему комплексных чисел.
$$\re \frac{e^{i(n+1)x}-1}{e^{ix}-1} = \re e^{i\tfrac{n+1}{2}x}\cdot e^{-i\tfrac{x}{2}}\frac{\sin \tfrac{n+1}{2}x}{\sin\tfrac{x}{2}}=\frac{ \sin \tfrac{n+1}{2}x}{\sin\tfrac{x}{2}} \re e^{\tfrac{inx}{2}}=
\frac{\cos \tfrac{nx}{2} \sin \tfrac{n+1}{2}x}{\sin\tfrac{x}{2}}$$


При решении задачи очень часто бывает, что сам факт существования того, о чём идёт речь вообще говоря является не очевидным. Например, далеко не каждая функция имеет на отрезке минимум и следовательно задача поиска минимального значения функции может быть просто не корректна. Поэтому важно заранее знать, что предмет исследования есть.  В связи с этим можно спросить, а бывает ли так, что есть поле, такое что каждый многочлен с коэффициентами из этого поля имеет корень в нём?

Ответ — да, такое поле есть. Первый и основной такой пример --- это как раз поле комплексных чисел.
\dfn[Алгебраическая замкнутость] Поле $K$ называется алгебраически замкнутым, если у любого многочлена  $f(x)\in K[x]$, отличного от константы, в $K$ найдётся его корень.
\edfn

\thrm[Основная теорема алгебры] Поле $\mb C$ алгебраически замкнуто.
\ethrm

Не совсем понятно, насколько это основная теорема. На такое звание она тянула в начале девятнадцатого века. На самом деле не совсем понятно, насколько это теорема алгебры, потому что в любом её доказательстве используются методы математического анализа. Мы не будем сейчас доказывать эту теорему. Для интересующихся, самое простое доказательство приведено ниже. 






\subsection*{Дополнение: доказательство основной теоремы алгебры}

\begin{thmm}[Основная теорема алгебры] Поле $\mb C$ алгебраически замкнуто.
\proof Пусть $f$ — многочлен степени $n\geq 1$ в $\mb C[x]$. Будем считать, что старший коэффициент $f$ равен единице. Пусть у этого многочлена нет корней. Рассмотрим функцию $|f|\colon \mb C \to \mb R$. Эта функция непрерывна и не принимает значения 0. Так как $f$ --- многочлен степени $n$, то на бесконечности $|f|$ растёт. Разберёмся точнее. Пусть
$c = |f(z_0)| > 0$ в некоторой точке $z_0$. Я утверждаю, что вне некоторого круга радиуса $R$ с центом в 0 функция
$|f|$ принимает значения строго больше чем $c$. Действительно возьмём $R= M \max(2,c)$, где $M$ --- сумма модулей всех коэффициентов многочлена $f$. Двойка здесь играет роль числа строго большего единицы.
Тогда для поиска инфимума $|f|$ достаточно ограничиться кругом радиуса $R$. Но круг радиуса $R$ --- компактное множество и непрерывная функция $|f|$ достигает на нём минимальное значение в точке $x_0$. Пусть $a_0 =f(x_0)$. Разложим
$f$ по степеням $(x-x_0)$. Тогда имеем
$$f(x) = a_0 + a_k (x-x_0)^k + \dots + a_n(x -x_0 )^n.$$
Здесь $a_k$ --- первый ненулевой коэффициент после $a_0$. Такой есть потому что иначе $f$ --- это константа. Теперь наша
задача понять, что мы можем немного сдвинуться от точки $x_0$ , так, чтобы значение $|f|$ уменьшилось. В районе точки
$x_0$ самое большое слагаемое в разложении $f$ это $a_0 + a_k (x-x_0)^k$ и оно практически полностью определяет поведение $f$.
У этого многочлена есть корень. Обозначим его за $y_0$. Будем двигаться из $x_0$ в направлении $y_0$ и смотреть, что происходит.
Рассмотрим $x_{\eps} = x_0 + \eps(y_0 -x_0 ), \,\eps < 1$. Тогда $x_{\eps} -x_0 = \eps(y_0 -x_0 )$. Тогда
$$|f(x_{\eps})| = |(1 - \eps^k)a_0 + \eps^{k+1} a_{k+1} (y_0 -x_0)^{k+1} + \dots+ \eps^n(y_0-x_0 )^n | \leq (1- \eps^k )|f(x_0)| + \eps^{k+1}N,$$
где $N$ — это некоторая константа не зависящая от $\eps$. Например, можно взять $N = \sum_{i=k+1}^n |a_i||y_0 -x_0 |^i$. Для достаточно
маленьких $\eps$ выражение $ -\eps^k |f(x_0)| + \eps^{k+1}N$ отрицательно. Тогда для всех достаточно маленьких $\eps>0$ выполнено неравенство $|f(x_{\eps})| < |f(x_0)|$.
Противоречие с минимальностью $|f(x_0)|$. \endproof
\end{thmm}





\chapter{Теория делимости}

\section{Области целостности}

\rm Вероятно, все кольца в этом курсе будут ассоциативными с единицей, а большая часть --- коммутативными. Поэтому теперь будем считать, что все кольца коммутативные ассоциативные и с единицей до тех пор, пока не оговорено противное.
\erm

Попробуем обобщить теорию делимости с кольца целых чисел на более широкий класс колец.

Давайте посмотрим на уравнение $ax=ay$. Если $a=0$, то, понятно, что любые $x$ и $y$ подходят. Однако, если $a\neq 0$, то, вообще говоря, не для произвольного кольца  следует, что $x=y$. Нужно некоторое свойство сократимости. Разберёмся поподробнее.

\dfn[Делитель нуля, нильпотент] Пусть $R$ --- кольцо.\\
1) Элемент $a\in R$ называется делителем нуля, если существует $b\in R$, $b\neq 0$, такой что $ab=0$. Будем говорить, что
$a=0$ --- это тривиальный делитель нуля.\\
2) Элемент $a\in R$ называется нильпотентом, если существует $n\in \mb N$, что $a^n= 0$. $a= 0$ --- тривиальный случай.
\edfn

\dfn[Область целостности] Кольцо $R$ называется областью целостности, если для любых $a,b\in R$,
если $ab=0$, то либо $a = 0$, либо $b= 0$.
\edfn

\lm Кольцо $R$ является областью целостности тогда и только тогда, когда для него выполнена сократимость.
\proof Покажем, что из целостности следует сократимость. Пусть $a\neq 0$ и $ab=ac$. Тогда $a(b-c)=0$. Тогда $b-c=0$. Что и требовалось.
\endproof
\elm


\noindent {\bf Примеры:}
\enm
\item Рассмотрим кольцо $\mb Z/n$.
Пусть так случилось, что $n\in \mb N$ -- составное. Опишем все нетривиальные делители 0 в $\mb Z/n$. Пусть $k \in \mb Z$ такое, что $d=\Nod(k,n)\neq 1$, но и $k \ndi n$.\\
Тогда и $k=ld$ и $\frac{n}{d}$ не равны нулю, как элементы $\mb Z/n$. С другой стороны 
$$k\frac{n}{d}= ld\frac{n}{d}= ln=0 \text{ в $\mb Z/n$}.$$
Например, $n=6$, а $k=4$. Тогда $d=2$, $\frac{n}{d} = 3$. Получаем $4\cdot 3=12=0$ в $\mb Z/6$.
Так как остальные $k$ либо дают ноль по модулю $n$ либо обратимы, то это полное описание.
\item Пусть $n=m^k$ , $m> 1$. Тогда $m$ --- нетривиальный нильпотент в $\mb Z/n$. Например, $3$ в $\mb Z/9$.
\item Поле всегда область целостности.
\item Рассмотрим кольцо функций $R^X$, где $X$ -- некоторое множество, а $R$ -- кольцо. Тогда $R^X$ -- область целостности тогда и только тогда, когда $R$ -- область целостности и $X$ состоит из одного элемента. Прежде всего, если $ab=0$, но $0\neq a,b \in R$ и в $X$ есть хотя бы один элемент $x_0\in X$, то
$$f(x)=\begin{cases} a,\, x=x_0\\
0, \text{ иначе }
\end{cases} 
\text{ и } 
g(x)=\begin{cases} b,\, x=x_0\\
0, \text{ иначе }
\end{cases} 
$$
это нетривиальные делители 0 в $R^X$. Если же $R$ область целостности, но в $X$ есть хотя бы две точки $x_0$ и $y_0$, то
$$f(x)=\begin{cases} 1,\, x=x_0\\
0, \, x=y_0\\
0, \text{ иначе }
\end{cases} 
\text{ и } g(x)=\begin{cases} 0,\, x=x_0\\
1,\, x=y_0\\
0, \text{ иначе }
\end{cases} 
,$$
так же дают нетривиальные делители 0.
\eenm




\section{Идеалы}

Рассмотрим конечный набор элементов $a_1, \dots, a_n \in R$ и рассмотрим уравнение первой степени
$$a_1 x_1 + \dots +a_n x_n =y.$$
Вопрос — для каких $y$ это уравнение разрешимо? Естественно получить удовлетворительный ответ в общем случае нельзя, но можно заметить какие-то свойства. А именно, сумма $y_1+y_2$ таких элементов тоже является решением уравнения и $ry$ -- тоже решение уравнения. Разовьём тему.

\exm\\
0) $0,R$ -- всегда идеалы в кольце $R$.\\
1) $n\mb Z \leq \mb Z$\\
2) Если заданы элементы  $a_1, \dots, a_n \in R$, то идеал $(a_1,\dots,a_n)=\{a_1 x_1 + \dots +a_n x_n\, |\, x_i\in R\}$ называется идеалом, порождённым $a_1,\dots,a_n$.\\
3) Идеал $(f)$ ещё обозначается как $fR$ называется главным идеалом.\\
4) Рассмотрим кольцо непрерывных функций $C([0,1])$. Пусть $Y$ некоторое подмножество в $[0,1]$. Тогда множество функций $I_Y=\{ f\in C([0,1])\,|\, f(y)=0, \text{ для всех } y\in Y\}$ есть идеал в $C([0,1])$.\\
5) Опишем все идеалы в кольце целых чисел:


\utv Любой идеал в кольце $\Z$ главный.
\eutv




\subsection{Гомоморфизмы колец}

В самом начале мы обсудили, что некоторые группы могут иметь альтернативное, более простое описание, как например группа вычетов $\mb Z/nm$, где $(n,m)=1$. Оформлено это замечание было в виде понятия изоморфизма групп. Чуть позже к нему добавилось понятие гомоморфизма, как способ просто связать, а не «приравнять» две структуры. Аналогичное определение есть и для колец.

\dfn[Гомоморфизм колец] Пусть $R$ и $S$ --- два кольца. Гомоморфизмом из $R$ в $S$ называется отображение $f\colon R \to S$, что
$\forall a,b \in R$ выполнено 
$$f(a+b)=f(a)+f(b)\text{  и  }f(a\cdot b)=f(a) \cdot f(b).$$
Если кольца $R$ и $S$ с единицей, то естественно дополнительно потребовать $f(1) = 1$. Сюръективные, инъективные, биективные гомоморфизмы называются эпи-, моно- и изоморфизмами соответственно.
\edfn

Отметим сразу два базовых свойства гомоморфизмов колец:

\lm Пусть $f\colon R \to S$  и $g\colon S \to T$ --- гомоморфизмы колец. Тогда $g\circ f\colon R \to T$ так же гомоморфизм
колец. Если $f$ — изоморфизм, то отображение $f^{-1}$ --- гомоморфизм (и, следовательно, изоморфизм). Тождественное отображение всегда гомоморфизм колец.
\elm
\proof Докажем про гомоморфность обратного. Пусть есть $a,b \in S$. Тогда $f(f^{-1}(a)\cdot f^{-1}(b))=f(f^{-1}(a))\cdot f(f^{-1}(b))=a\cdot b$. Тогда элемент $f^{-1}(a)\cdot f^{-1}(b)$ есть прообраз $a\cdot b$. Сумма --- аналогично.
\endproof



\dfn Пусть $f\colon R \to S $  -- гомоморфизм колец. Определим ядро гомоморфизма  как $\Ker f=\{ a \in R\,|\, f(a)=0\}$.
\edfn

\rm Ядро гомоморфизма колец это идеал. Действительно, если $f(a)=f(b)=0$, то $f(a+b)=f(a)+f(b)=0$. Аналогично, $f(ra)=f(r)f(a)=0$, если $a\in \Ker f$. 
\erm


Рассмотрим различные примеры гомоморфизмов: прежде всего на  глаза бросается пример гомоморфизма $\mb Z\to \mb Z/n$ переводящего элемент $n$ в его класс в $\mb Z/n$. Мы уже отмечали, что это гомоморфизм групп. Осталось заметить, что он так же переводит умножение в умножение. Оказывается, что мы можем легко описать все гомоморфизмы из $\mb Z$ в любое кольцо.


\thrm Пусть $R$ --- кольцо. Тогда существует единственный гомоморфизм из $\mb Z \to R$.
\ethrm

\proof Докажем единственность. Пусть $f\colon \mb Z \to  R$. Тогда $f(1)=1$, но тогда. $f(2)=f(1+1)=f(1)+f(1)$ и других вариантов нет. Аналогично для $n\in \mb N$ верно $f(n)=1+\dots+1$ $n$-раз. Так как $f(-n)=-f(n)$ то для отрицательных чисел тоже никакого выбора.

Из части про единственность мы уже поняли как выглядит гомоморфизм. Положим $$f(n)=\begin{cases}
1+\dots+1 \text{ $n$ раз }, n>0\\
-(1+\dots+1) \text{ взяв $-n$ раз }, n<0\\
0, n=0
\end{cases}.$$

Надо проверить, что это гомоморфизм. Проверим для произведения. Пусть $n,m\in \mb N$ тогда 
$$f(nm)=\sum_{i=1}^{nm} 1= \sum_{j=1}^n 1\cdot f(m)=\Big(\sum_{j=1}^n 1\Big)\cdot f(m)=f(n)\cdot f(m).$$
Остальные случаи следуют из этого. Сумма разбирается аналогично.
\endproof





Такое утверждение позволяет нам определить очень важный инвариант кольца:
\dfn Пусть $R$ --- кольцо. Рассмотрим тот самый единственный гомоморфизм $f\colon \mb Z \to R$. Тогда $\Ker f$
--- это подгруппа в $\mb Z$, то есть имеет вид $n\mb Z$ для некоторого натурального $n$. Это число $n$ называется характеристикой кольца $R$ и обозначается $n=\chr R$. Попросту говоря, характеристика --- это наименьшее число единичек, сложив которые можно получить $0$ (или число $0$, если это невозможно).
\edfn

Очевидно, что характеристика кольца $\mb Z/n$ есть $n$. Характеристика $\mb Z, \mb Q,\mb  R$  равна $0$. Характеристика $R[x]$ и $R[[x]]$ равна $\chr R$.

\rm Характеристика поля обязательно простое число или ноль. \proof  Пусть теперь $\chr K=n\neq 0$, а $f$ --- единственный гомоморфизм $f\colon \mb Z \to K$. Если $n=mk$, где $1<m,k< n$, то $f(m)\neq 0$ и $f(k)\neq 0$, но $f(m)\cdot f(k)=f(n)=0$, что и даёт нетривиальные делители нуля в $K$.
\endproof
\erm



Для того, чтобы покомфортнее чувствовать себя при работе с понятием гомоморфизма выясним, как описать все
гомоморфизмы из кольца многочленов $R[x]$ в произвольное кольцо $S$. Прежде всего отметим, что если $\Psi \colon R[x]\to S$ гомоморфизм, то ограничение $\psi= \Psi|_{R}$ --- на постоянные многочлены, есть гомоморфизм $\psi \colon R \to S$.

\thrm Пусть $R$ и $S$ --- два кольца. Пусть $\psi \colon R \to S$ гомоморфизм, а $\lambda$ --- некоторый элемент из $S$. Тогда
существует единственный гомоморфизм $\Psi \colon R[x]\to S$, такой, что ограничение $\Psi|_{R}=\psi$ и $\Psi(x)=\lambda$.
\ethrm
\proof Опять же в таких случаях сначала докажем единственность, заодно поняв, как вообще может быть устроен указанный гомоморфизм.
Действительно, нам задано как $\Psi$ действует на константах из $R$. Так же мы знаем выражение для $\Psi(t^n)= \lambda^n$. Но тогда 
$$\Psi(a_0+ a_1 t+\dots+a_nt^n)=\psi(a_0)+\psi(a_1)\lambda+\dots+\psi(a_n)\lambda^n$$
по аддитивности и мультипликативности. Заметим, что $\Psi$ обязано быть отображением подстановки точки $\lambda$ в многочлен. Таким образом, единственность есть.

Видно, что выбора для определения $\Psi$ нет. Осталось проверить, что формула $$\Psi(a_0+\dots+a_nt^n)=\psi(a_0)+\dots+\psi(a_n)\lambda^n$$
даёт гомоморфизм. Аддитивность опустим и проверим более сложную мультипликативность. Пусть $g(x)=b_0+\dots+b_mx^m\in R[x]$. 
Тогда 
$$\Psi(fg)=\sum_{k=0}^{n+m}\psi\left(\sum_{i+j=k} a_ib_j \right)\lambda^k=\sum_{k=0}^{n+m}\sum_{i+j=k} \psi(a_i)\psi(b_j) \lambda^{i+j}$$
С другой стороны 
$$\Psi(f)\Psi(g)=\left(\psi(a_0)+\dots+\psi(a_n)\lambda^n\right)\left(\psi(b_0)+\dots+\psi(b_m)\lambda^m\right)$$
Осталось раскрыть скобки по дистрибутивности
$$\Psi(f)\Psi(g)=\sum_{i=0}^{n}\sum_{j=1}^m \psi(a_i)\psi(b_j) \lambda^{i+j}$$
и перегруппировать слагаемые.
\endproof






Для чего вообще нужны гомоморфизмы? Один из подходов можно найти в следующем замечании --- при гомоморфизмах решение полиномиального уравнения переходят в решение того же уравнения. Придадим этому точный смысл.

\dfn[Образ многочлена при гомоморфизме] Пусть имеются два кольца $R$ и $S$ и гомоморфизм $\psi\colon R \to S$. Пусть так же есть многочлен $g \in R[x_1,\dots,x_n]$. Тогда определим многочлен $\psi(g)$, как многочлен, на коэффициенты которого подействовали отображением $\psi$.
\edfn




\lm[Решение переходит в решение]  Пусть имеются два кольца $R$ и $S$ и гомоморфизм $\psi\colon R \to S$. Если $\lambda$ корень $g(x)$, то $\psi(\lambda)$ -- корень $\psi(g(x))$.
\elm
Безусловно, то же самое верно и для систем уравнений. Если ясно, какой гомоморфизм имеется ввиду, то $\psi(g(x))$ я буду продолжать обозначать как $g(x)$. В частности, если $R=\mb Z$, то такой $\psi$ единственен и для $\psi(g(x))$ просто нет вариантов. 

Это соображение можно использовать для того, чтобы доказать что у целочисленного уравнения нет решений. Например, мы знаем, что отображение $\mb Z \to \mb Z/n$ -- это гомоморфизм колец. Значит целочисленное решение уравнения $f(x)=0$, где $f(x)\in \mb Z[x]$ переходит в решение того, же уравнения но с коэффициентами, взятыми по модулю $n$. Но тогда, если по модулю $n$ не было  решений, то не было и целочисленных решений. 





\section{Основные понятия}

\dfn[Делимость в кольцах] Будем говорить, что элемент $a\in R$ делится на $b \in R$, если существует
$c \in R$, что $a=bc$. Обозначать это будем как $a\di b$ (или $b|a$).
\edfn

\dfn[Ассоциированные элементы] Будем говорить, что элементы $a,b\in R$ ассоциированы, если $b\di a$ и $a \di b$.
\edfn

\lm[Переформулировки условия ассоциированности] Следующие условия эквивалентны:\\
1) $a$ и $b$ ассоциированы.\\
2) $a = \eps b$, где $\eps \in R^*$.\\
3) $(a) = (b)$.
\elm
\proof Очевидно, что 1) равносильно 3). 

Теперь заметим, что если $a = \eps b$, где $\eps \in R^*$, то $a\di b$ и $b=\eps^{-1}a \di a$. 

Пусть теперь $a\di b$ и $b\di a$. Тогда $a=bc_1$, $b=ac_2$. Подставляя одно в другое получаем $a=ac_1c_2$. Если $a=0$, то и $b=0$ и доказывать нечего. Если же $a\neq 0$ то сокращая получаем, что $1=c_1c_2$. Отсюда $c_1$ обратим, что и требовалось.
\endproof

\dfn[НОД] Рассмотрим элементы $a,b\in R$. Будем говорить, что $d$ является наибольшим общим делителем $a$ и $b$, если $a\di d$, $b\di d$ (то есть является общим делителем) и для любого другого общего делителя $d'$ $d\di d'$.
\edfn

\lm[Единственность НОД-а] Пусть $R$ --- область целостности $a,b \in R$. Тогда если у пары $a,b$ есть НОД,
то он единственен с точностью до ассоциированности.
\elm
\proof
Пусть $d$ и $d'$ два наибольших общих делителя. В частности, они общие делители для $a$ и $b$. Тогда $d \di d'$ так как $d$ наибольший и $d' \di d$ симметрично.
\endproof

\dfn[Неприводимый элемент] Элемент $p\neq 0$, $p\notin R^*$ называется неприводимым, если он не раскладывается на множители нетривиальным образом. То есть для любых $a,b \in R$, если $ab=p$, то либо $a$ обратим, либо $b$ обратим.
\edfn

\dfn[Простой элемент] Элемент $p\neq 0$ называется простым, если  для любых $a,b\in R$, если $ab\di p$, то или $a\di p$ или $b\di p$ и при этом $p\notin R^*$.
\edfn

\utv Простой ненулевой элемент в области целостности обязательно неприводим.
\proof Действительно, если $p=ab$, то либо $a\di p$, либо $b \di p$. Пусть $a$. Тогда $p$ и $a$ ассоциированы. Но тогда $p=\eps a=ab$, $\eps \in R^*$. Но тогда $\eps=b$. Что и требовалось.
\endproof
\eutv

\rm[Дополнительно] Рассмотрим подкольцо в $\mb R$ вида  $\mb Z[\sqrt{5}]=\{a+\sqrt{5}b\,|\, a,b\in \mb Z\}$. В нём есть элемент $3+\sqrt{5}$. Если умножить $(3+\sqrt{5})(3-\sqrt{5})=9-5=4 =2\cdot 2$, то увидим, что у числа $4$ есть два разложения. Заметим, что $2$ не делит ни $3+\sqrt{5}$ ни $3-\sqrt{5}$. Если мы покажем, что элемент $2$ неприводим, то получится пример неприводимого, но не простого элемента в кольце.

Пусть есть разложение $(a+b\sqrt{5})(c+d\sqrt{5})=2$. Тогда $(a-b\sqrt{5})(c-d\sqrt{5})=2$ так же. Значит
$2\cdot 2= (a+b\sqrt{5})(c+d\sqrt{5})(a-b\sqrt{5})(c-d\sqrt{5})=(a^2-5b^2)(c^2-5d^2)$. Это соотношение между целыми числами. Покажем, что оно не может быть выполнено, если в исходном разложении не было обратимых элементов.

Если $a^2-5b^2=\pm 1$, то $a+b\sqrt{5}$ обратим. Если $a^2-5b^2=\pm 4$, то $c^2-5d^2=\pm 1$, что приводит к желаемому результату. Значит остался случай $a^2-5b^2=\pm 2$. Посмотрим на это уравнение по модулю $5$. Оно принимает вид $a^2\equiv 2 \mod 5$. 

Легко проверить, что оно не имеет решений. Значит и исходное уравнение не имеет целочисленных решений.  Отсюда получаем, что $2$ не раскладывается нетривиальным образом на множители. Значит $2$ неприводим в $\mb Z[\sqrt{5}]$, но не прост.

Теперь несложно заметить, что у элементов $(3+\sqrt{5})^2=2(7-3\sqrt{5})$ и $4$ нет наибольшего общего делителя в $\mb Z[\sqrt{5}]$ так как оба они делятся на $2$ и на $3+\sqrt{5}$, но нет общего делителя, который бы на каждый из них делился.
\erm





\section{Основная теорема арифметики для областей главных идеалов}

Кроме условия сократимости для целых чисел важнейшим для нас фактом, было то, что в кольце целых чисел любой идеал является главным. Это давало нам возможность построить $\Nod$ и его линейное разложение. Оказывается, что это условие, достаточно для построения хорошей теории.

\dfn[Область главных идеалов] Область целостности $R$ называется областью главных идеалов, если любой идеал $I\leq R$ является главным.
\edfn

\exm\\
1) Целые числа.\\
2) Формальные степенные ряды над полем.
\lm Пусть $K$ --- поле, тогда все нетривиальные идеалы в кольце $K[[x]]$ имеют вид $(x^n)$. \elm
\proof
Пусть $I$ --- идеал. Рассмотрим элемент $a(x)\in I$ начинающийся с наименьшей возможной степени $x$, скажем $x^k$. Тогда любой элемент начинается хотя бы с $x^k$ и, следовательно, делится на $x^k$. Значит $I\subset (x^k)$. По выбору $a(x)$ имеет вид $a(x)=x^k g(x)$, где $g(x)$ имеет ненулевой свободный член. Тогда $g(x)$ обратим. Тогда $g^{-1}(x)a(x)=x^k\in I$. Значит $(x^k)\subset I$.
\endproof

Обсудим основные свойства областей главных идеалов, аналогичные целым числам.

\lm[Существует НОД]  Пусть $R$ --- область главных идеалов. Тогда для любых $a,b \in R$ существует $d$ -- наибольший общий делитель $a$ и $b$. Более того $(a,b)=(d)$.
\elm
\proof Пусть $d$ -- такое, что $(a,b)=(d)$. Тогда $a\di d$ и $b\di d$, то есть $d$ -- общий делитель. Далее пусть $d'$ другой общий делитель. Посмотрим на разложение $d=ax+by$. Заметим, что каждое из слагаемых делится на $d'$. Тогда и $d\di d'$. 
\endproof

\lm[Простой=неприводимый в ОГИ] Пусть $R$ --- область главных идеалов. Тогда любой неприводимый элемент прост. 
\begin{proof} Пусть элемент $p$ --- неприводим. Допустим $ab\di p$. Рассмотрим $(a,p)=(d)$. Так как $p$ --- неприводим, то $d$ или ассоциирован с $p$ и тогда $p|d|a$ и это нам подходит, или $d $ -- обратим. Если $d$ обратим, то $(a,p)=(d)=(1)$. Тогда рассмотрим разложение $1 = ax  + py$. Домножив на $b$ получаем $b = bax+bpy$. Так как оба слагаемых делятся на $p$, то и $b$ делится на $p$.
\end{proof}
\elm


Верны и остальные аналоги утверждений про делимость целых чисел. Перейдём к основной цели.



\thrm[Существование и единственность разложения] Пусть $R$ --- область главных идеалов. Тогда для любого $a\in R$, $a\neq 0$ существует представление $a$ в виде 
$$ a=\eps p_1\dots p_n,$$
где $p_i$ --- простые, а $\eps \in R^*$. Причём такое разложение единственно с точностью до ассоциированности и перестановки сомножителей. То есть для любого другого разложения $a=\eps' q_1\dots q_m$ верно, что $n=m$ и существует перестановка $\sigma \in S_n$ $q_i\sim p_{\sigma(i)}$.
\ethrm Для того, чтобы доказать, что разложение существует нам понадобится лемма.
\lm Пусть $R$ --- область главных идеалов. Рассмотрим некоторую возрастающую последовательность идеалов
$$I_1\subseteq I_2\subseteq \dots \subseteq I_n\subseteq \dots.$$ Тогда существует $N\in \mb N$, что для любого $i\in \mb N$, выполнено, что $I_{N}=I_{N+i}$.
\elm \proof[Доказательство леммы] Рассмотрим объединение $J=\cup_{n\in \mb N} I_n$. Тогда $J$ идеал. Тогда $J=(d)$. Тогда $d\in I_N$ для некоторого $N$. Тогда $J=I_N$. Понятно, что $J=I_N\subseteq I_{N+i}\subseteq J$, что означает, что все они равны между собой.
\endproof

\proof[Существование] Пусть $a\neq 0$. Тогда есть два варианта. Первый --- $a$ --- простой, тогда всё доказано. Второй --- у $a$ есть нетривиальный делитель $a=bc$. Если у нас есть разложение для $b$ и $c$, то есть и для $a$. Пусть его нет скажем для $b$. Тогда $b$ не прост и у него есть нетривиальный делитель $b_1$ у которого тоже нет разложения. Для $b_1$ есть делитель $b_2$ и т.д. Таким образом, у нас есть бесконечная цепочка делителей $a \di b \di b_1 \di b_2 \di \dots$. Тогда есть цепочка идеалов $(a)<(b)<(b_1)<(b_2)<\dots$. Все включения строгие так как $b_{i+1}$ нетривиальный делитель $b_i$ (то есть они не ассоциированы). Противоречие с леммой.

\proof[Единственность] Рассмотрим два разложения $a=\eps p_1\dots p_n=\eps' q_1\dots q_m$. Рассмотрим элемент $p_1$. Он делит $q_1\dots q_m$. Так как $p_1$ --- простой, то он делит какой-то сомножитель $q_{k_1}$. Тогда $p_1\sim q_{k_1}$ так как $q_{k_1}$ неприводим. Сократим на $p_1$ обе части равенства и продолжим по индукции.
\endproof

\lm В области главных идеалов $R$ элемент $a=\eps p_1^{\alpha_1}\dots p_k^{\alpha_k}$ делится на $b=\eps' p_1^{\beta_1}\dots p_k^{\beta_k}$ тогда и только тогда, когда $\alpha_i\geq \beta_i$. Здесь предполагается, что $p_i$ и $p_j$ не ассоциированы, если $i\neq j$  и их набор одинаков для обоих разложений. Зато $\alpha_i$ и $\beta_j$ могут быть раны нулю.
\proof Очевидно, что если условие выполнено, то $a\di b$. Обратно. Пусть $a\di b$, то есть $a=cb$ для некоторого $c=up_1^{\gamma_1}\dots p_k^{\gamma_k}$. Тогда для $a$ имеется два разложения
$$a=\eps p_1^{\alpha_1}\dots p_k^{\alpha_k}=u\eps'p_1^{\beta_1+\gamma_1}\dots p_k^{\beta_k+\gamma_k}.$$
Так как $p_i$ и $p_j$ не ассоциированы, то $\alpha_i=\beta_i+\gamma_i$. Так как $\gamma_i$ неотрицательно получаем нужное неравенство.
\endproof
\elm



\lm В области главных идеалов для любых двух элементов существует $\Nod$. Более того, если $a=\eps p_1^{\alpha_1}\dots p_k^{\alpha_k}$, а $b=\eps' p_1^{\beta_1}\dots p_k^{\beta_k}$, то $$\Nod(a,b)=d=p_1^{\min(\alpha_1,\beta_1)}\dots p_k^{\min(\alpha_k,\beta_k)}.$$
\proof
Очевидно, что $d$ есть общий делитель для $a$ и $b$. Рассмотрим другой общий делитель $d'=p_1^{\gamma_1}\dots p_k^{\gamma_k}$. Тогда $\gamma_i\leq \alpha_i$ и $\gamma_i\leq \beta_i$. Что и требовалось.
\endproof
\elm







Как понять, что ваше кольцо это область главных идеалов? Проще всего придумать аналог для деления с остатком -- процедуру, которая в некотором смысле уменьшает один элемент при помощи прибавления другого. Точнее:

\dfn[Евклидово кольцо] Пусть $R$ --- область целостности. Будем говорить, что $R$ --- евклидово, если существует функция $\nu \colon R\setminus \{0 \}\to \mb Z_{\geq 0}$ , что для любых $a,b \in R\setminus \{0\}$ существуют $q,r\in R$, что
$$a=bq+r \text{ и либо $\nu(r)<\nu(b)$, либо } r=0.$$
\edfn



\thrm[Все идеалы евклидова кольца главные] Пусть $I\leq R$ --- идеал в евклидовом кольце. Тогда $I$ --- главный.
\ethrm

\proof Повторим стандартное доказательство для $\mb Z$. Пусть $I\leq R$ --- идеал. Рассмотрим $a$ --- наименьший по евклидовой норме элемент $I$. Покажем, что $I=\lan a\ran$. Действительно, пусть $b\in I$ и $b\ndi a$. Тогда $b=aq+r$, причём $\nu(r)<\nu(a)$. Но $r$ тоже лежит в $I$. Противоречие с минимальностью нормы для $a$.
\endproof


Из примеров евклидовых колец у нас пока есть только целые числа, но мы будем над этим работать.






\section{Китайская теорема об остатках}

\thrm[Китайская теорема об остатках] Пусть $R$ -- область главных идеалов. Рассмотрим попарно взаимно простые элементы $r_1,\dots,r_n$. Тогда для любых $a_1,\dots,a_n\in R$ система сравнений
$$ \begin{cases}
x\equiv a_1 (\mod r_1)\\
\quad \vdots \\
x\equiv a_n (\mod r_n)
\end{cases}$$
имеет единственное решение по модулю $r_1\cdots r_n$.
\ethrm
\proof Доказательство идёт индукцией по $n$. База при $n=2$. Пусть элементы $r_1$ и $r_2$ взаимно просты. Прежде всего покажем единственность. Пусть $x$ и $y$ -- два решения. Тогда $x-y \equiv 0 \mod r_1$. То есть $x-y \di r_i$. Так как $r_i$ попарно взаимно просты, то это означает, что $x-y \di r_1 r_2 $. То есть $x\equiv y \mod r_1 r_2$.
Осталось показать существование решения. Если $r_1$ и $r_2$ взаимно простые, то значит единица представляется в виде $1=sr_1+tr_2$.

Давайте посмотрим на элемент $tr_2$. Он даёт 1 по модулю $r_1$ и 0 по модулю $r_2$, то есть соответствует паре $(1,0)$. Симметрично для  $sr_1$. Если же мы хотим получить пару $(a_1,a_2)$, то подойдёт элемент $x=a_1tr_2+a_2sr_1$.

Шаг. Пусть даны элементы $r_1,\dots, r_n$. Заметим, что $r_1$ и $r_2\cdots r_n$ -- пара взаимно простых элементов. По индукционному предположению система 
$$ \begin{cases}
x\equiv a_2 (\mod r_2)\\
\quad \vdots \\
x\equiv a_n (\mod r_n)
\end{cases}$$
равносильна одному сравнению $x\equiv x_0 \mod r_2\dots r_n$ для некоторого $x_0\in R$. Но тогда мы имеет систему из двух сравнений по взаимно простым модулям $r_1$ и $r_2\dots r_n$, которая, по доказанному в базе индукции имеет единственное решение по модулю $r_1\dots r_n$.
\endproof









\section{Первые применения}

\subsection{Теорема Лагранжа}

\subsection{Функция Эйлера}

\dfn Пусть $n$ -- натуральное число. Тогда определим функцию Эйлера $\ffi(n)$ как $\ffi(n)=|\mb Z/n^*|$. 
\edfn 

Попробуем посчитать функцию Эйлера. Рассмотрим случай, когда $n=p$ -- простое число. Тогда все числа от $1$ до $p-1$ взаимно просты с $p$. Значит $\ffi(p)=p-1$.
\rm В частности, $\mb Z/p$ -- поле, если $p$ --  простое
\erm
Если число $n=p^k$ есть степень простого числа $p$, то $$\ffi(p^k)=(p-1)p^{k-1}.$$
Для доказательства заметим, что число $x$ не взаимно просто с $p^k$ тогда и только тогда, когда оно делится на $p$. Любое число $x<p^k$ делящееся на $p$ имеет вид $pl$, где $l<p^{k-1}$. Таких чисел $p^{k-1}$ штук. Итого взаимно простых чисел $p^k-p^{k-1}$. \\
Далее заметим, что если  $n$ и $m$ взаимно простые, то 
$$\ffi(nm)=\ffi(n)\ffi(m).$$ 
Действительно, по Китайской теореме об остатках паре остатков -- $a_1$ по модулю $n$ и $a_2$ по модулю $m$ соответствует единственный остаток $x_0$ по модулю $nm$. Осталось заметить, $a_1$ взаимно прост с $n$  и $a_2$ взаимно просто с $m$ тогда и только тогда, когда $x_0$ взаимно прост с $nm$. Сформулируем теорему

\thrm
Пусть $n=p_1^{\alpha_1}\dots p_k^{\alpha_k}$ натуральное число, а $p_i$ -- различные простые. Тогда
$$\ffi(p_1^{\alpha_1}\dots p_k^{\alpha_k})=(p_1-1)p_1^{\alpha_1-1}\cdots (p_k-1)p_k^{\alpha_k-1}=n\left(1-\frac{1}{p_1}\right)\dots\left(1-\frac{1}{p_k}\right).$$
\ethrm
\proof Применим предыдущее замечание несколько раз воспользовавшись тем, что $p_i^{\alpha_i}$  и $p_j^{\alpha_j}$ взаимно просты при $i\neq j$. 
\endproof

\crl[Остатки как поля] Кольцо $\mb Z/n$ является полем тогда и только тогда, когда число $n$ --- простое.
\ecrl

\thrm[Эйлера] Пусть $n$ -- натуральное число и $a\in \mb Z/n^*$. Тогда $a^{\ffi(n)}=1$.
\ethrm
\proof
Рассмотрим все обратимые классы вычетов $x_1,\dots,x_{\ffi(n)}$. Заметим, что отображение $\mb Z/n^* \to \mb Z/n^*$ заданное правилом $x\to ax$ корректно и обратимо. Действительно, так как $a$ и $x$ обратимы, то $ax$ -- тоже обратим. Для построения обратного отображения возьмём $b=a^{-1}$. Тогда аналогичное отображение, заданное правилом $x\to b x$ -- обратное для исходного. Таким образом, $ax_1,\dots,ax_{\ffi(n)}$ это все возможные обратимые классы вычетов.

Значит совпадают произведения
$$ x_1\cdots x_{\ffi(n)} = a x_1 \cdots ax_{\ffi(n)} = a^{\ffi(n)} x_1 \cdots x_{\ffi(n)} $$ 
Заметим, что общий множитель из правой и левой частей $x=x_1 \cdots x_{\ffi(n)}$ обратим, как произведение обратимых элементов. Домножив на $x^{-1}$ получаем требуемое.

\endproof


\crl[Малая теорема Ферма] Пусть $p$ простое число, и $a\in \mb Z/p$. Тогда $a^{p} = a$.
\ecrl
\proof Если $a\neq 0$, то $a$ обратим и применима предыдущая теорема. Получаем, что $a^{p-1}=1$. Домножим на $a$ и получим требуемое. Если же $a=0$, то справа и слева стоит 0. 
\endproof






\subsection{Криптографическая система RSA}
Понятие сравнения и понятие кольца вычетов были мотивированы критериями неразрешимости уравнений в целых числах. Однако и существование решений в кольце вычетов может быть полезным. В качестве примера рассмотрим задачу про извлечение корня по модулю $n$.

\lm Рассмотрим уравнение  $x^k=a$ в $\mb Z/n$. Пусть $a\in \mb Z/n^*$ и $(k,\varphi(n))=1$.
Рассмотрим $s$ ---  решение уравнения $ks+\varphi(n)t=1$. В таком случае $x^k=a (\mod n)$ тогда и только тогда, когда $x=a^s(\mod n)$.
\proof
Пусть $x^k=a$. Заметим, что элемент $x$ -- обратим. Действительно, $x^{-1}=x^{k-1}a^{-1}$. Тогда возведём обе части равенства в степень $s$. Получим 
$$a^s=x^{ks}=x^{1-\ffi(n)t}=x.$$
Обратно, если взять $x=a^s$, то
$$x^k=a^{sk}=a^{1-\ffi(n)t}=a.$$
\endproof
\elm

Обсудим, как результат этой леммы может быть применён в криптографии.

Основную задачу криптографии можно сформулировать так: передать сообщение от одного адресата другому, так, чтобы в случае доступа к каналу связи третьего человека, он не смог получить текст исходного сообщения. 

Точнее третье лицо может получить только зашифрованное сообщение, по которому, по идее, не должно иметь возможность за разумное время восстановить исходное сообщение. С другой стороны, необходимо, чтобы получатель сообщения смог бы расшифровать полученное (не умерев от нетерпения).




В 1978 в работе \href{http://people.csail.mit.edu/rivest/Rsapaper.pdf}{ R.L. Rivest, A. Shamir, L. Adleman, A Method for Obtaining Digital Signatures and Public-Key Cryptosystems } было опубликовано описание алгоритма шифрования со следующим свойством: злоумышленник, перехвативший сообщение мог его дешифровать, но за недостижимое на практике время. В свою очередь получатель сообщения мог его быстро расшифровать используя секретный ключ. Таким образом, достигалась секретность переданного сообщения.



Основная идея этой системы шифрования состоит в том, что разложение  чисел на множители является гораздо более трудоёмкой операцией, чем  умножение. Таким образом, любые параметры, которые легко посчитать, зная разложение числа на множители и сложно найти, если такого разложения не знать, могут быть использованы для передачи сообщений.

Перейдём к описанию алгоритма. Пусть есть два человека, скажем Алиса и Боб. И Боб хочет написать Алисе. Для этого Алиса должна подготовить два ключа --  секретный, который знает только она и использует для расшифровки присланных сообщений и открытый -- с помощью которого любой может зашифровать сообщение для Алисы. В нашей ситуации, открытым ключом будет пара $n,e$, а секретным ключом будет набор чисел $p,q,d$. Здесь $p,q$ --- два простых числа, $n=pq$  и  $e<n$ --- некоторое число, взаимно простое с $\ffi(n)$. Число $d<n$ удовлетворяет сравнению $de\equiv 1\mod \ffi(n)$. Заметим, что зная $p,q$ нам известно $\ffi(n)=(p-1)(q-1)$ и зная $e$ легко найти число $d$

Множество возможных сообщений это $\mb Z/n^*$. На практике в качестве сообщения выступает произвольное число от $1$ до $n-1$. Вероятность попасть в число не взаимно простое с $n$ крайне мала.  Допустим Боб хочет послать Алисе сообщение $t$. Для этого ему необходимо вычислить 
$$s=t^e\mod n.$$
Алисе же для того чтобы расшифровать сообщение необходимо найти
$$t=s^d \mod n.$$



\subsection{Тесты на простоту}
В системе RSA ключевым является выбор простых чисел $p,q$. Это должны быть большие простые числа для того, чтобы разложение числа $n=pq$ на множители заняло много времени у злоумышленника. Кроме того, это должны быть достаточно случайные числа -- если брать простые из заранее известного маленького множества, то злоумышленник может просто проверить число $n$ на делимость на все элементы этого множества. Так же никакие специальные алгоритмы разложения на множители не должны быстро работать для $n=pq$. Мы обсудим некоторые подходы к созданию таких простых чисел.

Приведём здесь простейший пример того, как проверить, что некоторое число $n$ -- простое, если известно разложение на множители числа $n-1$. Простейшие методы генерации простых чисел основаны на тесте Люка:
\thrm[Тест Люка] Пусть $n=p_1^{\alpha_1}\dots p_k^{\alpha_k}+1$. Тогда, если существует такое  $a\in \mb Z/n$, что $a^{\frac{n-1}{p_i}}\neq 1 $, но $a^{n-1} = 1$, то тогда число $n$ --- простое.
\ethrm  
Для доказательства нам понадобится некоторое общее для всех групп свойство.
\lm Пусть $G$ -- группа, $a\in G$. Пусть $a^n=1$ и $a^m=1$. Тогда $a^{(n,m)}=1$. 
\elm
\proof Напишем линейное разложение $d$ наибольшего общего делителя чисел $n$  и $m$. Имеем $d=nu+mv$. Тогда
$$a^d= a^{nu+mv}=(a^n)^u (a^m)^v=1.$$
\endproof

\proof[Доказательство теоремы]
Пусть $n$ -- не простое. Тогда $\ffi(n)<n-1$. Заметим, что $a\in \mb Z/n^*$ так как $a^{n-1} = 1$. Таким образом, по условию теоремы $a^{n-1}= 1$  и $a^{\ffi(n)}=1$  одновременно. Но тогда $a^d=1$, где $d=(n,\ffi(n))$. Заметим, что $d<n-1$  и $d|n-1$. Тогда существует простое $p_i$ из разложения $n-1$, что $n-1=p_idl$, для некоторого $l\in \mb Z$. Но тогда $a^{\frac{n-1}{p_i}}=a^{dl}=1$ в $\mb Z/n$, что противоречит одному из условий.
\endproof

Это даёт возможность строить простые числа следующим образом -- взять набор не очень больших простых чисел $p_1,\dots,p_k$, выбрать случайно степени $\alpha_1,\dots,\alpha_k$ и построить число $n=p_1^{\alpha_1}\dots p_k^{\alpha_k}+1$. Для него стоит перебрать все маленькие $a \leq \log n$ или сгенерировать несколько $a$ случайно и проверить для них  условия теста Люка. 

\fct[Докажем в следующем семестре] Если $n$ простое, то такие $a$ существуют. Более того их должно быть $\ffi(n-1)$ штук. 
\efct

Однако, для  простых чисел,  сгенерированных таким образом, есть специальный алгоритм, раскладывающий на множители их произведение. Действительно, пусть $n=pq$, где $p,q$ -- простые, такие что $p-1$ и $q-1$ раскладываются на простые множители порядка $\log n$. Прежде всего заметим, что для того чтобы посчитать $a^k 
\mod n$ достаточно знать $a^s \mod p$  и $a^s \mod q$. Заметим, что если последовательно брать $s$ вида $s=\lcm(1,\dots,k)$ или $s=k!$, то довольно скоро (для $k$ порядка $\log n$)  $a^s\equiv 1 \mod p$ и $a^s \equiv 1 \mod q$ так как $\ffi(p)=p-1$ довольно скоро будет делить $s$ и аналогично для $q$. 

Однако с большой вероятностью наименьшее такое $k$, что $a^{k!}\equiv 1 \mod p$ не будет совпадать с аналогичным $k$ для сравнения по модулю $q$. Но тогда $(a^{k!}-1,n)$ будет нетривиальным делителем $n$ для некоторого $k$ порядка $\log n$.

Поэтому на практике оказалось, что удобнее брать случайные числа из заданного большого промежутка и проверять их на простоту вероятностым способом. При такой проверке мы, вообще говоря, не сможем доказать, что прошедшие проверку числа будут простыми, но у нас будет очень большая уверенность в этом. Кроме того,  злоумышленник с большой вероятностью не сможет проверить, что мы ошиблись и взяли не простые числа.

Рассмотрим простейший пример вероятностного теста: пусть $p$ -- простое число. Тогда по малой теореме Ферма известно, что для всех $(a,p)=1$ выполнено, что $a^{p-1}\equiv 1(\mod p)$. Итого получаем, что если найти такое $a$, что $(a,n)=1$ и $a^{n-1}\nequiv 1(\mod n)$, то $n$ точно не простое. Следовательно, получаем  тест:\\
\noindent{\bf Тест Ферма}\\
1) Дано некоторое число $n$. Берём случайное число $1<a<n$.\\
2) Вычисляем $b=a^{n-1} \mod n$. Если $b\neq 1$, то выдаем, что число $n$ не простое. Иначе ничего не говорим.

\dfn Если указанный тест  проходит для простого числа $a$, то есть $a^{n-1}\equiv 1(\mod n)$, то будем говорить, что $a$ является свидетелем простоты для теста Ферма числа $n$ и $n$ -- псевдопростое по основанию $a$.
\edfn

Почему этот тест удобен? прежде всего из-за его простоты. А именно, проверка $a^{n-1}\equiv 1 \mod n$ занимает $O(\log(n))$ операций умножения в $\mb Z/n$. 

Однако есть другой вопрос: а сможем ли мы с какой-то попытки при помощи такого теста отличить простое число от непростого? С одной стороны да, потому что перебирая все числа $a$ в случае непростого числа мы в какой-то момент наткнёмся на нетривиальный делитель. С другой стороны понятно, что делителей числа $n$ мало. Поэтому немного поменяем вопрос: верно ли, что для всякого непростого числа $n$ существует $(a,n)=1$, что $a$ не свидетель простоты $n$? Оказывается, что это не всегда так.

\utv Любое число, взаимно простое с числом 561 является свидетелем простоты числа 561. При этом число $561$ раскладывается на множители $561=3\cdot 11\cdot 17$.
\eutv

\dfn Составное число $n$ называется числом Кармайкла или псевдопростым числом, если $\forall a$, что $(a,n)=1$ выполнено $a^{n-1}\equiv 1(\mod n)$.
\edfn



\fct Чисел Кармайкла бесконечно много. \href{http://www.dms.umontreal.ca/~andrew/PDF/carmichael.pdf}{W.R. Alford, Andrew Granwill, Carl Pomerance, There are infinitely many Carmichael numbers}
\efct

Можно немного усовершенствовать этот тест: заметим, что во время быстрого возведения в степень мы сначала считаем $a^{\frac{n-1}{2}} $ в кольце $\mb Z/n$, а потом возводим это число в квадрат. Можем ли мы что-то конкретное сказать про $x=a^{\frac{n-1}{2}}$ когда $n$ -- простое. Да, сможем. А именно, для всякого $0\neq a \in \mb Z/n$ $x^2=a^{n-1}\equiv 1 \mod n$.
\lm Если $n$ -- простое число, то единственными решениями $x^2=1$ в кольце $\mb Z/p$ являются $x=\pm 1$.
\elm
\proof Распишем на языке сравнений. Сравнение $x^2\equiv 1 \mod p $ равносильно условию $x^2-1=(x-1)(x+1)\di p$. Но тогда либо $x+1\di p$, то есть $x\equiv -1 \mod p$, либо $x\equiv 1 \mod p$.
\endproof

Это приводит нас к следующему тесту на простоту для целых чисел:

\noindent{\bf Тест Эйлера}\\
1) Дано некоторое число $n$. Берём случайное число $1<a<n$.\\
2) Вычисляем $b=a^{(n-1)/2} \mod n$. Если $b\neq \pm 1$, то выдаем, что число $n$ не простое. Иначе ничего не говорим.




Тест Эйлера так же вычислительно прост. Однако он обладает и тем же недостатком. А именно, имеются псевдопростые для данного теста числа. Например, любое число, взаимно простое с  $1729=7\cdot 13\cdot 19$ подтверждает, что $1729$ простое.\\



Однако, можно пойти дальше. Представим число  $n-1$ в виде $n-1=m2^s$, где $m$ -- нечётное. в процессе вычисления $a^{n-1}$ при быстром возведении в степень мы вычисляем сначала $a^m \mod n$, а потом и числа $a^{2^r m}\mod n$ где $r<s$. Посмотрим, что мы можем  сказать про эти числа.

\thrm
Пусть $n$~--- простое число, обозначим $n-1=2^s\cdot m$, где $m$ нечётно. Тогда для любого $a\in\mb Z/n^*$ выполнено хотя бы одно из двух условий
\begin{itemize}
\item 
$a^m=1$;
\item
существует $0\leqslant r<s$ такое, что $a^{2^rm}=-1$.
\end{itemize}
\ethrm
\proof Пусть $l \geq 0$ -- наименьшее такое, что $a^{m2^l}= 1$. Такое $l$ есть, потому что $a^{m2^s}=a^{n-1}=1$. В частности $l\leq s$ Если $l=0$, то $a^m=1$, что подходит для нас. Иначе рассмотрим $x=a^{m2^{l-1}}$. Как и раньше имеем $x^2=1$. Но тогда по лемме $x=- 1$ ($x=1$ не подходит благодаря выбору $l$). Осталось взять $r=l-1$.
\endproof



\noindent{\bf Тест Миллера-Рабина} \\
1) Представим $n-1=2^sm$, где $m$ нечётно.\\
2) Берём случайное число $1<a<n$. Вычисляем $a^m \mod n$. Если $a^{m}\equiv \pm 1\mod n$, то $n$ вероятно простое. Иначе переходим к следующему шагу.\\
3) Для всех $1\leq r<s$ смотрим, верно ли что $a^{m2^r}\equiv -1 \mod n$. Если это так, то $n$ вероятно простое. Если такого $r$ -- нет, то $n$ -- составное.\\

\dfn Будем называть число $a\in\mb Z/n$ {\it свидетелем простоты} числа $n$, если число $n$ проходит описанный тест по основанию $a$. Число $n$ в этом случае называется сильно псевдопростым по основанию $a$.
\edfn

Тест Миллера-Рабина был придуман \href{https://www.cs.cmu.edu/~glmiller/Publications/Papers/Mi76.pdf}{Гэри Миллером}, как детерминированный тест на простоту. Предполагалось, что с помощью этого теста надо проверить первые $O(n^{1/7})$ чисел $a$. Если все эти $a$ были свидетелями простоты $n$, то по теореме, доказанной Миллером $n$ должно быть простым.

Так же Миллер получил следующий результат: в предположении обобщённой гипотезы Римана, если все числа $a\leq C \log^2 n $ являются свидетелями простоты числа $n$ для указанного теста, то $n$ простое. Позднее было показано, что в качестве константы $C$ можно взять 2. Большинство математиков верят, что обобщённая гипотеза Римана верна.

В свою очередь, Михаэль Рабин в статье \href{https://ac.els-cdn.com/0022314X80900840/1-s2.0-0022314X80900840-main.pdf?_tid=e8201658-e83e-11e7-9793-00000aab0f26&acdnat=1514074434_a1f0bbeeacc3f326abc83943e330aa13}{Probabilistic algorithm for testing primality} рассмотрел вероятностную версию этого алгоритма показав её эффективность с помощью следующей теоремы:


\thrm(Рабин) 
Для любого $n$ -- нечётного составного, $n>9$, существует не более $\frac{\ffi(n)}{4}$ свидетелей его простоты.
\ethrm

Таким образом, вероятность, что случайно выбранное число $a\in\mb Z/n$ является свидетелем простоты числа $n$ меньше $\frac14$. Если же  выбрать случайное число $a$ $k$ раз подряд, и каждый раз оно будет  свидетелем простоты числа $n$, то вероятность того, что $n$~--- составное, меньше $\frac{1}{4^k}$.









\chapter{Многочлены}



\section{Кольцо многочленов от одной переменной}


Теперь настало время разобрать второй по важности, после целых чисел пример евклидового кольца, а следовательно, и области главных идеалов --- кольцо многочленов над полем. Для начала разберём немного ситуацию не только над полем.

\dfn[Степень многочлена] Пусть $R$ -- кольцо, а $f=a_0+a_1x+\dots+a_nx^n$ ненулевой многочлен в $R[x]$. Положим степень $\deg f = \max_{a_n\neq 0} n$ -- номеру наибольшего ненулевого коэффициента. Будем говорить, что $a_{\deg f}$ --- старший коэффициент многочлена $f$. Если $f=0$ положим $\deg f= -\infty$.
\edfn

\rm В дальнейшем будем подразумевать, что $-\infty+m=-\infty$ для всех $ m \in \mb Z$.
\erm

\lm[Коэффициент при старшей степени] Пусть $R$ --- область целостности, а $f$ и $g$ два  многочлена из $R[x]$ степени $n$ и $m$. Тогда степень $\deg fg = \deg f +\deg g$. Более того, если $f$ и $g$ ненулевые и их старшие коэффициенты это $a_n$ и $b_m$, то старший коэффициент $fg$ равен $a_nb_m$.
\elm
\proof Случай когда один из многочленов нулевой тривиален. Пусть оба они ненулевые. Рассмотрим коэффициент $c_k$ при степени $k>n+m$ в произведении $fg$. Тогда $c_k=\sum_{i+j=k}a_ib_j$. Но каждое слагаемое есть 0, так как содержит нулевой сомножитель. Тогда $c_k=0$. С другой стороны в коэффициенте $c_{n+m}$ есть ровно одно ненулевое слагаемое $a_nb_m$. Тогда $c_{n+m}=a_nb_m\neq 0$. Следовательно степень $fg$  есть $n+m$.
\endproof

\crl[Кольцо многочленов --- область целостности] Пусть $R$ --- область целостности. Тогда $R[x]$ --- область целостности.
\proof Если $f$ и $g$ не равны 0, то $\deg fg =\deg f+ \deg g\neq - \infty$, то есть произведение не 0.
\endproof
\ecrl



\lm Пусть $R$ -- область целостности. Тогда $R[x]^*=R^*$.
\elm 
\proof  Пусть так случилось, что $fg=1$. Тогда степень $\deg f+\deg g=\deg 1=0$. Отсюда $\deg f=\deg g=0$. То есть оба они константы из $R$. Тогда указанное равенство $fg=1$ верно в $R$, то есть $f$ обратим в $R$.
\endproof

\rm Если в $R$ есть нильпотенты, то это вообще говоря не так. Вот пример: $R=\mb Z/p^2$, где $p$ -- простое. Тогда многочлены $1+px$ и $1-px$ обратны друг другу.
\erm

\rm Заметим, что, хотя для степени суммы равенства нет,  тем не менее можно написать неравенство
$$\deg (f+g) \leq \max(\deg f, \deg g).$$
\erm

\thrm[Деление с остатком в кольце многочленов] Пусть $R$ --- область целостности. Рассмотрим два многочлена $f,g \in R[x]$, причём старший коэффициент $g$ обратим. Тогда существуют единственные $q,r \in R[x]$ такие, что:\\
$$f=qg+r \text{ и } \deg r < \deg g.$$
\ethrm
\proof Сначала докажем единственность. Пусть $f=q_1g+r_1=q_2g+r_2$, $\deg r_1 < \deg g$ и  $\deg b_1 < \deg g$. Тогда $(r_2-r_1)=(q_1-q_2)g$. Если $q_1\neq q_2$, то  $(q_1-q_2)g$ большей степени, чем $r_2-r_1$, что противоречит равенству Значит $q_1=q_2$. Но тогда $r_1=r_2$.

Для доказательства существования приведём индукционный алгоритм построения. Индукция по степени $f$. Если $\deg f<\deg g$, то $r=f$, $q=0$. Пусть $f=a_nx^n+\dots+a_0$, а $g=b_mx^m+\dots + b_0$, $m\leq n$. Рассмотрим разность $$f_1=f-\tfrac{a_n}{b_m}x^{n-m}g.$$
Это многочлен степени меньшей $\deg f$. Тогда $f_1=q_1g+r_1$. Возьмём $q=q_1+\tfrac{a_n}{b_m}x^{n-m}$ и $r=r_1$.
\endproof






Напомню, что главной целью этого параграфа было введение нового важного примера евклидового кольца.

\thrm
Пусть $K$ -- поле. Тогда кольцо многочленов $K[x]$ является евклидовым кольцом с нормой -- $f \to \deg f$.
\proof Если $K$ поле, то старший коэффициент любого ненулевого многочлена обратим. Осталось воспользоваться теоремой о делении с остатком.
\endproof
\ethrm

А что вообще значат вопросы делимости в кольце многочленов?

\lm \label{remainder} Пусть $R$ -- область целостности, $a\in R$, $f\in R[x]$. Тогда $f\di (x-a)$ тогда и только тогда, когда $a$ корень
многочлена $f$.
\elm
\proof Поделим многочлен $f$ на $(x-a)$ с остатком $f=(x-a)q+r$. Многочлен $r$ --- какая-то константа из $R$. Тогда $f(a)=0+r=r$. C одной стороны, $r=0$ тогда и только тогда, когда $f \di (x-a)$, с другой стороны, тогда и только тогда когда $f(a)=0$.
\endproof

Самым, пожалуй, важным фактом, следующим из однозначности разложения в кольце многочленов, является оценка на число его корней.

\thrm Пусть $R$ -- область целостности, $0\neq f\in R[x]$. Тогда у $f(x)$ не более $\deg f$ корней. 
\ethrm
\proof Пусть $\lambda_1,\dots,\lambda_k \in R$ -- набор всех корней многочлена $f(x)$. Тогда $f(x)=(x-\lambda_1)g(x)$. Заметим, что $g(x)$ имеет $\lambda_2,\dots,\lambda_n$ своими корнями. Действительно $0=f(\lambda_i)=(\lambda_i-\lambda_1)g(\lambda_i)$. Если $i\geq 2$, то $\lambda_i-\lambda_1 \neq 0$ и тогда $g(\lambda_i)=0$. Продолжая так далее получим разложение $f(x)=(x-\lambda_1)\dots(x-\lambda_k)h(x)$. Сравнивая степени имеем $\deg f = k + \deg h$. Отсюда $\deg f \geq k$, что и требовалось.
\endproof


\dfn[Кратность корня] Пусть $R$ -- область целостности. Будем говорить, что многочлен $f\in R[x]$ имеет $a\in R$ корнем кратности $k$, если
$$f\di(x-a)^k \text{ и } f\ndi(x-a)^{k+1}.$$
\edfn



\dfn Пусть $R$ -- область целостности, $f\in R[x]$. Пусть $\lambda_1, \dots, \lambda_k$ -- все корни $f$ в $R$, причём их кратности равны $r_1,\dots,r_k$. Тогда число корней с учётом кратностей это $r_1+\dots+r_k$. 
\edfn

\rm Если $K$ -- поле, то многочлены $x-a$ и $x-b$ неприводимы в $K[x]$ и взаимно просты.
\erm

\thrm У многочлена $f\neq 0$ в поле не более чем $\deg f$ различных корней с учётом кратности.
\ethrm 
\proof Разложим $f(x)$ на неприводимые множители
$$f(x)=(x-\lambda_1)^{r_1}\dots (x-\lambda_k)^{r_k}\prod_{\deg p_i\geq 2} p_i^{\alpha_i}.$$
Сравнивая степени справа и слева получаем $\deg f=r_1+\dots+r_k+h$, где $h\geq 0$. Осталось заметить, что $r_1+\dots+r_k$ и есть число корней с учётом кратности.
\endproof

\lm Пусть $f,g \in K[x]$ многочлены степени меньше или равной $n$. Если есть $n+1$ точка $x_0,\dots,x_n$, что $f(x_i)=g(x_i)$, то $f(x)=g(x)$. 
\elm
\proof Рассмотрим разность $f-g$. Это многочлен степени не более $n$, но у него $n+1$ корень. Такое бывает только если $f-g=0$, то есть если $f=g$.
\endproof

\thrm[О формальном и функциональном равенстве многочленов] Пусть $K$ --- бесконечное поле. Тогда два многочлена равны в том и только том случае, когда их значения для всякой точки из $K$ равны.
\ethrm
\proof Пусть $f,g\in K[x]$  и $n$ -- это максимум из их степеней. Тогда рассмотрим $n+1$ точку $\lambda_0,\dots,\lambda_n \in K$. Тогда по условию они равны в этих точках и, следовательно, равны по лемме
\endproof


Приведём пример использования однозначности разложения на множители в кольце многочленов.

\utv Пусть $f\in K[x]$  многочлен степени $n$, и у него есть ровно $n$ различных корней $x_1,\dots,x_n$. Тогда 
$$f(x)=c \prod_{i=1}^n (x-x_i),$$
где $c$ -- это старший коэффициент $f(x)$.
\eutv
\proof Если $x_i$ -- различные корни $f(x)$, то как мы отмечали,  $f(x) \di \prod_{i=1}^n(x-x_i)$. То есть $f(x) = g(x)\prod_{i=1}^n(x-x_i)$. Но тогда дополнительный множитель $g(x)$ -- это константа. Осталось сравнить старшие коэффициенты слева и справа.
\endproof

\thrm[Теорема Вильсона] Натуральное число $p$ простое тогда и только тогда, когда $(p-1)! \equiv -1(\mod p)$.
\ethrm
\proof Если $p$ не простое, то есть $a<p$ --- нетривиальный делитель $p$ и следовательно делитель $0$ в $\mb Z/p$. Тогда произведение $(p-1)!$ тоже делитель 0.
Если $p$ простое, то два многочлена $x^{p-1}-1$  и $(x-1)\dots(x-(p-1))$ имеют $p-1$ общих корней в $\mb Z/p$ (по малой теореме Ферма)и одинаковый старший коэффициент. Но тогда они равны. Следовательно, равны и их свободные члены. С одной стороны это $(-1)$ с другой -- это $(p-1)!$.
\endproof




\dfn[Кольцо многочленов от нескольких переменных] Пусть $R$ — кольцо. Кольцом многочленов от $n$ переменных $x_1, \dots, x_n $ определяется индуктивно как $R[x_1,\dots,x_{n-1},x_n]=R[x_1,\dots,x_{n-1}][x_n]$.
\edfn



У нас тут возникли многочлены от нескольких переменных и настала пора разобраться с их записью.
\dfn Набор $\alpha=(\alpha_1,\dots,\alpha_n) \in (\mb N\cup\{0\})^{\times n}$ называется мультииндексом.
\edfn

\dfn Пусть $\alpha=(\alpha_1,\dots,\alpha_n)$ -- мультииндекс. Обозначим многочлен $x_1^{\alpha_1}\dots x_n^{\alpha_n}$ за $x^{\alpha}$.
\edfn

\rm Пусть $f(x_1,\dots,x_n) \in R[x_1,\dots,x_n]$. Тогда существуют единственный набор коэффициентов $a_{\alpha}\in R$, что $f(x_1,\dots,x_n)=\sum_{\alpha} a_{\alpha}x^{\alpha}$. Более того, такой набор коэффициентов должен обладать свойством, что все $a_{\alpha}$ кроме конечного числа равны 0. Обратно, если есть такой набор коэффициентов, то он задаёт многочлен $f(x_1,\dots,x_n)=\sum_{\alpha} a_{\alpha}x^{\alpha}$. Элементы $a_{\alpha}$ называются коэффициентами $f$. Многочлены вида $ax^{\alpha}$ называются мономами.
 \erm




 

Обсудим алгоритмический аспект вычисления значения многочлена в точке. А именно, с наскока может показаться, что для вычисления выражения $f(x)=a_0+a_1x+\dots+a_nx^n$ при подстановки $x=\lambda$ может понадобится $O(n^2)$ умножений. Однако, сразу видно, что можно сэкономить --- а именно возводя $\lambda$ в степень последовательно можно запоминать результаты вычислений и не считать их второй раз. Это даёт $2n-2$ умножение и $n$ сложений. Тем не менее для реализации указанного способа необходимо хранить некоторую информацию в памяти. Ещё более совершенный способ даёт схема Горнера. 

Представим выражение $a_n\lambda^n+\dots+a_1\lambda+a_0$ в виде $(\dots((a_n\lambda+a_{n-1})\lambda+a_{n-2})\dots)\lambda+a_0$. Определим элементы $b_i$ по правилу $b_i=b_{i+1}\lambda+a_i$, начиная с $b_n=a_n$. Тогда $b_0=f(\lambda)$. Видно, что при таком подходе используется только $n$ умножений и $n$ сложений. Однако, это ещё не всё:

\lm Числа $b_i$ в схеме Горнера являются коэффициентами неполного частного при делении $f(x)$ на $x-\lambda$. 
\elm
\proof
Пусть $f=(x-\lambda)q+f(\lambda)$, где $q(x)=c_nx^{n-1} +\dots +c_1$. Вначале заметим, что старший коэффициент неполного частного $c_n$ обязан быть равен $b_n=a_n$. Сравним теперь коэффициенты при степени $i$ справа и слева. Получаем $a_{i}=c_{i}-\lambda c_{i+1}$, то есть $c_i=c_{i+1}\lambda +a_{i}$ удовлетворяет той же рекуррентной формуле, что и $b_i$
\endproof

Оптимальность схемы Горнера и ещё много чего было доказано в 1966 году Виктором Паном в работе <<\href{http://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=rm&paperid=5823&option_lang=rus}{О способах вычисления значений многочленов}>>. Там же можно найти метод позволяющий сэкономить ещё половину операций, если разрешены предвычисления.




\section{Неприводимые многочлены над $\R$ и $\C$}

Многочлен с коэффициентами в поле однозначно раскладывается на неприводимые множители. Множителям первой степени соответствуют корни. Однако бывают и множители большей степени. Работать с ними часто не удобно. Гораздо приятнее считать, что они раскладываются на линейные множители, но над большим полем. 

\rm Многочлен степени 2 и 3 с коэффициентами в поле $K$ неприводим в $K[x]$ тогда и только тогда, когда он не имеет корней в $K$.
\erm

\exm\\
1) Многочлен $x^2-2$ неприводим, как многочлен в $\mb Q[x]$, но приводим как многочлен с вещественными коэффициентами.\\
2) Многочлен $x^3-x+1$ неприводим, как многочлен с рациональными коэффициентами.\\
3) Многочлен степени 3 из $\mb R[x]$ не может быть неприводимым.\\
4) Многочлен $x^2+1$ неприводим в $\mb R[x]$.\\


Посмотрим в этой связи на следствия основной теоремы алгебры.

\crl Пусть $0\neq f(x) \in \mb C[x]$ -- многочлен. Тогда $f(x)$ раскладывается на линейные множители. В частности,  у любого многочлена $f$ количество комплексных корней с учётом кратности равно его степени.
\ecrl
\proof Индукция по степени $f(x)$. Пусть $\lambda$ -- корень $f(x)$. Тогда $f(x)=(x-\lambda)g(x)$. Но тогда $g(x)$ раскладывается на линейные множители по индукционному предположению, а значит и $f(x)$.
\endproof

Посмотрим, что же получается для вещественных чисел.

\lm Пусть $\lambda \in \mb C$ корень $f(x)\in \mb R[x]$. Тогда $\ovl{\lambda}$ -- тоже корень $f(x)$.
\elm 
\proof Достаточно воспользоваться леммой про гомоморфизм и корень.
\endproof

\lm Пусть $f(x)\in \mb R[x]$ имеет комплексный корень $\lambda \notin \mb R$, тогда $f(x)\di(x-\lambda)(x-\ovl{\lambda})$ в $\mb R[x]$.
\proof Если $\lambda \notin \mb R$, то $\ovl{\lambda}\neq \lambda$ тоже корень $f$. Тогда $f(x)\di(x-\lambda)(x-\ovl{\lambda})$ в $\mb C[x]$. Заметим, что последний многочлен -- вещественный и неприводимый над $\mb R$, так как не имеет вещественных корней. Обозначим его $r(x)=(x-\lambda)(x-\ovl{\lambda})$. Предположим, что $f(x)\ndi r(x)$ в $\mb R[x]$. По неприводимости $r(x)$ получаем, что $(f(x),r(x))=1$. Рассмотрим линейное разложение НОД-а: $u(x)f(x)+g(x)r(x)=1$. Подставим $x=\lambda$. Правая часть равна 0, а левая 1. Противоречие. Значит $f(x)\di r(x)$, что и требовалось. 
\endproof
\elm

\thrm Любой неприводимый многочлен над $\mb R$ либо линеен, либо является многочленом второй степени с отрицательным дискриминантом.
\ethrm
\proof Рассмотрим неприводимый над $\mb R$ многочлен $p(x)$. У него есть комплексный корень $\lambda$. Если $\lambda \in \mb R$, то $p(x)\di (x-\lambda)$ и по неприводимости $p(x)=x-\lambda$.  Если $\lambda \notin \mb R$, то $p(x)\di (x-\lambda)(x-\ovl{\lambda})$ в $\mb R[x]$. Этот многочлен не имеет вещественных корней и, следовательно, имеет отрицательный дискриминант. Обратно, любой линейный многочлен и квадратичный многочлен с отрицательным дискриминантом неприводимы над $\mb R$.
\endproof

Доказательство даёт пример того, как получить разложение вещественного многочлена на вещественные же неприводимые множители -- для этого достаточно найти все комплексные корни, разбить их на пары сопряжённых друг другу и сгруппировать комплексные множители по этим парам.

Посмотрим пример: разложим на неприводимые вещественные множители многочлен $x^4+1$. Его комплексные корни -- это числа $\frac{\pm 1\pm i}{\sqrt{2}}$. Посмотрим на первую пару корней  -- это $x_1=\frac{1+i}{\sqrt{2}}$ и $x_2=\frac{1-i}{\sqrt{2}}$. Перемножив $(x-x_1)(x-x_2)$ получаем $x^2-\sqrt{2}x+1$. Для оставшейся пары получаем $x^2+\sqrt{2}x+1$. Итого
$$x^4+1=(x^2-\sqrt{2}x+1)(x^2+\sqrt{2}x+1).$$






\section{Производная}

Нам понадобится понятие производной многочлена. Оказывается, что это определение можно дать над любым кольцом. 

\dfn Пусть $R$ -- кольцо, а $f\in R[x] $ имеет вид $f(x)=a_0+\dots+a_nx^n$. Тогда определим производную как $f'(x)=a_1+2a_2x+\dots+na_nx^{n-1}$.
\edfn

Проверим, что все основные свойства производной верны в этом контексте.

\thrm[Свойства] Пусть $f(x),g(x) \in R[x]$. Тогда:
\begin{itemize}
\item $(f(x)+g(x))'= f'(x)+g'(x)$
\item $(\lambda f(x))'=\lambda f'(x)$ для всех $\lambda \in R$.
\item  $(f(x)g(x))'=f'(x)g(x)+f(x)g'(x)$
\item $(f^n(x))'=nf'(x)f^{n-1}(x)$.
\item $(f(g(x)))'=g'(x)f'(g(x))$.
\end{itemize}
\ethrm
\proof Первое и второе свойства понятны. Перейдём к третьему свойству. Докажем его сначала для мономов:
$$(x^{n+m})'=(n+m)x^{n+m-1}=nx^{n-1}x^{m}+mx^{m-1}x^n=(x^n)'x^m+(x^{m})'x^n.$$
Теперь пусть $f=\sum a_i x^i$, а $g(x)=\sum b_j x^j$ -- произвольные. Тогда 
$$(fg)'= 
\sum_{i,j} a_ib_j (x^{i+j})'= \sum a_ib_j (x^i)'x^j + \sum a_ib_j x^i(x^j)'=f'g+g'f$$ 
Покажем четвёртое свойство индукцией по $n$. База при $n=1$ тривиальна. Дальше имеем 
$$(f^n)'=(f\cdot f^{n-1})'=f'f^{n-1}+f(n-1)f'f^{n-2}=nf'f^{n-1}.$$
Теперь покажем последнее свойство. Распишем $f(x)=\sum a_i x^i$. Тогда $$(f(g(x)))'=\sum a_i (g^i(x))'= \sum i a_i g^{i-1}(x) g'(x)= g'(x) f'(g(x))$$


\endproof

\thrm Пусть $R$ -- кольцо. Если многочлен $f(x) \in R[x]$ делится на $g(x)^l$ то $f'(x) \di g(x)^{l-1}$. Более того, если $R=K$ -- поле, $g(x)$ неприводим, $\chr K > \deg f$ или $\chr K=0$, а $f(x) \ndi g(x)^{l+1}$, то $f'(x)\ndi g(x)^{l}$.
\ethrm
\proof Пусть $f(x)=g(x)^lh(x)$. Тогда $$f'(x)=lg(x)^{l-1}g'(x)h(x)+g(x)^lh'(x)=g(x)^{l-1}(lg'(x)h(x)+g(x)h'(x)) \di g(x)^{l-1}.$$
Пусть теперь $f(x) \ndi g(x)^{l+1}$, многочлен $g(x)$ неприводим и выполнено условие на характеристику. Наша задача доказать, что последний сомножитель в разложении $f'$ взаимнопрост с $g(x)$ при указанных условиях. Надо показать, что $lg'(x)h(x)\ndi g(x)$. Благодаря неприводимости $g(x)$  и тому что $f(x)\ndi g^{l+1}(x)$ получаем, что $h(x)$ и $g(x)$ взаимно просты.   Тогда вопрос сводится к взаимной простоте $lg'(x)$ и $g(x)$. Заметим, что если $lg'(x)$ не 0, то он действительно взаимнопрост с $g(x)$ так как $g(x)$ неприводим, а $lg'(x)$ имеет заведомо меньшую степень. 

Итого пусть $lg'(x)=0$. Понятно, что в условиях теоремы $l\neq 0$ в $K$. Следовательно, надо рассмотреть ситуацию, когда $g'(x)=0$.
Если $\chr K =0$, то и $g'(x)\neq 0$. Если $\chr K =p>\deg f$, то заметим, что $\deg f \geq \deg g=s$. В этом случае, старший член производной $g'(x)$ будет иметь вид $sa_s$, и, следовательно, не равен 0 так как $s\leq \deg f < \chr K$. 
\endproof


\crl Пусть $K$ --- поле, $f\in K[x]$ многочлен степени $n$ и $\chr K = 0$ или $\chr K>n$. Пусть $a$ корень $f$. Тогда кратность  $a$ как корня $f$ равна $l$ тогда и только тогда, когда кратность корня $a$ у $f'$ равна $l-1$.
\ecrl

\crl Пусть $K$ --- поле, $f\in K[x]$ многочлен степени $n$ и $\chr K = 0$ или $\chr K>n$. $a$ -- корень кратности $l$ многочлена $f(x)$ тогда и только тогда, когда $0=f(a)=f'(a)=\dots=f^{(l-1)}(a)$ и $f^{(l)}(a)\neq 0$.
\ecrl

\rm Это утверждение можно доказать когда $K$ -- область целостности, что мы и сделали на паре.
\erm

В условиях теоремы оказывается возможным избавиться от всех кратных корней (а на самом деле и от всех кратных множителей $f$). Точнее

\crl Пусть $K$ --- поле, $f(x)\in K[x]$ многочлен степени $n$ и $\chr K = 0$ или $\chr K>n$. Тогда многочлен $\frac{f(x)}{\Nod(f(x),f'(x))}$ имеет те же неприводимые множители, что и $f(x)$. При этом каждый  неприводимый множитель $f$ входит с кратностью $1$.
\ecrl

\noindent {\bf Пример:} Вот пример многочлена, у которого проблемы с кратностью корня у производной: $x^{p}$ в $\mb Z/p$. Его производная равна 0. И кратность корня -- бесконечность. Это несколько экзотичная ситуация. Для примера менее экзотичной ситуации посмотрим на многочлен $x^p(x-1) \in \mb Z/p[x]$. Его производная это $x^p$ имеет такую же кратность корня $x=0$, как и исходный многочлен.



Посмотрим, как связана производная и коэффициенты многочлена. Для этого посмотрим вот на какой вопрос. Рассмотрим простейший многочлен $x-a \in R[x]$. Можно ли разложить многочлен по степеням $x-a$? Ответ да, можно.

\utv Пусть $R$ -- кольцо, $a\in R$  и $f(x)\in R[x]$ степени $n$. Тогда существуют и единственные коэффициенты $a_0,\dots, a_n$, что $$f(x)=a_0+a_1(x-a)+\dots+a_n(x-a)^n.$$
\proof Рассмотрим отображение $R[x] \to R[y]$ переводящее $f(x)\to f(y+a)$. Пусть $f(y+a)=a_0+\dots+a_n y^n$. Подставим $y=x-a$. Тогда $f(x)=a_0+\dots+a_n(x-a)^n$. Таким образом, мы показали существование.
Для доказательства единственности заметим, что $f(x)=a_0+\dots+a_n(x-a)^n$, то необходимо при подстановке $x\to y+a$ этот многочлен перейдёт в $f(y+a)=a_0+\dots+a_ny^n$. Но коэффициенты $a_i$ в такой записи определены однозначно.
\endproof
\eutv



Вопрос состоит в том, как найти эти коэффициенты в разложении по степеням $x-a$?

\thrm[Формула Тейлора для многочленов]
Пусть $f(x)$ многочлен в $R[x]$, где $R$ -- область целостности и  $\chr R =0$, или $\chr R > \deg f=n$. Тогда имеет место формула
$$f(x)= f(a)+f'(a)(x-a)+\frac{f''(a)}{2}(x-a)^2+  \dots + \frac{f^{(n)}(a)}{n!}(x-a)^n.$$
\proof По предыдущей теореме у нас есть разложение
$$f(x)=a_0+a_1(x-a)+\dots+a_n(x-a)^n.$$
Возьмём $k$-ую производную от обеих частей равенства. Получим
$$ f^{(k)}(x)=k!a_k+(k+1)!a_{k+1}(x-a)+\dots+\frac{n!}{k!}a_n(x-a)^{n-k}.$$
Осталось подставить $x=a$.
\endproof
\ethrm







\subsection{Дополнительно: Метод Ньютона и лемма Гензеля} 

Условие, что у многочлена нет кратных корней, является важным для того, чтобы эффективно искать корни многочленов. В частности, для метода Ньютона приближённого поиска корней многочлена. Опишем этот метод.


Пусть $f$ -- многочлен без кратных корней (можно и с кратными корнями, но тогда метод может плохо работать). Рассмотрим некоторую точку $x_0$. Это нулевое приближение к корню. Построим следующее приближение. Можно посмотреть на $f(x)$ около точки $x_0$ 
$$f(x)=f(x_0)+f'(x_0)(x-x_0)+\dots.$$
и заменить $f(x)$ на свое <<линейное приближение>>.
Возьмём $x_1$ равным корню уравнения $0=f(x_0)+f'(x_0)(x-x_0)$. Его легко найти. Он равен $x_1=x_0-\frac{f(x_0)}{f'(x_0)}$. Поступая так и далее получим последовательность $x_n$ заданную правилом $$x_{i+1}=x_i-\frac{f(x_i)}{f'(x_i)}.$$

Таким образом, мы построили последовательность $x_n$, про которую мы надеемся, что она сходится к корню многочлена $f(x)$. Покажем, что так действительно бывает.

\thrm Пусть $f(x)\in K[x]$ (где $K=\mb R$ или $\mb C$). Пусть $\hat{x}\in K$ -- корень $f(x)$ кратности $1$. Тогда существует такая положительная $C>0$, что для любого $x_0 \in K$, такого что $|\hat{x}-x_0|<C$ последовательность точек метода Ньютона сходится к $\hat{x}$.
\ethrm
\proof Напишем оценку для $|\hat{x}-x_{i+1}|$ через $|\hat{x}-x_i|$. Имеем 
$$|\hat{x}-x_{i+1}|=\left|\hat{x}-x_i+\frac{f(x_i)}{f'(x_i)}\right|=\left|\frac{f'(x_i)(\hat{x}-x_i)+f(x_i)}{f'(x_i)}\right|$$
Распишем значение многочлена $f(x)$ в точке $\hat{x}$ используя разложение по степеням $x-x_i$. 
$$0=f(\hat{x})= f(x_i)+f'(x_i)(\hat{x}-x_i)+\frac{f''(x_i)}{2}(\hat{x}-x_i)^2 + (\hat{x}-x_i)^3 g(x_i)$$
Здесь хвост в разложении по Тейлору заменён на $(\hat{x}-x_i)^3 g(x_i)$ для некоторого многочлена $g(x)$. Начало этой суммы есть в точности числитель дроби. Тогда
$$|\hat{x}-x_{i+1}|=\left|\frac{\frac{f''(x_i)}{2}(\hat{x}-x_i)^2 + (\hat{x}-x_i)^3 g(x_i)}{f'(x_i)}\right|=|\hat{x}-x_i|^2\left|\frac{\frac{f''(x_i)}{2} + (\hat{x}-x_i) g(x_i)}{f'(x_i)}\right|$$

Заметим, что так как $\hat{x}$ не кратный корень, то $f'(\hat{x})\neq 0$. Значит существует константа $C_1>0$, что  $|f'(x)|>D_1>0$ для всех $x$, таких что $|\hat{x}-x|<C_1$ как и любая уважающая себя непрерывная функция. Вторая производная многочлена $f$ ограничена при $|\hat{x}-x|<C_1$, то есть $|f''(x)|<D_2$ для некоторого $D_2 \geq 0$ (так как многочлен -- непрерывная функция). Слагаемое же $|(\hat{x}-x) g(x)|<D_3$  ограничено при $|\hat{x}-x|<C_1$ по тем же причинам. 
Значит при $|\hat{x}-x_i|<C_1$
$$|\hat{x}-x_{i+1}|\leq |\hat{x}-x_i|^2 \frac{D_2+D_3}{D_1}= D |\hat{x}-x_i|^2.$$
Если, скажем, $|\hat{x}-x_i|<\frac{1}{2D}$, то $|\hat{x}-x_{i+1}|<\frac{1}{2}|\hat{x}-x_i|$. То есть расстояние уменьшилось по крайней мере вдвое. Отсюда следует, что выбрав $x_0$, такое что $|\hat{x}-x_0|<\min (C_1,\frac{1}{2D})$ расстояние до $\hat{x}$ на каждом шаге итерации будет уменьшаться вдвое, что и приведёт к сходимости последовательности $x_i$ к точке $\hat{x}$.
\endproof

На самом деле неравенство $|\hat{x}-x_{i+1}|<D|\hat{x}-x_i|^2$ означает, что для достаточно близких к $\hat{x}$ точек, число совпадающих знаков после запятой у $\hat{x}$ и $x_{i+1}$ примерно вдвое больше, чем у $\hat{x}$ и $x_i$. Это гораздо сильнее, чем уменьшение расстояния в два раза, что гарантирует на каждом шаге только один новый точный знак (в двоичной системе).

Так же видно, что если $\hat{x}$ кратный корень, то такая хорошая оценка невозможна.

К сожалению, достаточно близкая точка -- это далеко не любая точка. Мы  заранее не сможем предсказать для данной точки $x_0$ сойдётся ли последовательность из метода Ньютона, а если сойдётся, то к корню в каком интервале.

В качестве примера того, что точка может привести к несходящейся последовательности из метода Ньютона, легко привести многочлен и точку $x_0$, которая вызывает зацикливание. Например, рассмотрим такой многочлен $f(x)$, что $f(0)=f(1)=1$ и $f'(1)=1$, а $f'(0)=-1$. Таковым, например, является многочлен $(x-\frac{1}{2})^2+\frac{3}{4}$. Стартуя с точки $x_0=0$ мы приходим в точку $x_1=1$ и потом в $x_2=0$, то есть попадаем в цикл. Можно придраться и сказать, что у этого многочлена нет корней и потому всё так плохо. Однако, добавив слагаемое вида $c x^2(x-1)^2$ мы получим многочлен у которого есть корни (при подходящем $c$), но который удовлетворяет тем же условиям $f(0)=f(1)=1$ и $f'(1)=1$, а $f'(0)=-1$.

Несложно построить такой многочлен, для которого метод Ньютона будет приводить к циклу произвольной длины. Например, цикл длины три получится, если $f(-1)=f(0)=f(1)=1$ и $f'(0)=f'(1)=1$, но $f'(-1)=-\frac{1}{2}$.



Рассмотрим на первый взгляд другую задачу. Можно ли свести решение уравнения по модулю $p^n$ к решению уравнению по модулю $p$? Оказывается, что во многих случаях это так. И помогает в этом лемма Гензеля.

\lm Пусть $f(x)\in \mb Z[x]$ и дано целое число $a$, что $f(a)\equiv 0 (\mod p)$. Предположим так же, что $f'(a)\nequiv 0 ( \mod p)$. Тогда  для любого $k\in \mb N$ существует единственное $b$ по модулю $p^k$, что $f(b)\equiv 0(\mod p^k)$ и $b\equiv a (\mod p)$.   
\elm
\proof Будем доказывать утверждение индукцией по $k$. При $k=1$ надо взять $b=a$. Перейдём к шагу индукции. Заметим, что если мы нашли $b$ по модулю $p^k$ и показали его единственность, то это $b$ годится и для всех меньших степеней $p$, причём единственность так же выполнена. Таким образом, достаточно показать, что если нашлось единственное такое $b$ по модулю $p^k$, что $f(b)\equiv 0 mod p^k$  то существует единственное $b_{new}$ по модулю $p^{2k}$ удовлетворяющее аналогичному условию по модулю $p^{2k}$. То есть достаточно удваивать степень.

Перейдём к доказательству. Заметим, прежде всего, что $b_{new}$, если оно есть, должно давать решение исходной задачи по модулю $p^k$. Так как мы предположили, что $b$ есть единственное решение по модулю $p^k$, то $b_{new}\equiv b \mod p^k$. Таким образом, нам необходимо искать $b_{new}$ в виде $b_{new}=b+rp^k$. Нам надо найти подходящее $r$ и показать, что оно однозначно определено по модулю $p^k$ -- отсюда последует единственность $b_{new}$ по модулю $p^{2k}$.

Разложим $f(x)$ по степеням $(x-b)$. Получаем $f(x)=f(b)+f'(b)(x-b)+(x-b)^2g(x)$. Подставим $x=b+rp^k$ и посмотрим на получившееся по модулю $p^{2k}$. 
$$f(x)=f(b)+f'(b)rp^k+r^2p^{2k}g(x)\equiv f(b)+f'(b)rp^k \mod p^{2k}$$
Таким образом, необходимо найти $r$, что $f(b)+f'(b)p^kr\equiv 0 \mod p^{2k}$. Заметим, что $f(b)$, как целое число, делится на $p^k$. Разделив всю левую часть на $p^k$ придём к сравнению сравнение по модулю $p^k$
$$\frac{f(b)}{p^k}+rf'(b)\equiv 0\mod p^k.$$
Заметим, что $f'(b)\equiv f'(a) \mod p$ и потому взаимно просто с $p$. Значит $f'(b)$ обратимо по модулю $p^k$. Тогда 
$$r\equiv -\frac{f(b)}{p^k}f'(b)^{-1}\mod p^k.$$
Это показывает существование и единственность для $r$ и, следовательно, для $b_{new}$.  
\endproof
Указанный процесс похож на процесс уточнения вещественного решения уравнения. Действительно, из  соотношения на $r$ можно получить $b_{new}$ по модулю $p^{2k}$. Имеем $$b_{new}=b+rp^k=b-\frac{f(b)p^k}{p^k}f'(b)^{-1}=b-\frac{f(b)}{f'(b)} (\mod p^{2k})$$
То есть, даже формула для нового <<приближения>> в точности совпадает с формулой из метода Ньютона.

Лемма Гензеля может быть применена для нахождения целочисленных корней полиномиального уравнения $f(x)=0$, где $f(x)\in \mb Z[x]$. Для этого выберем простое число $p$ и решим сначала уравнение $f(x)=0$ в $\mb Z/p$. Если $p$ мало, то сделать это можно перебором всех остатков по модулю $p$. Найдём решения $a_0,\dots,a_k$. Далее, используя лемму Гензеля, поднимем эти решения по модулю степени $p^{\alpha}$ для достаточно большого $\alpha$. Здесь уместно воспользоваться следующим замечанием:

\rm Пусть $N$ -- нечётное натуральное число. Тогда множество чисел от $\frac{-N+1}{2}$ и до $\frac{N-1}{2}$ является полной системой вычетов по модулю $N$. Если $x_0$ -- целое число, что $|x_0| <\frac{N}{2}$  и $x$ -- это элемент из указанной системы вычетов, такой что $x\equiv x_0 \mod N$, то $x=x_0$.
В случае чётного $N$ нужно чуть поменять систему вычетов.
\erm

Это замечание означает, что $x_0$ -- целочисленный корень уравнения $f(x)=0$ должен быть равен своему каноническому  из указанной полной системы вычетов по модулю $p^{\alpha}$, если $\frac{p^{\alpha}}{2}> |x_0|$. Но $x_0$ так же должен являться решением $f(x)=0$ в $\mb Z/p^{\alpha}$, а все решения в $\mb Z/p^{\alpha}$ мы уже перечислили. Таким образом, для нахождения корня достаточно просто взять и подставить всех представителей корней по модулю $p^{\alpha}$ в $f(x)$ и проверить, действительно ли они дают целочисленные решения.  

Остался вопрос в выборе $p$ и выборе $\alpha$. Выбор $\alpha$ должен быть таким, чтобы $\frac{p^\alpha}{2}$ был заведомо больше по модулю, чем любой корень $f(x)$. Для этого необходимо иметь оценку сверху на корни многочлена.

\lm Пусть $f(x) \in \mb R[x]$ представлен в виде $f(x)=a_0+a_1x+\dots+a_n x^n$. Тогда, если $x_0$ корень $f(x)$, то $|x_0|\leq \max\{1,\frac{1}{a_n}(|a_0|+\dots+|a_{n-1}|)\}$. 
\elm
\proof  Без ограничения общности будем считать $a_n=1$. Предположим противное Пусть теперь $x_0$ -- решение, $|x_0|> 1$ и $|x_0|> (|a_0|+\dots+|a_{n-1}|)$. Тогда $-x_0^n=a_0+a_1x_0+\dots+a_{n-1} x_0^{n-1}$. Оценим модуль левой части 
$$|a_0+a_1x_0+\dots+a_{n-1} x_0^{n-1}|< |a_0|+\dots+|a_{n-1}| |x_0|^{n-1} \stackrel{|x_0|\geq 1}{<} |x_0|^{n-1}(|a_0|+\dots+|a_{n-1}|) < |x_0|^n$$
Но это противоречит тому, что $x_0$ решение.
\endproof

Попробуем немного разобраться с применением леммы Гензеля для  решения уравнения $x^k-a=0$, то есть для извлечения корней из $a \in \mb Z$. Какое  $p$ можно выбрать для данных $a$ и $k$?  
Возьмём производную $kx^{k-1}$. Посмотрим, когда у $x^k-a$ есть общий корень $kx^{k-1}$. Есть две возможности. Первая состоит в том, что $k=0$ по модулю $p$. Вторая состоит в том, $0$ -- единственный корень $x^{k-1}$, является и корнем $x^k-a$.  Для того, чтобы исключить эти возможности потребуем, чтобы $k\ndi p$ и $a\ndi p$. Для таких простых $p$ уже можно применять лемму Гензеля. Такие $p$ найдутся и, как мы обсудили, найдутся достаточно маленькие.  Однако, будет несколько нехорошо, если у уравнения по модулю $p$ будет много решений. В принципе и этого иногда можно избежать. Вспомнив про RSA  получаем, что если $(k,p-1)=1$, то тогда решение единственно. Если $k=2$, то решений у квадратного уравнения не более 2 и это не страшно. Если $k$ чётное, то можно сначала попытаться извлечь квадраты, а потом извлекать корень нечётной степени. Если же с самого начала $k$ нечётно, то такое $p$ найти можно.

\upr Оцените количество операций, которое необходимо затратить для вычисления корня из целого числа при помощи леммы Гензеля.
\eupr



\subsection{Дополнительно: Изоляция корней и правило знаков Декарта}

Мы обсудили, как приближённо найти корни многочлена при помощи метода Ньютона. Однако, как мы показали, метод Ньютона не всегда сходится к решению системы так как, например, может попасть в цикл. Такого не происходит, если метод Ньютона стартует из начальной точке, достаточно близкой к корню. Таким образом, встаёт задача нахождения таких промежутков, что в этих промежутках находится по одному вещественному корню многочлена и метод Ньютона сходится к этому корню для любой начальной точки из промежутка.

Рассмотрим первую часть проблемы -- нахождение промежутков, в которых вещественный многочлен содержит один вещественный корень. Это задача называется задачей изоляции корней. Большинство методов решения этой задачи позволяют по произвольному промежутку $(a,b)$ сказать, сколько корней $f(x)$ содержится в этом промежутке. Само по себе это знание позволяет найти приближённо все корни многочлена, ведь зная, что на промежутке $(a,b)$ содержится один корень мы просто можем поделить промежуток пополам и посмотреть, в какую из половин этот корень попал и тем самым ещё больше уточнить, куда попал корень. Стоит заметить, правда, что такой вариант бинарного поиска работает гораздо медленнее метода Ньютона -- за одну итерацию он делит промежуток пополам -- то есть уточняет на один двоичный знак корня больше в то время как метод Ньютона, фактически, удваивает число верных знаков.

Первым работающий метод предложил в 1829 году французский математик Жак Шарль Франсуа Штурм. Как и более ранние методы, дающие оценку на число корней на промежутке $a,b$ теорема Штурма формулируется используя понятие числа перемен знака.

\dfn Пусть $c_0,\dots,c_n$ -- последовательность вещественных чисел. Тогда число  перемен знака в этой последовательности вычисляется по следующему правилу: сначала выкидываются все 0, а потом в оставшейся последовательности считается число пар соседних чисел разного знака.
\edfn
\noindent Сформулируем теорему Штурма:

\begin{thmm}[Штурм] Пусть $f(x) \in \mb R[x]$ -- многочлен без кратных корней. $[a,b]$ -- промежуток на котором ищутся корни. При этом $f(a),f(b)\neq 0$. Рассмотрим последовательность многочленов $f_i$ определённых по правилу $f_0(x)=f(x)$, $f_1(x)=f'(x)$, а при $2\leq i\leq n$ имеем $f_{i-2}(x)=f_{i-1}(x)q(x)-f_i(x)$, причём $\deg f_i \leq \deg f_{i-1}$. Тогда число вещественных корней многочлена $f(x)$ равно $W(a)-W(b)$, где $W(a)$ равно числу перемен знака в последовательности $f_0(a),\dots,f_n(a)$. 
\end{thmm}

Доказательство см. в учебнике Кострикина -- Введение в алгебру. Часть 1. Основы алгебры, стр 245. или в книге Прасолова Многочлены стр. 42.

Однако большая часть современных методов нахождения корней основывается на другой теореме, позволяющей лишь дать оценку на число корней на промежутке. 

\thrm[Фурье-Бюдан] Пусть $f(x)\in \mb R[x]$ c $\deg f=n$, а промежуток $[a,b]$ таков, что $f(a),f(b)\neq 0$. Тогда число корней многочлена $f(x)$ на промежутке $[a,b]$ с учётом кратности меньше или равно $V(a)-V(b)$, где $V(a)$ есть число перемен знака в последовательности $f(a),f'(a),\dots,f^{(n)}(a)$. Кроме того, число корней с учётом кратности отличается от $V(a)-V(b)$ на чётное число.
\ethrm
\proof Построим доказательство следующим образом: точка  $x$  будет двигаться от $a$ к $b$ и мы будем смотреть, как меняется число $V(a)-V(x)$. Прежде всего заметим, что если между точками  $x$  и $y$ нет корней ни у $f(x)$, ни у его производных, то $V(x)=V(y)$, так как знаки $f^{(i)}(x)$ и $f^{(i)}(y)$ одинаковы.

Таким образом, $V(x)\neq V(y)$, возможно лишь если между $x$ и $y$ есть корень $f^{(j)}(x)$. Посмотрим как меняется $V(x)$, когда мы проходим  через $x_0$ -- корень некоторой производной $f(x)$. 

Если $x_0$ -- корень какой-то производной $f(x)$, то в последовательности $f(x_0),\dots,f^{(n)}(x_0)$ есть нули. Все нули в этой последовательности можно разбить на блоки нулей. Блок длины $r$ начиная с позиции $k$ означает, что у $f^{(k)}(x)$ точка $x_0$ есть корень кратности $r$. Блоки нулей разделены между собой какими-то ненулевыми значениями. Количество перемен знака в последовательности $f(x),f'(x),\dots, f^{(n)}(x)$ до точки $x_0$ и после точки $x_0$ может отличаться только потому, что поменяли знак производные, которые лежали в каком-то блоке нулей в последовательности $f(x_0),f'(x_0),\dots, f^{(n)}(x_0)$. 

Будем разбираться с каждым блоком нулей по отдельности. Рассмотрим сначала основной случай, когда $x_0$ -- это корень кратности $r$ самого многочлена $f(x)$. Это означает, что последовательность $f(x_0), \dots, f^{(n)}(x_0)$ начинается с блока из $r$ нулей. Разберёмся с переменами знаков, связанных с этим блоком.

Удобно будет обозначать многочлен, делящийся на $(x-x_0)^k$ как $O((x-x_0)^k)$. Это довольно неплохо согласовано с определениями из математического анализа. Если $x_0$ -- корень кратности $r$, то  $f(x)=c_r(x-x_0)^r + O((x-x_0)^{r+1})$, где $c_r\neq 0$.  Тогда первые $r$ производных $f(x)$ будут иметь вид $$f^{(k)}(x)= \frac{r!}{(r-k)!}c_r(x-x_0)^{r-k}+O((x-x_0)^{r-k+1}), \text{ где } 0\leq k\leq r.$$ 
Знак $k$-ой производной в окрестности точки $x_0$ совпадает со знаком $c_r(x-x_0)^{r-k}$. Без ограничения общности можно считать, что $c_r>0$, так как количество перемен знака не изменится, если мы домножим $f(x)$ на $-1$. Тогда получаем следующую расстановку знаков
$$\begin{array}{c|c|c|c}
 \substack{\text{номер}\\ \text{производной} } &\text{ до $x_0$ }& x=x_0 & \text{ после $x_0$ }\\
\hline
0 & (-1)^r & 0 & +\\
\hline
\vdots & \vdots & \vdots & \vdots \\
\hline
r-1 & - & 0 & +\\
r & + & + & +
\end{array}.$$
До точки $x_0$ было $r$ перемен знака в соответствующем блоке нулей, а после стало $0$. $V(x)$ уменьшилось на $r$ при проходе через корень $f(x)$, что и ожидалось.

Осталось разобраться с блоками, соответствующими $x_0$ -- корню производной  $f^{(k)}(x)$ кратности $r$, где $k\geq 1$. В этой ситуации $f^{(k-1)}(x_0)\neq 0$ и $f^{(k+r)}(x_0)\neq 0$, а все производные между ними равны 0 в точке $x_0$. Представим 
$$f^{(k-1)}= a_0+ a_r(x-x_0)^{r+1}+O((x-x_0)^{r+2}) \text{ и } f^{(k)}=(r+1)a_r(x-x_0)^r+O((x-x_0)^{r+1})$$
Здесь $a_0\neq 0$  и $a_r \neq 0$. Будем считать, что $a_r>0$. Тогда среди производных от $k$ до $k+r$ происходит ровно $r$ смен наков до  и ни одной смены знака после. Осталось разобраться, с тем, что происходит со сменой знака между $k-1$ и $k$-ой производными. Нарисуем таблицу   

$$\begin{array}{c|c|c|c}
 \substack{\text{номер}\\ \text{производной} } &\text{ до $x_0$ }& x=x_0 & \text{ после $x_0$ }\\
\hline
k-1& \sgn a_0 & \sgn a_0 & \sgn a_0\\
\hline
k & (-1)^r & 0 & +\\
\hline
\vdots & \vdots & \vdots & \vdots \\
\hline
k+r-1 & - & 0 & +\\
k+r & + & + & +
\end{array}.$$
Пусть $r$ чётно. Тогда перемена знака между первой и второй строками таблицы до $x_0$ и после $x_0$ одинаково зависит от знака $\sgn a_0$. Значит при проходе через $x_0$ $V(x)$ уменьшается на $r$, то есть на чётное число, что и требовалось.

Если же $r$ нечётно, то при $ a_0>0$ до точки $x_0$ есть перемена знака между первой и второй строкой, а после $x_0$ перемены нет. Итого $V(x)$ уменьшится на $r+1$ -- чётное число. Если же $a_0<0$, то до $x_0$ перемены не было, а после появилась, то есть $V(x)$ уменьшилось на $r-1$ -- неотрицательное чётное число.
\endproof


\crl[Правило знаков Декарта] Пусть $f(x)=a_0+\dots+a_nx^n$. Тогда число положительных корней с учётом кратности для многочлена $f(x)$ не превосходит числа количества перемен знака в последовательности $a_0,\dots,a_n$ и отличается от количества перемен знака на чётное число.
\proof Устремим точку $x$ к бесконечности. Тогда $V(x)$ будет равно нулю. То есть $V(0)-V(x)=V(0)$. Но $V(0)$ есть число перемен знака в последовательности $a_0,a_1,2a_2,\dots,n!a_n$, что совпадает с числом перемен знака в последовательности $a_0,a_1,\dots,a_n$. Осталось применить теорему Фурье-Бюдана.
\endproof
\ecrl

\crl Если количество перемен знака в последовательности $a_0,\dots,a_n$ равно единице, то у многочлена $f(x)=a_0+\dots+a_nx^n$ есть единственный положительный корень, а если перемен знака нет, то положительных корней нет.
\ecrl

На правиле знаков Декарта основаны несколько алгоритмов, позволяющие найти интервалы содержащие корни. Ключевым ингредиентом здесь является теорема Винсента.

\begin{thmm}[Винсент,1836] Пусть $f(x)\in \mb R[x]$ -- многочлен. Если задано целое число $a$, то условимся говорить, что многочлен $x^nf(a+\frac{1}{x})$ есть результат преобразования координат $x \to a+\frac{1}{x}$ в многочлене $f(x)$. Тогда для любой последовательности целых чисел $a_0,\dots,a_n,\dots$, что $a_0\geq 0$, $a_i>0$ при $i>0$ существует такое $n$, что после цепочки преобразований $x\to a_0 +\frac{1}{x},\dots, x\to a_n+\frac{1}{x}$  результирующий многочлен $g(x)$ будет иметь либо одну, либо ни одной вариаций знака в последовательности коэффициентов. Более того, если число перемен знака равно единице, то у многочлена $f(x)$ есть единственный корень в интервале между $$a_0+\cfrac{1}{a_1+\cfrac{1}{\ddots\,+\cfrac{1}{a_{n-1}+\cfrac{1}{a_n}}}} \text{ и } a_0+\cfrac{1}{a_1+\cfrac{1}{\ddots+\,\cfrac{1}{a_{n-1}}}}.$$
\end{thmm}

Выбор подходящей последовательности $a_i$ даёт быстрый ответ на вопрос про корни многочлена и даёт приближение к ним. Подробнее смотри \cite[стр 56-61]{McNamee}







\section{Интерполяция}

Довольно часто требуется решить следующую задачу: пусть $K$ --- некоторое поле. Пусть дан набор различных точек
$x_1,\dots, x_n \in K$ и дан набор значений $a_1,\dots,a_n\in K$. Требуется построить многочлен $f\in K[x]$, такой что $f(x_i)=a_i$.
Прежде всего заметим, что у нас есть некоторая свобода выбора. А именно, рассмотрим многочлен $\ffi(x)=(x-x_1)\dots(x-x_n)$. Тогда можно к любому решению интерполяционной задачи прибавить кратное многочлена $\ffi(x)$ и снова получить решение интерполяционной задачи. Таким образом, можно любое решение заменить на остаток от деления на многочлен $\ffi(x)$. В частности, если есть какое-то решение, то есть решение степени строго меньше $n$.

\dfn[Задача интерполяции] Пусть дан набор различных точек $x_1,\dots,x_n\in K$ и дан набор значений
$a_1,\dots, a_n\in K$. Требуется построить многочлен $f\in K[x]$, такой что $f(x_i)=a_i$ и $\deg f < n$.
\edfn

\thrm Пусть $K$ -- поле. $x_1,\dots,x_n \in K$, $a_1,\dots,a_n \in K$ и $x_i\neq x_j$ при $i\neq j$. Тогда задача интерполяции разрешима и притом единственным образом. Более того её решение может быть найдено по формуле
$$f(x)=\sum_{i=1}^n a_i\frac{\prod_{j\neq i}(x-x_j)}{\prod_{j\neq i } (x_i-x_j)}=\sum_{i=1}^n a_i\frac{\ffi(x)}{\ffi'(x_i)(x-x_i)},$$
где $\ffi(x)=(x-x_1)\dots(x-x_n)$.
\ethrm

\proof Заметим, что $\ffi'(x_i)=\prod_{j\neq i}(x_i-x_j)$. Теперь очевидно, что указанная формула даёт решение нужной степени. Единственность очевидна из леммы о многочленах, совпадающих в достаточном числе точек.
\endproof

Последняя формула называется интерполяционной формулой Лагранжа. На интерполяционную задачу можно посмотреть немного по-другому. А именно, условие, что $f(x_i)=a_i$ можно переписать как $f(x)\equiv a_i \mod (x-x_i)$. Таким образом, интерполяционная задача это частный случай китайской теоремы об остатках для многочленов. Для того, чтобы продвинуться дальше сформулируем Китайскую теорему об остатках в достаточно общей ситуации. 


\subsection{Дополнительно: факторизация колец}


В предыдущем разделе мы заметили, что разрешимость произвольной системы уравнений в целых числах может быть проверена переходом от $\mb Z$ к меньшему кольцу $\mb Z/n$. Нам хочется иметь подобный инструмент в произвольной ситуации.

\dfn Пусть $I$ -- идеал в кольце $R$. Определим отношение сравнимости на кольце $R$ по модулю $I$ как $a\equiv b \mod I$, если $a-b \in I$.
\edfn

\utv Это отношение эквивалентности. Класс эквивалентности элемента $a$ будем обозначать как $a+I$ (или по старому: $\ovl{a}$).
\proof Действительно, если $a\equiv a$, потому что $a-a =0 \in I$. Если $a\equiv b \mod I$, то $a-b \in I$. Тогда $b-a \in I$, то есть $b\equiv a \mod I$. Аналогично $a-b \in I$ и $b-c \in I$, то значит их сумма $a-b+b-c=a-c$ лежит в $I$. То есть $a\equiv c$.
\endproof
\eutv

\dfn Пусть $R$ кольцо, $I$ --- идеал. Рассмотрим фактормножество $R/\equiv_I$. Будем обозначать его просто как $R/I$. Определим на нём структуру кольца задав следующие операции:\\
$$(a+I)+ (b+I) = (a+b)+I \text{ и } (a+I)\cdot (b+I)=ab+I. $$
\edfn

\lm[Конструкция работает!] Пусть $R$ --- кольцо, $I$ --- идеал. Тогда $R/I$ --- кольцо.
\elm
\proof
Прежде всего необходимо проверить корректность заданных операций. Рассмотрим классы $a+I$ и $b+I$ и два других представителя $a+i_1$ и $b+i_2$. Тогда $$(a+i_1)(b+i_2)=ab +ai_2+i_1b+ i_1i_2 \in ab+I,$$
то есть результат произведения не зависит от выбора представителя. Аналогично для суммы.

Далее, необходимо проверить все аксиомы: аксиомы абелевой группы для сложения, дистрибутивность, ассоциативность  и коммутативность для умножения и существование единицы.

Докажем одно из этих свойств, оставив остальные на проверку читателю. Покажем ассоциативность умножения: пусть есть три класса $\ovl{a},\ovl{b},\ovl{c}$. Тогда
$$(\ovl{a}\ovl{b})\ovl{c}=(\ovl{ab})\ovl{c}=\ovl{(ab)c}=\ovl{a(bc)}=\ovl{a}(\ovl{b}\ovl{c})$$
Таким образом, видно, что все свойства $R/I$ наследуются им от кольца $R$. В частности, $0$ это класс $\ovl{0}$, а $1$-ца в $R/I$ это класс $\ovl{1}$.
\endproof

Если идеал $I$, по которому мы факторизуем порождён элементами $a_1,\dots,a_n$, то вместо $R/(a_1,\dots,a_n)$ мы будем писать $R/a_1,\dots,a_n$. В частности, в случае главного идеала $(a)$ будем писать $R/a$.

\crl[Китайская теорема об остатках на языке колец] Пусть $R$ -- область главных идеалов. Рассмотрим попарно взаимно простые элементы $r_1,\dots,r_n$. Тогда кольца
$$R/(r_1 \cdots r_n) \cong R/(r_1) \times\dots \times R/(r_n)$$
изоморфны посредством естественного отображения
$$ x +(r_1\cdots r_n) \to \left(x+(r_1),\dots,x+(r_n)\right).$$
\ecrl 
\proof Биективность указанного отображения равносильна предыдущей формулировке китайской теоремы об остатках. Таким образом, осталось обратить внимание на корректность данного отображения и то, что указанное отображение является гомоморфизмом колец. 
\endproof



Теорема о делении с остатком в кольце многочленов  пригождается нам для описания колец вида $R[x]/g(x)$. Работать с элементами фактора, как с классами эквивалентности сложно. Для того, чтобы побороть эту сложность, надо научиться выбирать из каждого класса эквивалентности по каноническому представителю и работать только с этими представителями. Например, хорошее описание фактора есть в случае  целых чисел. Там ключевую роль играла теорема о делении с остатком. В случае многочленов аналогичную функцию выполняет степень.


\thrm[Описание фактора с помощью остатков] Пусть $K$ -- поле. Рассмотрим многочлен $g \in K[x]$, $g\neq 0$. Тогда для любого многочлена $f(x)$ существует единственный многочлен $r(x)$, такой, что $\ovl{f(x)}=\ovl{r(x)} \,(\mod g(x))$ и $\deg r(x) < \deg g(x)$.
Этот канонический представитель класса $f(x)$ можно найти как остаток от деления $f(x)$ на  $g(x)$. Если $r_1(x)$ -- канонический представитель класса $f_1(x)$, а $r_2(x)$ -- канонический представитель класса $f_2(x)$, то 
$$r_1(x)+r_2(x) \text{ представитель  $f_1(x)+f_2(x)$ и } r_1(x)r_2(x) \mod g(x) \text{ -- представитель } f_1(x)f_2(x).$$
Формула для представителя суммы -- это ключевое отличие теории для многочленов от теории для $\mb Z$, упрощающее работу с многочленами.
\ethrm
\proof
Первая часть --- это просто переформулировка теоремы о делении с остатком. Вторая часть --- заметим, что $r_1+r_2$ сравним с $f_1+f_2$ и степень $\deg(r_1+r_2)\leq \max(\deg r_1, \deg r_2) < \deg g$.
\endproof

\rm Вообще в этом описании можно поле $K$ заменить на область целостности, если потребовать, чтобы старший коэффициент $g(x)$ был обратим.
\erm


\rm Заметим, что базовое поле $K$ лежит внутри фактора $K[x]/f(x)$  в качестве классов постоянных многочленов (если $\deg f\geq 1$, конечно).
\erm




Вот например многочлен $x^3-10x-7$. У него три вещественных корня, то есть в кольце $\mb R[x]$ он раскладывается в произведение $x^3-10x-7=(x-\alpha_1)(x-\alpha_2)(x-\alpha_3)$. По китайской теореме об остатках получаем, что $$\mb R[x]/(x^3-10x-7) \cong \mb R[x]/(x-\alpha_1)\times \mb R[x]/(x-\alpha_2)\times \mb R[x]/(x-\alpha_3)\cong \mb R\times \mb R\times \mb R.$$
Последний изоморфизм появился благодаря тому, что для любого поля $K$ факторкольцо $K[x]/(x-\lambda)$ изоморфно $K$.






Сформулируем, когда фактор $R/p$ обладает хорошими свойствами.



\lm[Критерий целостности для фактора] Пусть $p\neq 0$ -- некоторый элемент в  области целостности $R$. В этом случае $p$ -- простой тогда и только тогда, когда $R/pR$ -- область целостности.
\proof $R/pR$ -- область целостности тогда и только тогда, когда $\ovl{ab}\neq 0$, если $\ovl{a}\neq 0$ и $\ovl{b} \neq 0$, где $\ovl{a}$ и $\ovl{b}$ классы элементов $a,b \in R$. Это происходит тогда и только тогда, когда $ab\notin I$, если $a\notin I$ и $b\notin I$. Что и есть определение простого идеала. Таким образом, понятие простоты -- это практически дословная переформулировка того, что фактор --- область целостности.
\endproof
\elm



Теперь скажем несколько слов про специфику ситуацию с факторами для области главных идеалов.

\lm Пусть $R$ --- область главных идеалов. Тогда элемент $p\neq 0$ простой в $R$ тогда и только тогда, когда $R/p$ - поле.
\proof Пусть $pR$ --- простой идеал в $R$. Покажем, что $R/p$ поле. Рассмотрим элемент $a$, который даёт ненулевой класс в $R/p$. Тогда $a \ndi p$ и, следовательно, $a$ и $p$ взаимно просты. Тогда единица представима в виде $1=ax+py$. Тогда элемент $x$ обратен к $a$ в $R/p$.
Обратно, если $R/p$ -- поле, то $R/p$ -- область целостности. Значит $p$ -- простой.
\endproof
\elm

\rm Для произвольного кольца это не так. Например в кольце $\mb Q[x,y]$ есть идеал $I=(y)$. Это простой идеал так как $\mb Q[x,y]/y\cong \mb Q[x]$, то есть область целостности. С другой стороны $\mb Q[x]$ --- не поле. 
\erm

\subsection{Теорема об изоморфизме и её роль в описании различных колец}

На самом деле, конструкция факторизации даёт отличное описание для различных колец. Возьмём рациональные числа и рассмотрим наименьшее подкольцо в $\mb R$, содержащее $\sqrt[3]{2}$. Кроме собственно $\sqrt[3]{2}$ в этом кольце так же должен лежать $\sqrt[3]{4}=(\sqrt[3]{2})^2$ и все возможные их суммы, то есть элементы вида $a+b\sqrt[3]{2}+c\sqrt[3]{4}$, где $a,b,c \in \mb Q$. Множество таких чисел действительно подкольцо в $\mb R$ и обозначается как $\mb Q[\sqrt[3]{2}]$. 

Априори неясно, обязательно ли  брать $\sqrt[3]{4}$, ведь он может быть элементом вида $a+b\sqrt[3]{2}$. Оказывается, что брать его обязательно. Кроме того, я утверждаю, что  это кольцо --  поле. Почему так? Как, например, обратить элемент $1+3\sqrt[3]{2}+\sqrt[3]{4}$? Или может это 0?

Для этого посмотрим на $\mb Q[x]/x^3-2$. Многочлен $x^3-2$ неприводим над $\mb Q$ так как у него нет корней в $\mb Q$. Значит  $\mb Q[x]/x^3-2$ поле. Как оно связано с $\mb Q[\sqrt[3]{2}]$? Я утверждаю, что эти кольца изоморфны.

Вместо того, чтобы строить соответствие руками я приведу общую машинку, которая даёт это соответствие за "так". 

\dfn Пусть $f\colon X \to Y$ -- отображение множеств. Тогда образом $f$ называется подмножество $Y$ вида
$$\Im f= f(X)=\{y \in Y\, |\, \exists x \in X, \text{ что } f(x)=y\}$$
\edfn

\rm Образ гомоморфизма колец -- всегда подкольцо. 
\erm

Ещё одно определение я дам в контексте групп (потому что его можно там сформулировать).

\dfn Пусть $f\colon G \to H$. Тогда ядром $f$ называют $\Ker f=\{g\in G\,|\, f(g)=1\}$.
\edfn

\lm $\Ker f$ -- это подгруппа в $G$. Гомоморфизм $f$ является мономорфизмом тогда и только тогда, когда $\Ker f=\{1\}$. В этом случае мы будем говорить, что  ядро тривиально (тривиальная подгруппа).
\elm
\proof По свойствам гомоморфизма $1\in \Ker f$. Аналогично, если $g \in \Ker f$, то $f(g^{-1})=(f(g))^{-1}=1^{-1}=1.$ Если же $a,b\in \Ker f$, то $f(ab)=f(a)f(b)=1\cdot 1=1$. Таким образом, $\Ker f $ -- подгруппа в $G$. 

Покажем, что инъективность для гомоморфизма равносильна тому, что $\Ker f=\{1\}$. Действительно, если $f$ -- инъективно, то $f(x)\neq f(y)$, если $x\neq y$. В частности, если взять $x=1$, то получаем, что $f(y)\neq f(1)=1$ для всякого $y\neq 1$. Но это и означает, что  в ядре лежит только единица.  
Таким образом, условие, что $\Ker f=\{1\}$ это просто часть условия инъективности.

Покажем, что этого условия достаточно для инъективности. Действительно, пусть $f(x)=f(y)$. Тогда $$f(xy^{-1})=f(x)f(y)^{-1}=f(x)f(x)^{-1}=1.$$
Значит элемент $xy^{-1}$ лежит в ядре. Но тогда $xy^{-1}=1$, то есть $x=y$, что и требовалось.
\endproof

\rm Не забывайте, что в случае аддитивных обозначений тривиальность ядра означает $\Ker f=\{0\}$.
\erm

\rm Ядро гомоморфизма колец -- это ядро $f$ как гомоморфизмов аддитивных групп.
\erm

\rm Отметим ещё раз, что $\Ker f$ -- идеал в $R$. Проверим самое интригующее: если $r\in R$, а $a\in \Ker f$, то $ra \in \Ker f$. Действительно $f(ra)=f(r)f(a)=f(r)\cdot 0=0$.
\erm 



\thrm[Теорема об изоморфизме] $f\colon R \to S$. Тогда есть изоморфизм $\psi \colon R/\Ker f \cong \Im f$. Причём $\psi$ задаётся как $\psi(a+\Ker f)=f(a)$. 
\ethrm
\proof Правило для $\psi$ задано с одним только, <<но>>, что необходима проверка корректности. Обозначим $\Ker f=I$. Пусть $a=b+k$, где $k\in I$. Тогда $f(a)=f(b+k)=f(b)+0=f(b)$.

Теперь надо показать, что это гомоморфизм. Покажем, только что произведение переходит в произведение (а остальное оставим в качестве упражнения). Действительно
$$\psi((a+I)(b+I))=\psi(ab+I)= f(ab)=f(a)f(b)=\psi(a+I)\psi(b+I).$$

Почему это биекция? Заметим, что отображение $\psi$ сюръективно, ведь элемент вида $f(a)$ получается как образ $\psi(a+I)$. Проверим инъективность. Покажем, что $\Ker \psi =\{0\}$. 
Пусть $a+I\in \Ker \psi$. Это значит, что $\psi(a+I)=f(a)=0$. Но тогда $a\in \Ker f$ и класс $a$ равен $0$ в $R/\Ker f$. 
\endproof

Эту теоремы можно воспринимать так -- если <<обрезать>> всё лишнее от $R$ и $S$, то останутся одинаковые куски.  

Воспользуемся этой теоремой. Рассмотрим отображение $\psi \colon \mb Q[x] \to R$, переводящее $f(x) \to f(\sqrt[3]{2})$. Это гомоморфизм колец. Его образ -- это $\mb Q[\sqrt[3]{2}]$. Найдём его ядро. Очевидно, в ядре лежат многочлены кратные $x^3-2$. Но почему только они? Пусть $r(x)\in \Ker \psi$. Тогда либо $r(x)\di x^3-2$ и мы всё доказали, либо $r(x)$ взаимно прост с $x^3-2$. Третьего не дано, потому что $x^3-2$ неприводим. Итак, пусть $(r(x),x^3-2)=1$. Рассмотрим линейное разложение НОД-а $r(x)g(x)+(x^3-2)f(x)=1$. Подставим $x=\sqrt[3]{2}$. Левая часть обнулится, а в правой останется 1. Противоречие. Значит $\Ker \psi = (x^3-2)\mb Q[x]$. Ясно что это довольно общий аргумент. 

Применим теорему об изоморфизме. Тогда $$\mb Q[x]/x^3-2= \mb Q[x]/\Ker \psi \equiv \mb Q[\sqrt[3]{2}].$$

Как теперь обратить элемент $2+\sqrt[3]{2} + \sqrt[3]{4}$? В поле  $\mb Q[x]/x^3-2$ ему соответствует класс многочлена $x^2+x+2$. Найдём линейное разложение НОД-а для этого многочлена и $x^3-2$ 
$$(x^2+x+2)(-x^2+2)+(x-1)(x^3-2)=2.$$
Отсюда следует, что класс $\frac{1}{2}(-x^2+2)$ обратен к $x^2+x+2$. Переходя обратно в $\mb Q[\sqrt[3]{2}]$ получаем, что элемент $\frac{1}{2}(-\sqrt[3]{4}+2)$ обратен к $2+\sqrt[3]{2} + \sqrt[3]{4}$.

Заметим, что в $\mb Q[\sqrt[3]{2}]$ лежит корень уравнения $x^3-2$. Значит корень этого уравнения лежит и в $\mb Q[x]/x^3-2$. Заметим, что это просто класс $\ovl{x}$. Это более менее общий факт.



\utv[У неприводимого многочлена где-то есть корень] Пусть $f(x)\in K[x]$ -- неприводимый многочлен. Тогда у $f(x)$ есть корень в поле $L=K[x]/f(x)$.
\eutv
\proof Понятно, что $L$ -- поле. Вспомним, что элементы из $K$ представляются классами постоянных многочленов. 
Утверждение теперь состоит в том, что класс элемента $x$ и есть корень $f(x)$ в $L$.

Действительно, если $f(x)=a_nx^n+\dots+a_0$, то $$f(\ovl{x})=\ovl{a}_n\ovl{x}^n+\dots+\ovl{a}_0=\ovl{a_nx^n+\dots+a_0}=\ovl{f(x)}=\ovl{0}$$.
\endproof

\rm Конечно, можно сформулировать аналогичное утверждение и для приводимых многочленов.
\erm






\subsection{Дополнительно: Интерполяция по Эрмиту}

Рассмотрим более общий вариант интерполяционной задачи. А именно, попробуем решить задачу следующего вида.
Пусть задан набор точек $x_1,\dots, x_n$ и для каждой точки $x_i$ задан набор чисел $a_{i,0}, a_{i,1},\dots , a_{i,k_i-1}$. Интерполяционная задача Эрмита состоит в следующем: найти $f$ такой, что $j$-тая производная $f^{(j)}(x_i)=a_{i,j}$. Так же подобную задачу называют интерполяционной задачей с кратными узлами. Покажем, что она имеет решение.



\thrm Пусть $K$ -- поле характеристики 0 (или достаточно большой положительной характеристики). Решение задачи интерполяции по Эрмиту существует и единственно среди многочленов степени меньше $\sum_{i=1}^n k_i$.
\ethrm
\proof Сведём задачу к китайской теореме об остатках. А именно пусть $f(x)$ многочлен. Тогда значения его производных в точке $x_i$ равны $a_{i,j}$ $j\in \ovl{0,k_i-1}$ тогда и только тогда, когда
$$f(x)\equiv a_{i,0}+a_{i,1}(x-x_i)+a_{i,2}\tfrac{(x-x_i)^2}{2!}+\dots+ a_{i,k_i-1}\tfrac{(x-x_i)^{k_i-1}}{(k_i-1)!}\,\, (\mod (x-x_i)^{k_i}).$$
Рассмотрев условия во всех точках $x_i$ получаем систему сравнений. Элементы $(x-x_i)^{k_i}$ взаимно простые. Применяя китайскую теорему об остатках получаем, что у данной системы сравнений есть единственное решение по модулю $\prod_{i}(x-x_i)^{k_i}$. Но тогда среди решений есть единственное, меньшее по степени чем $\prod_{i}(x-x_i)^{k_i}$. Итого у интерполяционной задачи по Эрмиту есть и единственное решение степени меньшей, чем $\sum k_i$.
\endproof

\rm Вообще для  задачи интерполяции по Эрмиту  есть формула, аналогичная формуле Лагранжа, но она не сильно хороша (см. Сборник задач по алгебре под редакцией Кострикина, стр. 93, задача 30.14 ).\erm

\subsection{Метод Ньютона решения интерполяционной задачи}

Как решить интерполяционную задачу Эрмита? Довольно простым способом решения будет метод Ньютона. Разумеется он годится и для решения обычной интерполяционной задачи. Суть метода Ньютона состоит в том, что мы на каждом шаге меняем многочлен $f(x)$ так, чтобы он удовлетворял  ещё одному условию. 

Пусть задан набор точек $x_1,\dots, x_n$ и для каждой точки $x_i$ задан набор чисел $a_{i,0}, a_{i,1},\dots , a_{i,k_i}$. Пусть так случилось, что многочлен $f(x)$ и его производные принимают в точках $x_1,\dots,x_{i-1}$  значения $a_{s,j}$, где $s\in \ovl{1,i-1}$, а $0\leq j< k_s$. Пусть так же в точке $x_i$ выполнено соотношение, что $f^{(j)}(x_i)=a_{i,j}$ при всех $0\leq j<k$, где $k<k_i-1$. Построим такой новый многочлен $f_{new}(x)$, который удовлетворяет описанным соотношениям и новому соотношению $f^{(k)}_{new}(x_i)=a_{i,k}$.

Будем искать многочлен $f_{new}$ в виде $$f_{new}(x)= f(x)+c (x-x_i)^k\prod_{s=1}^{i-1} (x-x_s)^{k_s}.$$
Заметим, прежде всего, что добавка не портит условия интерполяционной задачи, выполнения которых мы уже добились. Осталось подобрать константу $c$ так, чтобы $f^{(k)}_{new}(x_i)=a_{i,k}$. Для этого необходимо посчитать $k$-ую производную многочлена $(x-x_i)^k\prod_{s=1}^{i-1} (x-x_s)^{k_i}$  в точке $x=x_i$. Обозначим за $\ffi(x)=\prod_{s=1}^{i-1} (x-x_s)^{k_i}$. Я утверждаю, что $k$-ая производная $(x-x_i)^k\prod_{s=1}^{i-1} (x-x_s)^{k_i}$ в $x=x_i$ равна $k!\ffi(x_i)$. Это можно получить, например, следующим образом: разложим $\ffi(x)$ по степеням $(x-x_i)$. Имеем $\ffi(x)=\ffi(x_i)+c_1(x-x_i)+\dots$. Домножая на $(x-x_i)^k$ получаем разложение по степеням $(x-x_i)$ интересующего нас многочлена. Но коэффициент при $(x-x_i)^k$ это как раз $\ffi(x_i)$. 

В частности, мы видим, что $\ffi(x_i)\neq 0$ и значит уравнение на $c$ разрешимо.

Осталось заметить, что на каждом шаге мы добавляем многочлен степени $k+\sum_{s=0}^{i-1} k_s$, что меньше или равно, чем требуемая степень.





\subsection{Интерполяция и остаток}

Интерполяционная задача пригождается при поиске остатка от деления одного многочлена на другой. А именно, представим себе, что мы хотим найти остаток от деления $r(x)$ многочлена $f(x)$ на $\ffi(x)=(x-x_1)\dots (x-x_n)$, где $x_i\neq x_j$ при $i\neq j$. 
Заметим, что и $r(x)$ и $f(x)$ принимают в указанных точках $x_i$ одинаковые значения. Так же, степень $r(x)< n=\deg \ffi(x)$. Но тогда $r(x)$ восстанавливается при помощи, например, формулы Лагранжа по значениям $r(x_i)=f(x_i)$.

Аналогично, если $\ffi(x)=(x-x_1)^{k_1}\dots (x-x_n)^{k_n}$, то $r(x)$ восстанавливается по $r^{(j)}(x_i)=f^{(j)}(x_i)$ для $i\in \ovl{1,n}$ и $0\leq j < k_i$.

\subsection{Разделение секрета по Шамиру}
Одним из применений понятие интерполяции находит в задаче разделения секрета. Задача состоит в следующем: пусть есть некий секрет $S$. В качестве $S$ можно взять число определённого размера. Есть $n$ участников, которые хотят получить некоторую информацию (для $i$-го участника некоторое число  $t_i$), так, что если соберутся любые $k$ из них, то по своим числам $t_i$ они смогут восстановить секрет $S$, но если соберутся $k-1$ из них, то они не смогут восстановить секрет.

Разделение секрета поручено некому доверенному лицу, которое не есть какой либо из участников. Как реализовать такое разделение доверенному лицу? Так как на число $S$ есть ограничение по размеру, то его можно считать элементом поля $\mb Z/p$ для достаточно большого простого $p$.

Для этого доверенное лицо сгенерирует случайным образом элементы $a_1,\dots,a_{k-1}$ и составит из них и секрета $S$ многочлен $f(x)=S+a_1x+
\dots+a_{k-1}x^{k-1}$. Затем, вычислим $t_i=f(i)$ при $i$ от $1$ до $n$. Для того, чтобы эта схема работала необходимо, чтобы $n<p$.

Осталось заметить, что собравшись вместе любые $k$ участников могут восстановить  $f(x)$
и, следовательно, секрет $S$ решив интерполяционную задачу. С другой стороны, никакие $k-1$ из них не смогут достоверно восстановить свободный коэффициент $f(x)$. Более того они не смогут получить вообще никакой информации про этот коэффициент, то есть про число $S$. Здесь безусловно важно, что в качестве узлов интерполяции не берётся 0.


\section{Дискретное преобразование Фурье}

При произвольном выборе точек $x_i$ сложность задач интерполяции и подстановки $n$ при наших текущих знаниях есть $O(n^2)$. Однако, оказывается, что в определённых случаях можно подобрать такие $x_i$, что и задача интерполяции и задача о подстановке точек будут решаться заведомо быстрее.

Пусть есть некоторое поле $K$ и элемент $\omega \in K$. Рассмотрим упорядоченную $n$-ку $x=(a_0,\dots,a_{n-1})\in K^n$. Построим по ней многочлен $g(x)=a_0+\dots+a_{n-1}x^{n-1}$. Тогда определим $$F_{\omega}(x)=(g(1), g(\omega)\dots,g(\omega^{n-1}))\in K^n$$


Отображение $F_{\omega}\colon K^n \to K^n$ называется дискретным преобразованием Фурье. Если $\omega$ -- первообразный корень, то отображение $F_{\omega}$ обратимо, ведь нахождение прообраза равносильно решению задачи интерполяции для точек $x_i=\omega^i$, где $i\in \ovl{0,n-1}$. 

Пусть дан многочлен $g(x) \in K[x]$ степени меньше, чем $n$. Попробуем восстановить его свободный член $a_0$ по значениям $g(1),\dots,g(\omega^{n-1})$. Несколько экспериментов показывают, что 
$$a_0=\frac{1}{n}\left(g(1)+\dots+g(\omega^{n-1})\right).$$
Эта формула работает, если $n \in K^*$. Давайте её докажем. Для этого распишем правую часть
$$\frac{1}{n}\left(g(1)+\dots+g(\omega^{n-1})\right)=\frac{1}{n}\sum_{i=0}^{n-1} \sum_{k=0}^{n-1} a_k \omega^{ik}=\frac{1}{n}\sum_{k=0}^{n-1} a_k\sum_{i=0}^{n-1} \omega^{ik}$$

Коэффициент при $a_k$ равен $\frac{1}{n} \sum_{i=0}^{n-1}\omega^{ik}$. Таким образом, для  достаточно показать, что
\lm Пусть $\omega \in K$ -- первообразный корень степени $n$, то $$\sum_{i=0}^{n-1}\omega^{ik}=\begin{cases} n, k=0\\
0, k\neq 0
\end{cases}.$$
\elm
\proof Случай $k=0$ тривиален. Пусть теперь $k\neq 0$. Заметим, что в этом случае $1-\omega^{k}$ не $0$ и, следовательно, обратим в $K$. Так же получаем
$$\sum_{i=0}^{n-1}\omega^{ik}=\omega^{k}\sum_{i=0}^{n-1} \omega^{(i-1)k}=\omega^{k}\sum_{s=-1}^{n-2} \omega^{sk}=\omega^{k}\sum_{i=0}^{n-1} \omega^{ik}.$$
Для последнего перехода нужно заметить, что $\omega^{-1}=\omega^{n-1}$. Отсюда получаем, что 
$$0=(1-\omega^k)\sum_{i=0}^{n-1} \omega^{ik}.$$
Так как $1-\omega^k$ обратим, то  $$\sum_{i=0}^{n-1}\omega^{ki}=0.$$
\endproof

\upr На самом деле, если в поле $K$ есть первообразный корень степени $n$ из единицы, то $n \in K^*$ автоматически.
\eupr

Отвлечёмся немного и посмотрим, что нам даёт вычисление для $a_0$. Немного расширим его применимость. А именно, пусть $g(x)=a_0+\dots+ a_m x^m$. Чему тогда равно выражение $\frac{1}{n}\left(g(1)+\dots+g(\omega^{n-1})\right)$? Заметим, что $\omega^i$ -- это корни $x^n-1$. Следовательно, если многочлен $r(x)$ это остаток от деления $g(x)$ на $x^n-1$, то как обычно $g(\omega^i)=r(\omega^i)$ откуда мы сразу  получаем, что $\frac{1}{n}\left(g(1)+\dots+g(\omega^{n-1})\right)$ есть свободный член у $r(x)$. Но свободный член у $r(x)$ -- это $a_0+a_n+a_{2n}+\dots + a_{m-(n\mod m)}$. Для этого достаточно заметить, что при вычислении остатка от деления $g(x)$ на $x^n-1$ надо заменить все вхождения $x^n$ на $1$.

Как это может пригодиться? Рассмотрим, например, следующую комбинаторную задачу. Пусть дано множество $X=\{1,\dots,100\}$. Необходимо посчитать количество таких подмножеств $A\subseteq X$, что $\sum_{i\in A} i$ делится на, скажем, $5$. Почему эта задача вообще связана с многочленами? Для этого рассмотрим многочлен $$g(x)=(1+x)(1+x^2)\cdots(1+x^{100}).$$
коэффициент при $x^m$ в этом многочлене есть количество разбиений числа $m$ на слагаемые от $1$ до $100$, что тоже самое, что и число подмножеств $A \subseteq X$, что сумма элементов из $A$ в точности равна $m$. 

Таким образом, наша задача равносильна подсчёту суммы коэффициентов при $x^m$ у многочлена $g(x)$ по всем $m$, делящимся на 5.

Но мы уже знаем как искать такую сумму. Пусть $\omega=e^{\frac{2\pi i}{5}}$ -- первообразный корень степени $5$ из $1$ в $\mb C$. Тогда указанная сумма равна 
$$\frac{1}{5}\left(g(1)+\dots+g(\omega^4)\right).$$

Прежде всего заметим, что $g(1)=2^{100}$. Посчитаем $g(\omega)$. Заметим, что $$(1+1)(1+\omega)\dots(1+\omega^4)=(1+\omega)\dots(1+\omega^4)(1+\omega^5)=(1+\omega^l)\dots(1+\omega^{4+l})(1+\omega^{5+l}).$$
Отсюда $g(\omega)= ((1+1)(1+\omega)\dots(1+\omega^4))^{20}$. Заметим, что $x^5-1=(x-1)\dots(x-\omega^4)$. Подставляя $x=-1$ получаем, что 
$$-2=(-1)^5(1+1)(1+\omega)\dots(1+\omega^4).$$
Откуда $g(\omega)=2^{20}$. От элемента $\omega$ мы использовали только то, что $\omega^i$ при $0\leq i <5$ пробегает все корни $x^5-1$. Но таким же свойством обладают $\omega^2, \omega^3,\omega^4$. То есть $$g(\omega)=g(\omega^2)=g(\omega^3)=g(\omega^4)=2^{20}.$$
Получаем ответ 
$$\frac{1}{5}\left(g(1)+\dots+g(\omega^4)\right)=\frac{2^{100}+4\cdot 2^{20}}{5}.$$

Вернёмся назад к вычислению обратного отображения для преобразования Фурье.

\thrm[Вычисление обратного к преобразованию Фурье] Пусть $n\in \mb N$ некоторое натуральное число, а $\omega$ -- первообразный корень степени $n$ в поле $K$. Пусть так же $n \in K^*$. Тогда дискретное преобразование Фурье  $F_{\omega}$ обратимо и обратное к нему задаётся формулой $$(F_{\omega}^{-1})(b)_i=\frac{1}{n}\sum_{j=0}^{n-1} b_j \omega^{-ij}=\frac{1}{n} F_{\omega^{-1}}(b)_i.$$
\ethrm
\proof Достаточно доказать, что $F_{\omega^{-1}}(F_{\omega}(a))=a$. Для нулевой компоненты мы это знаем


\endproof

\subsection{Быстрое преобразование Фурье}

Перейдём теперь к алгоритму быстрого вычисления значений в указанных корнях из $1$. Этот алгоритм называется быстрым преобразованием Фурье.

\thrm[Быстрое преобразование Фурье] Пусть $n=2^k$ и $\omega \in K$ первообразный корень степени $n$ из 1-цы. Тогда дискретное преобразование Фурье можно провести за $O(n\log n)$ операций.
\ethrm
\proof  Будем считать, что вычисление $\omega^i$, по всем $0\leq i<n$ проделано заранее. Оно требует не более $n-2$ операций. Докажем индукцией по $n$, что для вычисления дискретного преобразования Фурье необходимо $\frac{3}{2} n\log n$ операций сложения и умножения без учёта операции домножения числа на $(-1)$, то есть операции взятия противоположенного. База при $n=1$ очевидна. Пусть дан многочлен $f(x)=a_0+a_1x+\dots+a_{n-1}x^{n-1}$. Сгруппируем все слагаемые с чётными степенями $x$  и отдельно с нечётными. Получим представление многочлена $f(x)$ в виде 
$$f(x)=r(x^2)+xs(x^2)$$
для некоторых многочленов $r(x), s(x)$, чья степень не превосходит $\frac{n}{2}$. Тогда, для того, чтобы посчитать $f(\omega^i)$ нужно посчитать $r(\omega^{2i})$ и $s(\omega^{2i})$. Это можно сделать используя преобразование Фурье относительно $\omega^2$. В свою очередь $\omega^2$ есть первообразный корень степени $\frac{n}{2}$ и для вычисления $F_{\omega^2}(r)$ и $F_{\omega^2}(s)$ необходимо $2\frac{3n}{4}(\log n -1)=\frac{3n}{2}(\log n -1)$ операций по индукционному предположению.

Чтобы вычислить $f(\omega^i)$  если $i<n/2$, то нужно сложить   $$F_{\omega^2}(r)_i+\omega^{i}F_{\omega^2}(s)_i,$$ либо, если $i\geq n/2$,  вычесть 
$$F_{\omega^2}(r)_{(i-n/2)}-\omega^{i-n/2}F_{\omega^2}(s)_{(i-n/2)}.$$
Здесь мы воспользовались тем, что $\omega^{n/2}=-1$ заменив $\omega^i$ на $-\omega^{i-n/2}$. 
Получаем $\frac{n}{2}$ умножений и $2\frac{n}{2}$ сложений не считая $\frac{n}{2}$ смен знака. Итого в сумме 
$$\frac{3n}{2}(\log n -1) +\frac{3}{2}n=\frac{3}{2}n\log n,$$
что и требовалось.
\endproof


Указанный в теореме алгоритм вычисления обладает одним недостатком: для вычисления ответа необходимо хранить все промежуточные вызовы $F_{\omega^{2^s}}$. Это неудобно. Можно было бы проанализировать указанный алгоритм и избавится от этой проблемы, однако проще (и полезнее) поменять точку зрения.



Вспомним, что если дан многочлен $f(x)$, то посчитать его значение в точке $a$ это тоже самое, что посчитать остаток от деления $f$ на $x-a$. Таким образом, нам нужно посчитать остатки от деления на $x-\omega^i$ по всем $0\leq i\leq n-1$. 
Далее заметим, что если мы хотим посчитать остаток $f \mod u(x)$ и $f \mod v(x)$, то можно сначала посчитать $r(x)=f\mod u(x)v(x)$, а потом уже посчитать $r(x) \mod u(x)$ и $r(x) \mod v(x)$.

Какое отношение это имеет к нашей ситуации: многочлен $f(x)=a_0+\dots a_{n-1}x^{n-1}$ имеет степень не более $n-1$ и совпадает со своим остатком от деления на $x^n-1$. Многочлен $x^n-1=x^n-\omega^n=(x-1)(x-\omega)\dots(x-\omega^{n-1})$ раскладывается в виде произведения $x^{n/2}-\omega^{n/2}$ и $x^{n/2}+\omega^{n/2}=x^{n/2}-1=x^{n/2}-\omega^0$. Половина корней $x^n-1$ является корнями первого множителя, половина -- корнями второго. Оба множителя можно разложить и дальше.
Точнее $$x^{2^{k-s}}-\omega^{j2^{k-s}}=(x^{2^{k-(s+1)}}-\omega^{j2^{k-(s+1)}})(x^{2^{k-(s+1)}}-\omega^{j2^{k-(s+1)}+\frac{n}{2}})  \text{ для всех } 0\leq j< 2^s.$$ 

Мы хотим вычислить остатки от деления на $x-\omega^j=x^{2^{k-s}}-\omega^{j2^{k-s}}$ при  $s=k$.

На шаге $0$ мы знаем остаток от деления $f(x)$ на $x^n-1=x^{2^k}-\omega^{2^k}$. Пусть на шаге $s$ мы знаем остаток от деления $f(x)$  на $x^{2^{k-s}}-\omega^{j2^{k-s}}$ для всех $0\leq j< 2^s$. Если на шаге $s$ мы сможем найти все остатки от деления $f(x)$ на  $x^{2^{k-(s+1)}}-\omega^{j2^{k-(s+1)}}$ по всем $0\leq j< 2^{s+1}$, то на шаге $s=k-1$ мы вычислим $f(\omega^j)$ по всем $0\leq j <2^k$, что, собственно, и нужно.

Посмотрим, что происходит на шаге $s$. Чтобы найти остаток от деления $f(x)$ на $x^{2^{k-(s+1)}}-\omega^{j2^{k-(s+1)}}$ надо взять остаток от деления  $f(x)$ на $x^{2^{k-s}}-\omega^{j2^{k-s}}$ и поделить его на $x^{2^{k-(s+1)}}-\omega^{j2^{k-(s+1)}}$. Таким образом, если на шаге $s$ необходимо будет поделить $2^s$ многочленов степени меньше $2^{k-s}$ на $2^{s+1}$ многочленов степени $2^{k-(s+1)}$ специального вида. Чтобы понять, что это можно легко сделать сформулируем лемму.

\lm Пусть $n$ -- чётное натуральное число, а $f(x)=\sum_{i=0}^{n-1} a_ix^i$ многочлен из $K[x]$. Тогда остаток от деления многочлена $f(x)$ на $x^{n/2}-c$ находится по формуле
$$r(x)=\sum_{i=0}^{n/2-1}(a_i+ca_{i+n/2})x^{i}. $$
\elm
\proof При вычислении остатка по модулю $x^{n/2}-c$ достаточно заменить все вхождения $x^{n/2}$ в многочлен $f(x)$ на $c$. Откуда и получаем эту формулу.
\endproof

Отсюда видно, что для вычисления указанного остатка необходимо $\frac{n}{2}$ умножений и $\frac{n}{2}$ сложений. Причём, если параллельно считать остаток от деления на $x^{n/2}+c$, то можно сэкономить $\frac{n}{2}$ умножений, заменив их на домножение на $-1$ (то есть на взятие противоположенного). Таким образом,  для вычисления дискретного преобразования Фурье указанным способом необходимо как и раньше
$$\frac{3}{2}\left(n+2\frac{n}{2}+4\frac{n}{4}+\dots+2^k\frac{n}{2^k}\right)=\frac{3}{2}nk=\frac{3}{2}n\log n$$
умножений и сложений  в $K$ не считая домножений на $-1$.

Для чего применяется быстрое преобразование Фурье? Самое базовое применение -- это быстрое произведение многочленов. А именно, пусть в поле $K$ есть $\omega$ -- первообразный корень степени $2n=2^{k+1}$ из единицы и $n \in K^*$. Пусть даны два многочлена $f,g$, что $\deg f, \deg g <n$. Тогда и $f$ и $g$  и их произведение $f\cdot g$ восстанавливаются по их значениям в точках $\omega^i$. Но если мы знаем $f(\omega^i)$ и $g(\omega^i)$, то мы знаем $fg(\omega^i)=f(\omega^i)g(\omega^i)$.

То есть посчитать произведение $fg$ можно следующим образом $$fg= \frac{1}{2n}F_{\omega^{-1}}\left(F_{\omega}(f)\cdot F_{\omega}(g)\right).$$
Здесь точкой обозначено поточечное произведение в $K^n$. Это выражение можно вычислить за $O(n\log n)$ операций в $K$, что заметно меньше наивных $O(n^2)$.

Осталось ещё несколько вопросов, касательно преобразования Фурье. 

\enm 
\item Что происходит, когда мы считаем произведение многочленов по указанной формуле, но их степени могут быть больше $n$?
\item А что если характеристика поля равна 2?
\item Как обстоят дела с преобразованием Фурье в других кольцах?
\item Для чего ещё можно использовать быстрое преобразование Фурье?
\eenm

Ответ на первый вопрос мы уже знаем -- отображение $F_{\omega}^{-1}$ восстанавливает многочлен по его значениям в корнях из единицы, то есть решает задачу интерполяции. Если степень исходного многочлена больше  или равна числа точек, то восстанавливается лишь его остаток при делении на $(x-x_1)\dots(x-x_n)$. В нашем конкретном случае получаем, что если $\deg f+\deg g \geq n$, то $\frac{1}{n}F_{\omega^{-1}}\left(F_{\omega}(f)\cdot F_{\omega}(g)\right)$ есть остаток от деления $fg$ на $x^n-1$.  


Если характеристика поля равна $2$, то можно использовать корни степени $3^k$ из единицы, если они есть. Существуют, впрочем, и другие приёмы по этому поводу.


Поговорим немного про теорию для колец. Начнём с того, что определение первообразного корня степени $n$ в этом контексте придётся поменять.



\dfn Элемент  $\omega \in R$ называется первообразным корнем  степени $n$ из единицы, если $\omega$ -- корень степени $n$ из $1$-цы и $1-\omega^i$ не делитель нуля при $1\leq i< n$.
\edfn

\rm Если $\omega \in R$ -- первообразный корень степени $n$ из единицы  и $0\leq k < n$, то 
$$\sum_{i=0}^{n-1}\omega^{ik}=\begin{cases} n, k=0\\
0, k\neq 0
\end{cases}.$$
В большинстве источников именно это условие на суммы служит определением для первообразного корня. Однако, легко показать, что эти определения, фактически, совпадают.
\erm

\upr Покажите, что если $n \in R^*$, то если для любого  $0\leq k < n$ выполнено
$$\sum_{i=0}^{n-1}\omega^{ik}=\begin{cases} n, k=0\\
0, k\neq 0
\end{cases},$$
то $\omega$ -- первообразный корень степени $n$ из единицы в $R$.
\eupr

\rm Пусть $\omega \in R$ -- первообразный корень степени $n$. Тогда\\
1) $\omega^{-1}$ тоже первообразный корень степени $n$ из $1$.\\
2) если $n=pq$, то $\omega^q$ -- это первообразный корень степени $p$ из единицы в $R$.\\
3) В частности, для чётного $n$, $\omega^{\frac{n}{2}}=-1$. Действительно $0=\omega^n-1=(\omega^{\frac{n}{2}}-1)(\omega^{\frac{n}{2}}+1)$. Осталось заметить, что по условию $(\omega^{\frac{n}{2}}-1)$ не делитель нуля.
\erm

Понятно, что бывают кольца, например $\mb Z$, в которых нет первообразных корней большой степени. Однако в случае $\mb Z$ быстрое умножение многочленов всё равно возможно. Основным инструментом для этого служит переход к достаточно большому модулю $m$ для которого есть первообразные корни степени $2^l$ для большого $l$.

Точнее, если мы хотим умножить два многочлена $f,g\in \mb Z[x]$ степени $n$, и коэффициенты $f$ и $g$ оцениваются по модулю числом $N$, то коэффициенты произведения оцениваются как $nN^2$. Если мы выберем теперь модуль $m >2nN^2$, то зная произведение $fg \mod m$ мы восстановим само произведение $fg \in \mb Z[x]$. 

Осталось только понять, как найти модуль $m$, что в кольце $\mb Z/m$ есть первообразные корни степени $2^l$.


\utv Пусть $m=2^{2^s}+1$. Тогда $2$ является первообразным корнем степени $2^{s+1}$ из единицы в кольце $\mb Z/m$. 
\eutv
\proof Предположим противное. Рассмотрим наименьшее $k<2^{s+1}$, что $2^k-1$ делитель нуля в $\mb Z/m$. Это значит, что $(2^k-1,m)\neq 1$. В частности, у $2^k-1$ и $m$ есть общий простой делитель $p$. Тогда $2^k\equiv 1 \mod p$. С другой стороны, по модулю $m$ выполнено сравнение $2^{2^s}\equiv -1 \mod m$. Значит, оно же верно по модулю $p$. Из этого следует, что $2^{2^{s+1}}\equiv 1 \mod p$.

Воспользуемся теоретико-групповой леммой. Получим, что $2^d\equiv 1$, где $d=(2^{s+1},k)$. Отсюда из минимальности $k$ получаем, что $k=2^l$ для некоторого $l\leq s$. Но если $2^{2^l} \equiv 1 \mod p$, то возведя несколько раз в квадрат получим $2^{2^s}\equiv 1 \mod p$, что противоречит сравнению $2^{2^s}\equiv -1\mod p$.   
\endproof

Что же можно сказать про другие применения быстрого преобразования Фурье? Одним из наиболее интересных сюжетов здесь является быстрое умножение целых чисел. Идея такова: представим целые  числа  $a,b$ в записи по основанию $t$, то есть $a=a_0+a_1t+\dots+a_lt^l$ и аналогично $b$. Это означает, что $a$ и $b$ есть значения некоторых многочленов $f,g$ в точке $t$. Мы хотим посчитать значение  произведения $fg$ этих многочленов в точке $t$. Посчитаем сначала произведение. Это мы умеем делать быстро при помощи быстрого преобразования Фурье вычисленного по модулю $m=2^{2^s}+1$. 

К сожалению, в таком наивном виде алгоритм не работает. Нужно сильно конкретизировать как выбирать $t$ и как считать значение $fg(t)$. Для этого $t$ обычно тоже берут степенью двойки.

Так или иначе, но Шёнхаге и Штрассену в 1971 году удалось построить на этой основе практически применимый алгоритм умножения чисел со сложностью $O(n\log n \log\log n)$. Доказательства смотри \cite[стр. 270]{AHU}.


Однако, совершенно недавно в матре 2019 года был анонсирован алгоритм умножения, работающий за $O(n\log n)$ \href{https://hal.archives-ouvertes.fr/hal-02070778/document}{David Harvey, Joris Van Der Hoeven. Integer multiplication in time $O(n \log n)$.}




\section{Поле частных}

Мы уже замечали с вами, что некоторые утверждения про многочлены с целыми коэффициентами удобно получать, рассматривая их как многочлены с рациональными коэффициентами и пользоваться тем, что $\mb Q$ -- поле. Наша задача смоделировать аналогичную ситуацию для любой области целостности.

\dfn[Поле частных] Пусть  $R$ -- область целостности. Определим кольцо $Q(R)$ как
фактор множества пар
$$ Q(R)=\{ (a,u)\,|\, a\in R, \, u\in R\setminus\{0\} \,\}/\sim$$
по отношению эквивалентности $\sim$, заданного правилом
$$ (a,u)\sim (b,v), \text{ если } av=bu.$$
Класс элемента $(a,u)$ будем обозначать  $\frac{a}{u}$ и называть дробью.
Операции сложения и умножения введём подобно рациональным числам:
$$ \tfrac{a}{u}+\tfrac{b}{v}=\tfrac{av+bu}{uv} \text{ и } \tfrac{a}{u}\cdot\tfrac{b}{v}=\tfrac{ab}{uv}.$$
\edfn




\thrm[Конструкция работает] Пусть  $R$ -- область целостности. Тогда  кольцо $Q(R)$ корректно определено и является полем.
\ethrm
\proof
Прежде всего составим план того, что необходимо проверить:
\enm
\item Отношение $\sim$ действительно отношение эквивалентности
\item Операции сложения и умножения определены корректно
\item Выполнены все свойства сложения
\item Выполнена дистрибутивность
\item Выполнены все свойства умножения
\eenm
Обсудим подробно только некоторые из этих пунктов. Например, проверим транзитивность указанного отношения. Пусть $av=bu$ и $bw=cv$. Тогда $avbw=cvbu$. Сократим на $vb$. Это возможно так как $vb \neq 0$ в $R$ и $R$ -- область целостности.

Заметим, что дроби $\frac{a}{u}$ и $\frac{sa}{su}$ равны (при $s\neq 0$).
Теперь перейдём к корректности операций. Рассмотрим сумму. Пусть $\tfrac{a}{u}\sim \tfrac{a'}{u'}$, а  $\tfrac{b}{v}=\tfrac{b'}{v'}$. Тогда сумма
$$\tfrac{a'v'+b'u'}{u'v'}\sim \tfrac{uva'v'+uvb'u'}{uvu'v'}= \tfrac{au'vv'+bv'u'u}{uvu'v'}\sim \tfrac{av+bu}{uv}.$$

Из оставшихся свойств отметим лишь, что нулевой элемент -- это дробь $\frac{0}{1}$, противоположенная дробь к дроби $\frac{a}{u}$ это $\frac{-a}{u}=\frac{a}{-u}$, единица -- это $\frac{1}{1}$, а $\left(\frac{a}{u}\right)^{-1}=\frac{u}{a}$. Разумеется, если $a\neq 0$.
\endproof

\thrm[Область целостности вкладывается в своё поле частных] Пусть  $R$ -- область целостности. Отображение $i\colon R\to Q(R)$ заданное по правилу $a\to \tfrac{a}{1}$ является инъективным гомоморфизмом колец.
\ethrm
\proof Проверим инъективность. Покажем, что ядро нулевое. Пусть $\tfrac{a}{1}= \tfrac{0}{1}$. Тогда $1\cdot a=0\cdot 1$. Но это значит, что $a=0$. 
Тот факт, что указанное отображение -- гомоморфизм колец, оставляется в качестве легкого упражнения. 

\thrm[Универсальное свойство] Пусть $R$ -- область целостности и $f\colon R \to S$ -- гомоморфизм колец, такой, что для любого $a\in R$, $a\neq 0$, то $f(a)\in S^*$. Тогда Существует и единственный гомоморфизм колец $\hat{f} \colon Q(R)\to S$, такой что $f=\hat{f}\circ i_R$. Здесь $i_R$ это описанное вложение $R$ в $Q(R)$. Фактически мы требуем, чтобы $f(a)=\hat{f}(a)$ для всех $a\in R$. 
\ethrm
\proof Как всегда начнём с единственности. Вместо дроби $\tfrac{a}{1}$ буду писать просто $a$. Рассмотрим дробь $\tfrac{a}{u}=au^{-1}$. Тогда $\hat{f}(au^{-1})=\hat{f}(a)\hat{f}(u)^{-1}=f(a)f(u)^{-1}$. Значит вариантов нет.

Теперь надо показать, что отображение, заданное правилом
$$\hat{f}(\tfrac{a}{u})=f(a)f(u)^{-1}$$
корректно задано и является гомоморфизмом. Проверка прямолинейна.
\endproof


\endproof

Таким образом, можно считать, что любая область целостности лежит в некотором поле. Это позволяет сводить некоторые утверждения про области целостности к аналогичным утверждениям про поля.


\crl У многочлена $f$ в области целостности не более чем $\deg f$ различных корней с учётом кратности.
\ecrl
\proof Пусть корни $f$ в $R$ это $x_0,\dots,x_k$ и их кратности это $\alpha_0,\dots,\alpha_k$.  Очевидно, что если $f\di (x-x_i)^{\alpha_i}$ в $R[x]$, то $f\di (x-x_i)^{\alpha_i}$ в $Q(R)[x]$. Но тогда сумма кратностей корней $f$ в $R$ не превосходит суммы кратностей этих корней в $Q(R)$, которая, в свою  очередь, меньше или равна $\deg f $.
\endproof

Посмотрим на эту конструкцию в некоторых конкретных ситуациях.
\enm
\item Поле частных $Q(\mb Z)$ это $\mb Q$. 
\item Поле частных $Q(K[[x]])$, где $K$ -- поле, обозначается как $K((x))$ и называется полем рядов Лорана с коэффициентами из $K$.
\item Поле частных $Q(K[x_1,\dots,x_n])$, где $K$ -- поле, обозначается как $K(x_1,\dots,x_n)$ и называется полем дробно-рациональных функций от $n$ переменных над полем $K$.  
\eenm

Поговорим подробнее про $K((x))$. Имеет место следующее описание:
\utv Пусть $K$ -- поле и дан элемент $f(x)\in K((x))$. Тогда существует единственное представление $f(x)$ в виде $f(x)=c_{-k}x^{-k}+\dots+ c_{-1}x^{-1}+c_0+c_1x+\dots$. Здесь первые $k$ слагаемых это дроби вида $\frac{c}{x^i}$, а оставшееся, начиная с $c_0$, задают ряд из $K[[x]]$. 
\eutv 
\proof Пусть $f(x)=\frac{h(x)}{g(x)}$, где $g(x)$ делится на наименьшую возможную степень $x$. Представим  $g(x)$ в виде $x^k u(x)$, где $u(x)$ обратим. Тогда имеет место равенство 
$$\frac{h(x)}{g(x)}=\frac{h(x)u^{-1}(x)}{x^k}.$$
Представим числитель в виде ряда, начав нумерацию слагаемых с $-k$. Получаем 
$h(x)u^{-1}(x)=c_{-k}+c_{-k+1}+\dots + c_0 +\dots $. В таких обозначениях дробь $\frac{h}{g}$ как раз имеет нужный вид.

Покажем единственность такого представления. Пусть есть два представления для одной и той же дроби. Прежде всего будем считать, что отрицательные индексы и в том и в другом начинаются с номера $-k$. Имеем $$a_{-k}x^{-k}+\dots+a_0+\dots=b_{-k}x^{-k}+\dots +b_0 +\dots.$$
Домножим на $x^k$. Получим равенство рядов. Но ряды равны тогда и только тогда, когда их коэффициенты равны. Единственность доказана. 
\endproof

\rm Собственно рядом Лорана обычно называют выражение вида $$a_{-k}x^{-k}+\dots+a_0+a_1x+a_2x^2+\dots+a_n x^n +\dots$$
то есть сумму степеней $x$, где количество слагаемых с отрицательными степенями конечно.
\erm

Благодаря универсальному свойству имеем, что имеет место вложение $K(x) \hookrightarrow K((x))$. А именно имеет место вложение $K[x]\to K[[x]]\to K((x))$. Но при таком вложении любой многочлен $f(x)\neq 0$ обратим, как элемент $K((x))$. Это и даёт возможность воспользоваться универсальным свойством.




\section{Дробно-рациональные функции}


Поговорим о специальных свойствах поля $K(x)$. Это поле в целом напоминает поле рациональных чисел, так как является полем частных евклидового кольца.

\lm Пусть $\frac{f}{g} \in K[x]$. Тогда существуют  единственные многочлены  $u,v$, где старший коэффициент $v$ равен единице, что $\Nod(u,v)=1$ и $\frac{f}{g}=\frac{u}{v}$. 
\elm
\proof Возьмём какие-то $f,g$ и рассмотрим $d=\Nod(f,g)$. Тогда $u=c^{-1}\frac{f}{d}$, а $v=c^{-1}\frac{g}{d}$ подходят, где $c$ -- это старший коэффициент $\frac{g}{d}$. Пусть есть две пары $u,v$ и $u_1,v_1$ подходящие по условию. Тогда $uv_1=vu_1$. Так как $u$ и $v$ взаимно просты выполнено $v\di v_1$. Симметрично $v_1\di v$. Тогда $v=cv_1$. Но их старшие коэффициенты равны 1. Отсюда $c=1$ и $u=u_1$.
\endproof

\dfn Если многочлены $u,v$ таковы, что $(u,v)=1$, то запись $\frac{u}{v}$ согласимся называть несократимой дробью.
\edfn

\dfn Дробь $\frac{f}{g} \in K(x)$ называется правильной, если $\deg f< \deg g$.
\edfn

\rm Это определение не зависит от представления дроби в виде отношения двух многочленов.
\erm

\lm Любая дробь $\frac{f}{g}\neq 0$ единственным образом представляется в виде суммы многочлена $h(x)\in K[x]$ и правильной дроби $\frac{r}{g_1}$. Если обе дроби  $\frac{f}{g}$ и $\frac{r}{g_1}$ несократимы и старший коэффициент $g$ и $g_1$ равен 1, то $g_1=g$.
\proof Покажем существование. Можем считать дробь $\frac{f}{g}$ несократимой и старший коэффициент $g$ равным 1. Поделим с остатком $f(x)=h(x)g(x)+r(x)$, где $\deg r(x)<\deg g(x)$. Тогда
$$\frac{f(x)}{g(x)} =\frac{h(x)g(x)+r(x)}{g(x)} =h(x)+\frac{r(x)}{g(x)}.$$
Кроме того, заметим, что дробь $\frac{r(x)}{g(x)}$ так же несократимая.
Покажем единственность. Пусть
$$h_1(x)+\frac{r_1}{g_1}=h_2(x)+\frac{r_2}{g_2}.$$
Имеем равенство многочленов.
$$(h_1(x)-h_2(x))g_1(x)g_2(x)=g_1(x)r_2(x)-r_1(x)g_2(x).$$
Если $h_1\neq h_2$, то степень многочленов справа строго меньше степени многочлена слева. Таким образом, $h_1=h_2$, а значит $\frac{r_1}{g_1}=\frac{r_2}{g_2}$. Возьмём теперь в качестве $h_2=h$, $r_2=r$ и $g_2=g$ из доказательства существования. Предположим так же, что $\frac{r_1}{g_1}$ несократима и старший коэффициент $g_1$ равен 1. Тогда по единственности $\frac{r}{g}=\frac{r_1}{g_1}$. Осталось  применить единственность представления в виде несократимой записи и получить, что $g=g_1$.
\endproof
\elm

\rm Сумма двух правильных дробей -- снова правильная дробь. Произведение двух правильных дробей -- тоже правильная дробь. Таким образом, правильные дроби образуют подкольцо (без единицы!) в $K(x)$. Впрочем, единицу можно добавить, разрешив константы.
\erm

\dfn[Простейшие дроби] Пусть $K$ --- поле, $p\in K[x]$ --- неприводимый многочлен со страшим коэффициентом единица. Тогда дробь
$$\frac{f(x)}{p(x)^{k}} \text{ называется простейшей, если $f \neq 0$ и $\deg f < \deg p$}. $$
\edfn

\lm[О разложении по основанию] Пусть даны два многочлена $f(x)\in K[x]$ и $p(x)\in K[x]$, причём $\deg p(x)\geq 1$. Рассмотрим такое $s$, что $s\deg p < \deg f < (s+1)\deg p$. Тогда существуют единственные $a_i(x)\in K[x]$ $0\leq i\leq s$, что 
$$\deg a_i(x) < \deg p(x) \text{ и } f(x)=\sum_{i=0}^sa_i p^i(x).$$
\elm
\proof Докажем существование и единственность индукцией по степени $f(x)$. При $\deg f < \deg p$ возьмём $a_0(x)=f(x)$. Пусть теперь степень $f(x)$ произвольная.  Поделим $f(x)$ на $p(x)$ с остатком $$f(x)=p(x)q(x)+a_0(x).$$
Заметим, что  $\deg q(x) = \deg f(x)-\deg p(x) <  \deg f(x)$. Применим индукционное предположение для $q(x)$. $$q(x)=a_1+a_2p+\dots+a_s p^{s-1}.$$
Подставляя, получим представление для $f(x)$. 
Так же по индукции доказывается, что такое представление единственно: $a_0$ восстанавливается как остаток $f(x) \mod p(x)$, $a_1$ -- как остаток $\frac{f(x)-a_0}{p(x)} \mod p$ и так далее.
\endproof

\crl Несократимая дробь вида $\frac{r(x)}{p(x)^{\alpha}}$ единственным образом может быть представлена в виде суммы простейших
$$\frac{r(x)}{p(x)^{\alpha}}= \sum_{j=1}^{\alpha} \frac{r_j(x)}{p^j(x)},$$
где $r_s \neq 0$.
\proof Для выполнения указанных условий необходимо  и достаточно, чтобы $r(x)=\sum_{j=1}^{\alpha} r_j p(x)^{\alpha-j} $, и $\deg r_i < \deg p(x)$. 
\endproof
\ecrl


\lm[О разложении в сумму правильных дробей со взаимно простыми знаменателями] Пусть $\frac{f}{g} \in K(x)$ -- правильная дробь и многочлен $g(x)\in K[x]$ раскладывается на взаимно простые множители $g=g_1g_2$, где $(g_1,g_2)=1$. Тогда существуют единственные $h_1,h_2 \in K[x]$, что $$\frac{f}{g}=\frac{h_1}{g_1}+\frac{h_2}{g_2},$$
что дроби $\frac{h_1}{g_1}$ и $\frac{h_2}{g_2}$ -- правильные.
\elm
\proof Равенство
$ \frac{f}{g}=\frac{h_1}{g_1}+\frac{h_2}{g_2}$
имеет место тогда и только тогда, когда $h_1$ и $h_2$ являются решениями линейного уравнения $$h_2g_1+h_1g_2=f.$$
Так как $(g_1,g_2)=1$, то решение такого уравнения существует и, более того, если есть второе решение $\hat{h_1}, \hat{h_2}$, то $\hat{h_1}\equiv h_1\mod g_1$ и $\hat{h_2}\equiv h_2 \mod g_2$. В частности, существует и единственно решение $h_1, h_2$, где $\deg h_1 < \deg g_1$. Рассмотрим такое $h_1$. Ему однозначно соответствует $h_2$, такое что $g_1h_2= f- g_2h_1$. Заметим, что степень правой части строго меньше  $\deg g_1+ \deg g_2$. Отсюда следует, что степень $h_2$ меньше $\deg g_2$. Существование и единственность доказаны.
\endproof


\thrm[О разложении на простейшие] Пусть $K$ ---  поле. Тогда для любой несократимой дроби $0
\neq \frac{f}{g} \in K(x)$ существуют единственные многочлен $h\in K[x]$, неприводимые многочлены $p_1, \dots, p_n$ со старшим коэффициентом 1, натуральные числа $\alpha_1,\dots, \alpha_n$ и многочлены $r_{ij}$, где $i\in \ovl{1,n}$, и $j\in \ovl{0,\alpha_i}$, что дроби
$$ \frac{r_{ij}}{p_i^{j}} \text{ --- простейшие и } \frac{f}{g}=h+\sum_{i,j} \frac{r_{ij}}{p_i^{j}}.$$
При этом, если  $r_{i\alpha_i}$ не ноль и старший коэффициент $g$  равен 1, то $g=\prod p_i^{\alpha_i}$.
\ethrm

\proof Покажем существование разложения. Будем предполагать, что дробь $\frac{f}{g}$ несократима и старший коэффициент $g$ равен 1. По лемме о разложении дроби в виде суммы многочлена и правильной дроби представим $$\frac{f(x)}{g(x)}= h(x)+\frac{r(x)}{g(x)}$$
Переходя от $\frac{f}{g}$ к $\frac{r}{g}$, можно считать, что изначально дана правильная дробь.
В таком предположении будем доказывать существование разложения по индукции. Если $g=p^{\alpha}$, то разложение правильной дроби $\frac{f}{g}$ получается благодаря следствию из леммы о разложении по основанию.


Это отличная база для индукции по степени многочлена $g(x)$  (или по числу его различных неприводимых делителей). Для того чтобы показать шаг индукции докажем лемму



Перейдём к доказательству теоремы. Разложим $g(x)$ в виде $g(x)=p_1^{\alpha_1}\dots p_k^{\alpha_k}$. Множители $p_1^{\alpha_1}$ и $\prod_{i\geq 2}p_i^{\alpha_i}$ взаимно простые. Тогда по лемме существует единственное разложение в сумму правильных дробей $$\frac{f}{g}=  \frac{h_1}{p_1^{\alpha_1}}+\frac{h_2}{\prod_{i\geq 2}p_i^{\alpha_i}}.$$

Каждое из слагаемых раскладывается на простейшие благодаря индукционному предположению.\\

\proof[Единственность] Пусть $\frac{f}{g}$ -- несократимая дробь и старший коэффициент $g$ равен 1. Такое представление единственно для любой дроби. Пусть
$$\frac{f}{g}=h+\sum_{i,j} \frac{r_{ij}}{p_i^{j}},$$
где $r_{i\alpha_i}$ не ноль. Прежде всего заметим, что $$\sum_{i,j} \frac{r_{ij}}{p_i^{j}}$$
правильная дробь. Тогда $h$ определяется однозначно из единственности представления дроби в виде суммы многочлена и правильной дроби. Заменяя $\frac{f}{g}$ на $\frac{f}{g}-h$ можем считать, что дробь $\frac{f}{g}$ правильная. Теперь  покажем, что  $g=\prod p_i^{\alpha_i}$. Введём обозначение $$r_i= \sum_{j=1}^{\alpha_i} r_{ij} p_i^{\alpha_i-j}.$$
Приведя разложение на простейшие для $\frac{f}{g}$ к общему знаменателю  получим дробь 
$$\sum_{i,j} \frac{r_{ij}}{p_i^{j}}= \frac{\sum_{i} r_{i}\prod_{k\neq i} p_k^{\alpha_{k}}}{\prod p_i^{\alpha_{i}}}.$$
Покажем, что эта дробь несократима. Выберем некоторый неприводимый множитель знаменателя $p_l$ и покажем, что числитель не делится на $p_l$. Слагаемые вида $$r_i \prod_{k\neq i} p_k^{\alpha_{k}}$$ делятся на $p_l$, если $l \neq i$. С другой стороны, $r_l$ не делится на $p_l$, так как $r_{l\alpha_l}$ не делится (потому что он не ноль и степени меньше $\deg p_l$). Тогда и вся сумма не делится на $p_l$.
Но это значит, что $\prod p_i^{\alpha_i}=g$ из единственности несократимой записи для дроби. Таким образом,мы показали единственность $p_i$  и $\alpha_i$. Покажем единственность $r_i$. Имеем представление $$\frac{f}{g}=\frac{r_1}{p_1^{\alpha_1}}+\sum_{i\geq 2} \frac{r_i}{p_i^{\alpha_i}}$$
Первое слагаемое есть правильная дробь со знаменателем $p_1^{\alpha_1}$. Второе слагаемое есть правильная дробь со знаменателем $\prod_{i\geq 2}p_i^{\alpha_i}$. По единственности из леммы о разложении в сумму двух дробей со взаимно простыми знаменателями получаем единственность $r_1$. Переставляя слагаемые получаем единственность для всех $r_i$. Но по лемме о разложении по основанию $r_{ij}$ однозначно восстанавливаются по $r_i$, что и завершает доказательство.
\endproof

\zd Какой аналог у последней теоремы в рациональных числах?
\ezd

Рассмотрим теперь конкретные поля. Так как поле $\mb C$ алгебраически замкнуто, то все неприводимые многочлены над $\mb C$ имеют степень 1. Это заметно упрощает жизнь, так как в числителе простейшей дроби могут стоять только константы.

\crl Для любой дроби $\frac{f}{g}\in \mb C(x)$ существует представление в виде $h(x)+\sum_{i=1}^k\sum_{j=1}^{\alpha_i}\frac{A_{ij}}{(x-x_i)^j}$, где $h(x)\in \mb C[x]$, $A_{ij}\in \mb C$, а $x_i$ -- корни $g(x)$ кратности $\alpha_i$.
\ecrl 

\upr Сформулируйте аналогичное следствие для $\mb R$.
\eupr

Самый стандартный, но далеко не самый эффективный способ нахождения разложения на простейшие -- метод неопределённых коэффициентов. Приведём пример нахождения некоторого разложения, которое использует конструкцию интерполяции. Рассмотрим рациональную функцию $$\frac{1}{x^n-1}.$$
Хочется найти её разложение на простейшие в $\mb C$. Корни  многочлена $x^n-1$ нам известны. Это $$\eps_l=e^{\tfrac{2i \pi l}{n}},\,\,\,l\in \ovl{0,n-1}.$$ Многочлен $g(x)=1$ восстанавливается по своим значениям в точках  $\eps_l$ по формуле Лагранжа
$$1=\sum_{l=0}^{n-1} \frac{ x^n-1}{n\eps_l^{n-1}(x-\eps_l)}.$$
Роль функции $\ffi(x)$ здесь играет $x^n-1$. Тогда
$$\frac{1}{x^n-1}=\frac{\sum_{l=0}^{n-1} \frac{ x^n-1}{n\eps_l^{n-1}(x-\eps_l)}}{x^n-1}= \sum_{l=0}^{n-1} \frac{\eps_l}{n(x-\eps_l)}.$$



\section{Дополнительно. Степенные ряды как производящие функции}


\dfn[Линейное рекуррентное соотношение] Будем говорить, что последовательность $x_n$ удовлетворяет линейному рекуррентному соотношению $k$-го порядка, если существуют числа $a_0,\dots,a_{k}$, что $a_k,a_0\neq 0$ и
$$a_k x_{n+k}+a_{k-1}x_{n+k-1}+\dots+a_0x_n=0$$
\edfn


\rm Вообще говоря, ничто не мешает считать, что начальные коэффициенты $a_0,\dots,a_s=0$. Просто это означает, что до номера $k+s$ последовательность может быть любой, а после $k+s$ начинает удовлетворять соотношению с коэффициентами $a_{s+1},\dots,a_k$.
\erm

\dfn[Производящая функция] Пусть дана последовательность $a_n$, $n\geq 0$. Производящей функцией для последовательности $a_n$ назовём формальный степенной ряд $f(x)=\sum_{i=0}^{\infty} a_ix^i$.
\edfn



\thrm Ряд из $K[[x]]$ является рядом некоторой правильной дроби $\frac{f(x)}{g(x)}$, тогда и только тогда, когда его коэффициенты  удовлетворяют линейному рекуррентному соотношению. Более того, порядок наименьшего линейного рекуррентного соотношения, которому удовлетворяют коэффициенты, равен степени знаменателя в несократимой дроби.
\ethrm

\proof
Заметим, что, ряд Лорана  несократимой дроби $\frac{f}{g}$ лежит в $K[[x]]$ тогда и только тогда, когда $g(x)\ndi x$. Если $g(x)\ndi x$, то $g(x)$ обратима в $K[[x]]$, откуда и вся дробь лежит в $K[[x]]$. Обратно, если $g \di x$, и $\frac{f}{g}$ лежит в $K[[x]]$, то $f\ndi x$ и тогда $f$   обратимо в $K[[x]]$ откуда $g(x)^{-1} \in K[[x]]$, что не возможно, потому что свободный член $g$ равен 0.

Пусть $q(x)$ есть ряд правильной несократимой дроби $\frac{f(x)}{g(x)}$. Тогда $q(x)$ удовлетворяет соотношению $g(x)q(x)=f(x)$. Мы уже один раз выписывали соотношение на коэффициенты $g(x)$, когда $f(x)$ был равен 1. Поступим аналогично. Пусть $g(x)=b_nx^n+\dots +b_0$, $f(x)=a_mx^m+\dots +a_0$, а $q(x)=c_0+c_1x+\dots$. Тогда имеем уравнения на коэффициенты $q(x)$

$$ \sum_{j=0}^{n} b_j c_{i-j} =a_i .$$
Выражение справа равно 0 при $i>m$. Выражение слева при $i\geq n$ всегда имеет $n$ слагаемых. Функция $g(x)$ не может делиться на $x$ так как иначе мы не получили бы элемент из $K[[x]]$ или дробь была бы сократима. Тогда $b_0\neq 0$. Таким образом, при $i\geq n$ получаем рекуррентное соотношение на $c_j$

$$ b_0 c_{j+n}+b_1 c_{j+n-1}+\dots + b_n c_j=0.$$
Обратно, пусть $c_j$ удовлетворяют рекуррентному соотношению
$$ b_0 c_{j+n}+b_1 c_{j+n-1}+\dots + b_n c_j=0, \text{ где } b_0,b_n \neq 0.$$

Тогда возьмём в качестве $g(x)= b_n x^n+\dots+b_0$. Как теперь найти $f(x)$? Вспомним условие на коэффициенты и положим
$$a_i= \sum_{j=0}^{n} b_{j} c_{i-j}, \text{ где } i\in \ovl{0,n}.$$
Это и есть коэффициенты $f(x)$. Допустим дробь $\frac{f}{g}$ сократима. Тогда по уже доказанному она удовлетворяет соотношению меньшего $\deg g$ порядка.
\endproof

Рассмотрим простейший пример. Какая рациональная функция соответствует последовательности удовлетворяющей соотношению $z_{n+1}=\lambda z_n$, $z_0=1$? Эта последовательность имеет вид $z_n=\lambda^n$. Ей соответствует ряд $$1+ \lambda x+\dots + \lambda^nx^n+\dots .$$
Это ряд для функции
$$\frac{1}{1-\lambda x}.$$
Значит функции
$$\frac{1}{x-\lambda}=\frac{-1}{\lambda}\frac{1}{1-\frac{x}{\lambda}}$$
соответствует последовательность $z_n=-{\frac{1}{\lambda}^{n+1}}$, то есть некоторая геометрическая прогрессия.

А что соответствует ${\frac{1}{(1-\lambda x)^k}}$, где $k\geq 2$? Вспомним про производную. Заметим, что $$\frac{d^{k-1}}{dx^{k-1}}\frac{1}{1-\lambda x}=\frac{\lambda^{k-1} (k-1)!}{(1-\lambda x)^k}.$$
Переписывая получаем
$$\frac{1}{(1-\lambda x)^k}= \frac{1}{\lambda^{k-1} (k-1)!}\frac{d^{k-1}}{dx^{k-1}}\frac{1}{1-\lambda x}=  \sum_{n=0}^{\infty} C_{n+k-1}^{k-1} \lambda^{n}x^{n}.$$

\crl Пусть последовательность комплексных чисел $z_n$ удовлетворяет соотношению $a_k z_{n+k}+a_{k-1}z_{n+k-1}+\dots+a_0z_n=0$. Пусть многочлен $p(x)=a_k x^k+\dots +a_0$ имеет корни $\lambda_1$ кратности $k_1$, $\ldots$, $\lambda_l$ кратности $k_l$. Тогда последовательность $z_n$ имеет вид
$$ p_1(n)\lambda_1^n+\dots+p_l(n)\lambda_l^n,$$
где $p_i$ многочлен степени не выше $k_i$.
\proof
Рассмотрим многочлен $g(x)=a_0x^k+\dots+a_k$. Заметим, что $$g(x)=x^k p\left(\frac{1}{x}\right)= x^k\prod\left(\frac{1}{x}-\lambda_i\right)^{k_i}= \prod (1-\lambda_ix)^{k_i}.$$
Последовательность $z_n$ имеет производящую функцию вида
$$\frac{h(x)}{g(x)}.$$
Разложим её на простейшие над $\mb C$. Получим
$$\frac{h(x)}{g(x)}=\sum_{i=1}^l \sum_{0\leq j < k_i} \frac{b_{ij}}{(1-\lambda_ix)^j}.$$
Каждое слагаемое является производящей функцией для последовательности вида $p_{ij}(n)\lambda_i^n$, $\deg p_i < k_i$. Осталось просуммировать при одинаковом $i$.
\endproof
\ecrl

\dfn Многочлен $a_kx^k+\dots +a_0$ называется характеристическим многочленом линейной рекуррентной последовательности.
\edfn

Рассмотрим пример: пусть $f_n$ --- последовательность чисел Фибоначчи. Она удовлетворяет рекуррентному соотношению $f_{n+2}-f_{n+1}-f_n=0.$ Такой последовательности соответствует многочлен $g(x)=-x^2-x+1$ и $f(x)=x$. Рассмотрим дробь $F(x)=\frac{x}{-x^2-x+1}$  и разложим её в сумму простейших. Корни знаменателя это $\lambda_1=\frac{-1+\sqrt{5}}{2}$ и $\lambda_2=\frac{-1-\sqrt{5}}{2}$. Получим
$$F(x)= -\left(\frac{\lambda_1}{\sqrt{5}(x-\lambda_1)}-\frac{\lambda_2}{\sqrt{5}(x-\lambda_2)}\right).$$
Представим каждое слагаемое в виде ряда и получим формулу
$$f_n= \frac{1}{\sqrt{5}}(\ffi^{n-1}-\ovl{\ffi}^{n-1}),$$
где $\ffi=\frac{1+\sqrt{5}}{2}$, а $\ovl{\ffi}=\frac{1-\sqrt{5}}{2}$

Пусть последовательность $a_{n+2}=4a_{n+1}-4a_n$ начинается с $a_1=2$, $a_0=0$. Найдём общую формулу. Рассмотрим дробь $$A(x)=\frac{2x}{4x^2-4x+1}=\frac{2x}{(2x-1)^2}.$$
Как найти разложение в ряд? Заметим, что $$\frac{2}{(2x-1)^2}= \frac{d}{dx}\frac{-1}{2x-1}.$$
Вспомним, как считается производная для рядов. Тогда
$$A(x)=\sum_{n=0} (n+1) 2^{n+1} x^{n+1}=\sum_{n=0} n2^nx^n.$$




Заметим, что если $z_n$ --- комплексная последовательность, удовлетворяющая линейному рекуррентному соотношению, то её производящая функция $f(x)=\frac{h(x)}{g(x)}$ имеет конечный набор комплексных точек, в которых она не определена. Более того, мы даже знаем эти комплексные точки --- это корни $g(x)$, то есть обратные к корням характеристического многочлена. Общая философия, которая за этим стоит такая --- поведение последовательности определяется  <<особыми точками>> её производящей функции, то есть точками на комплексной плоскости, куда эта  функция не может быть продолжена, например, её полюсами.




\chapter{Основы линейной алгебры}


\setcounter{zad}{0}
\setcounter{lem}{0}
\setcounter{thm}{0}
%\setcounter{defn}{0}
\setcounter{cor}{0}




\section{Системы линейных уравнений и метод Гаусса}

\dfn Пусть $R$ -- кольцо. Тогда системой $m$ линейных уравнений от $n$ неизвестных называется набор условий

$$\begin{cases}
a_{11}x_1+\dots + a_{1n}x_n=b_1\\
\vdots \\
a_{m1}x_1+\dots+a_{mn}x_n=b_m
\end{cases},$$
где $a_{ij}, b_i \in R$.
\edfn

Совершенно понятно, что система линейных уравнений определяется однозначно числами $a_{ij}$ и $b_i$. Эти числа удобно организовывать в матрицы.

\dfn Матрица размера $m\times n$ над кольцом $R$ -- это набор чисел проиндексированных двумя индексами $a_{ij}$ $i\in \ovl{1,m}$, $j\in \ovl{1,n}$. Множество всех матриц размера $m\times n$ над кольцом $R$ обозначается как $M_{m\times n}(R)$. Обычно матрицы  будут обозначаться заглавными буквами, например $A$. Тот факт, что матрица $A$ имеет размер $m\times n$ будем записывать как $A\in M_{m\times n}(R)$. Рисовать матрицы мы будем в виде таблиц.
\edfn

\dfn Матрица системы линейных уравнений называется матрица $A\in M_{m\times n}$, состоящая из коэффициентов $a_{ij}$ этой системы. Матрица размера $m\times (n+1)$ содержащая дополнительно столбец $b_1,\dots, b_m$ называется расширенной матрицей системы. Мы будем отчёркивать столбец $b$, чтобы выделить его особую роль и будем обозначать расширенную матрицу системы как $(A|b)$.
$$\begin{cases}
a_{11}x_1+\dots + a_{1n}x_n=b_1\\
\vdots \\
a_{m1}x_1+\dots+a_{mn}x_n=b_m
\end{cases} \to
\left( \bar{ccc|c}
a_{11}& \dots& a_{1n} &b_1\\
\vdots& \ddots & \vdots&  \vdots \\
a_{m1}&\dots&a_{mn}& b_m
\ear \right).
$$

\edfn


От каждой системы нас прежде всего интересует множество её решений. Поэтому логично ввести определение:

\dfn Две системы линейных уравнений от одного и того же набора переменных $x_1,\dots,x_n$ называются эквивалентными, если множества их решений совпадают.
\edfn

Как для данной системы линейных уравнений можно построить эквивалентную? Прежде всего заметим, что если есть два уравнения, то по ним можно построить много новых, а именно, пусть имеют $\lambda$ и $\mu$ из $R$. Тогда сложив два уравнения с коэффициентами $\lambda$ и $\mu$ получаем третье
$$\begin{cases}
a_1x_1+\dots+a_nx_n=c\\
b_1x_1+\dots+b_nx_n=d
\end{cases} \Rightarrow (\lambda a_1+\mu b_1)x_1+ \dots +(\lambda a_n+ \mu b_n)x_n= \lambda c+\mu d.$$
Понятно, что если набор $x_1,\dots,x_n$ -- решение первых двух, то и нового тоже.

Получать новые уравнения мы научились, но увеличивать число уравнений в системе  -- не то, к чему мы стремимся. Нам бы хотелось получать систему такого или меньшего размера. Введём определение элементарных преобразований.

\dfn Пусть дана система уравнений
 $$\begin{cases}
a_{11}x_1+\dots + a_{1n}x_n=b_1\\
\vdots \\
a_{m1}x_1+\dots+a_{mn}x_n=b_m
\end{cases},$$
Элементарным преобразованием первого типа над этой системой линейных уравнений называется следующая операция. Рассмотрим уравнения с номерами $i$ и $j$, где $i\neq j$ и элемент $\lambda \in K$. Тогда прибавим  $i$-ое уравнение к $j$-ому с коэффициентом $\lambda$ и поместим результат на место $j$-го уравнения.
\edfn

\rm Очевидно, что решение новой системы содержит решения старой. Однако верно и наоборот, так как старая система получается из новой аналогичным прибавлением $i$-ого уравнения к $j$-ому, но с коэффициентом $-\lambda$.
\erm

\dfn  Элементарным преобразованием второго типа называется преобразование, домножающее $i$-ое уравнение на коэффициент $\lambda \in R^*$.
Элементарным преобразованием третьего типа называется преобразование меняющее местами $i$-ое и $j$-ое уравнения местами.
\edfn

\rm Элементарное преобразование второго типа приводит к эквивалентной системе так как есть обратное преобразование -- домножение на $\lambda^{-1}$. Для преобразования третьего типа эквивалентность тривиальна.
\erm





Нам будет удобно вместо системы линейных уравнений работать с её упрощённой записью -- матрицей этой системы. Поэтому логично перевести понятия элементарных преобразований на язык матриц.

\dfn Элементарным преобразованием строк первого типа над матрицей $A$ называется прибавление к $j$-ой строчке матрицы $A$ её $i$ строки с некоторым коэффициентом $\lambda$. Преобразованием второго типа называется домножение $i$-ой строчки на обратимый элемент $\lambda \in R^*$. Элементарным преобразованием третьего типа называется перестановка $i$-ой и $j$-ой строк в матрице $A$. 
\edfn

Мы научились строить по системе уравнений новую равносильную ей систему. Прежде чем обсудить общую ситуацию, посмотрим на конкретный пример. Решим систему из трёх уравнений с тремя неизвестными:
$$\begin{cases}
x+3y+z=1\\
2x+5y+z=-1\\
x-y+2z=-1
\end{cases}.
$$
Для простоты перейдём к матричной форме. Вот расширенная матрица системы:
$$\left(
\bar{rrr|r}
1& 3 & 1 & 1\\
2& 5 & 1 & -1\\
1& -1 & 2 &-1
\ear\right).$$
Применим цепочку элементарных преобразований строк
$$\left(
\bar{rrr|r}
1& 3 & 1 &\tikznode{g1}{$1$}\\
2& 5 & 1 &\tikznode{g2}{$-1$}\\
1& -1 & 2 &\tikznode{g3}{$-1$}
\ear\!\!\right) \quad \quad \quad \quad\to 
\left(
\bar{rrr|r}
1& 3 & 1 &1\\
0& -1 & -1 &\tikznode{gg2}{$-3$}\\
0& -4 & 1 &\tikznode{gg3}{$-2$}
\ear\!\!\right)
\quad \quad \to 
\left(
\bar{rrr|r}
1& 3 & 1 &1\\
0& -1 & -1 &-3\\
0& 0 & 5 &\tikznode{ggg3}{$10$}
\ear\!\!\right)
$$
\begin{tikzpicture}[remember picture,overlay,cyan,rounded corners]
  \draw[<-,shorten <=12pt,shorten >=12pt]
  (g2) -- +(1.0,0)  |- (g1);
  \draw[<-,shorten <=12pt,shorten >=12pt]
  (g3) -- +(1.6,0) |- (g1);
  \node (b2) at ($1/2*(g1)+1/2*(g2)+(1.2,-0.1)$) {$\scriptstyle{-2}$};
  \node (b3) at ($1/2*(g1)+1/2*(g3)+(1.8,0)$) {$\scriptstyle{-1}$};
  
  \draw[<-,shorten <=12pt,shorten >=12pt]
  (gg3) -- +(1.0,0)  |- (gg2);
  \node (b4) at ($1/2*(gg2)+1/2*(gg3)+(1.3,-0.1)$) {$\scriptstyle{-4}$};
  \node (b5) at ($(ggg3)+(0.8,0)$) {$ \cdot \frac{1}{5}$};
\end{tikzpicture}

На последнем шаге нижняя строчка даёт уравнение $z=2$. Подставляя $z=2$ в уравнение второй строки мы получаем $-y-2=-3$, откуда $y=1$. Из первой строки находим $x=-4$.

Коротко указанную процедуру можно охарактеризовать так -- избавляемся при помощи одного уравнения от вхождения одной из переменных во все нижние уравнения. Какие вопросы могут нас ждать на этом пути?

\enm
\item Когда можно провернуть первый шаг этого алгоритма? Тогда, когда первый элемент $a_{11}$ делит первый элемент второй строки $a_{21}$. Если $R$ абы какое, то это бывает крайне редко. Поэтому мы ограничим себя случаем $R=K$ -- поле.
\item Даже если рассматривать системы линейных уравнений над полем получается, что для первого шага должно быть выполнено, что $a_{11}\neq 0$. 
\item А ещё в примере число уравнений $m$ и число неизвестных $n$ в системе было одинаковым. Что изменится, если $n \neq m$? Как написать общий ответ?
\eenm


Ответим на эти вопросы. Для этого со всеми деталями опишем метод исключения Гаусса. Продолжим работать в матричной записи. Для начала определимся, какого вида матрицу мы хотим получить после того, как закончим преобразования строк в случае системы произвольного размера.

\dfn Главным элементом строки матрицы $A$ будем называть первый ненулевой элемент в этой строке.
\edfn

\dfn Будем говорить, что матрица $A$ имеет ступенчатый вид, если каждая новая строчка начинается с большего количества нулей, чем предыдущая, либо целиком состоит из нулей. Говоря строго, для $i$-ой строки номер столбца в котором стоит главный элемент строки строго больше, чем аналогичный  номер у $i-1$ строки, если только строка не целиком состоит из нулей.
\edfn

Утверждение, которое стоит за методом Гаусса можно сформулировать следующим образом:

\thrm Любую матрицу над полем $K$ можно привести элементарными преобразованиями строк к ступенчатому виду. Более того, можно добиться того, чтобы для каждой строки её главный элемент был равен 1, а в столбце над ним стояли нули.
\ethrm
Предъявим индукционный алгоритм для получения ступенчатого вида:\\
{\bf Случай 1:} Элемент $a_{11}\neq 0$. Тогда прибавим к $i$-ой строке первую с коэффициентами $-\frac{a_{i1}}{a_{11}}$. Получится матрица у которой в первом столбце стоят нули, кроме первой позиции. Вычеркнем первый столбик и первую строчку и продолжим по индукции.\\
{\bf Случай 2:} Элемент $a_{11}=0$, но в первом столбце в $i$-ой строчке стоит ненулевой элемент. Поменяем строку с номером $i$ с первой строкой и продолжим, как в случае 1.\\
{\bf Случай 3:} Весь первый столбец нулевой. Тогда вычеркнем первый столбец и продолжим по индукции.


Указанные преобразования очевидно приводят матрицу к ступенчатому виду. 
Теперь заработаем единицы в главных элементах строк. Пусть  $a_i$ -- главный элемент строки $i$. Тогда умножим её на $a_i^{-1}$.

Способ добиться нулей над главными элементами называется обратным ходом метода Гаусса.

Будем идти вверх по строкам матрицы, начиная с первой ненулевой строки. Посмотрим на последнюю ненулевую строку -- скажем строку $k$, первый  столбец с ненулевым элементом в которой имеет номер $j_k$, и прибавим её ко всем строкам выше с коэффициентом $-a_{lj_k}$ для $l$-ой строки. После чего перейдём к следующей строке.\\



\noindent {\bf Как при помощи этого всего решить систему?}
Приведём расширенную матрицу системы к ступенчатому виду. Рассмотрим последнюю ненулевую строчку. Если её ненулевой элемент находится в самом последнем отчёркнутом столбце, то решений нет, потому, что эта строчка соответствует уравнению $0x_1+\dots+0x_n=b_1\neq 0$, которое, как ни крути, решений не имеет.

Если же такого не происходит, то разделим переменные на два класса. Каждая переменная соответствует столбцу матрицы. Если в  столбце $1\leq i\leq n$ стоит главный элемент какой-то строки, то $x_i$ будем называть зависимой переменной. Если же в $i$-ом столбце нет главного элемента никакой строки, то $x_i$ будем называть независимой.

Выпишем все независимые переменные $x_{i_1},\dots,x_{i_r}$. Осталось заметить, что, выбрав любые значения для $x_{i_1},\dots,x_{i_r}$,  мы однозначно восстановим значения всех остальных переменных из уравнений.

Более того, значения остальных переменных представляются в виде значений многочленов первой степени от  $x_{i_1},\dots,x_{i_n}$. Так выглядит стандартное описание всех решений линейного уравнения, которое выдаёт метод Гаусса.

\rm Описанный метод подразумевает работу со строчками в определённом порядке. В частности, перестановка строк делается только в экстренных случаях. Но, в принципе, никто не запрещает для удобства переставлять строчки и прибавлять их друг к другу в произвольном порядке -- лишь бы вид системы в конце позволял проанализировать множество решений.
\erm

Давайте разберёмся, где же может пригодиться наш метод решения систем линейных уравнений?

\exm\\
Допустим мы хотим наладить некоторую поисковую систему. Что это значит? Поисковая система индексирует страницы в сети и то, какая страница на какую ссылается. Иными словами, поисковая система видит ориентированный граф $G$, вершины которого -- это страницы в сети, а рёбра проводятся, если один сайт ссылается на другой.

Что же должна сделать поисковая система? Ей неплохо было бы назначить каждому сайту его важность. То есть необходима функция из множества вершин графа в вещественные числа  $W\colon G \to \mb R$. Важность сайта зависит от того, насколько много на него ссылаются. Это приводит  к следующей системе уравнений:
$$w_i=\sum_{j\to i} \frac{1}{d_j^{out}}w_j,$$
где $d_j^{out}$ исходящая степень вершины $j$. 



Приведём пример, в котором возникает линейная система над $\mb Z/2$. Это будет описания довольно эффективного эвристического алгоритма для разложения числа $n$ на множители. 

Заметим, что для того, чтобы разложить нечётное число $n$ на множители достаточно (и необходимо) найти нетривиальное решение сравнения $$x^2=y^2(\mod n),$$
то есть такое решение, где $x\neq \pm y (\mod n)$. В этом случае число $n$ имеет нетривиальный множитель $(n,x-y)$.

Для получения такого разложения можно взять набор всех простых $p_1,\dots,p_h$ меньших некоторой константы $M$. Далее будем случайно брать элементы $x_i$ ( удобно брать их около $\sqrt{n}$) и считать  $x_i^2 \mod n$. Если $h$ большое, то с высокой вероятностью $x_i^2\mod  n$ есть произведение $p_1^{\alpha_{1i}}\dots p_h^{\alpha_{hi}}$. Оставим только такие $x_i$, что для $x_i^2$ есть указанное разложение. Найдём $h+1$ такое $x_i$. Построим теперь решение сравнения. Будем искать $x$ в виде $x=\prod x_i^{\eps_i}$, где $\eps_i \in \{0,1\}$. Посчитаем $x^2 \mod n$. Это число сравнимо с 
$$x^2 =x_1^{\eps_1}\dots x_{h+1}^{\eps_{h+1}} \equiv p_1^{\alpha_{11}\eps_1 +\dots+\alpha_{1h+1}\eps_{h+1}} \dots p_h^{\alpha_{1h}\eps_1 +\dots+\alpha_{h h+1}\eps_{h+1}} \mod n.$$
Если искать $y$ в виде $y=\prod p_i^{\gamma_i}$, то такой точно найдётся, если разрешима система
$$\begin{cases}
\alpha_{11} \eps_1+\dots+\alpha_{1h+1}\eps_{h+1}=0 \mod 2 \\
\vdots\\
\alpha_{h1} \eps_1+\dots+\alpha_{hh+1}\eps_{h+1}=0 \mod 2
\end{cases}$$
Это однородная система из $h+1$ уравнения и $h$ неизвестных над полем $\mb Z/2$. У неё скорее всего есть нетривиальное решение. Это позволяет найти нам $y$. К сожалению, никто не гарантировал, что такой $y$ даёт нетривиальное решение. Но если даёт, то разложение найдено. В указанном алгоритме часто берут $M$ порядка $e^{\sqrt{\ln \ln\ln n}}$. Это приводит к субэкспоненциальному алгоритму для разложения числа на множители -- алгоритму Диксона.

Казалось бы, мы научились решать произвольную систему линейных уравнений -- что же ещё можно спросить? Поставим несколько вопросов, на которые ответим в дальнейшем. Вот первый и основной из них:
\enm
\item Предположим, что две системы эквивалентны, например, потому что получились одна из другой перестановкой строк. Верно ли, что ответ для общего решения в методе Гаусса будет содержать одинаковое число независимых параметров? А что будет, если одна система из другой получается заменой переменных?
\item В методе Гаусса видно, что решающую роль играет матрица системы. Вопрос: как по матрице системы определить, для каких $b$ система будет разрешима?
\item Мы интуитивно догадываемся, что обычно решение системы из $n$ уравнений и $m$ неизвестных описывается $m-n$ параметрами (если это число не отрицательно, конечно). Однако это не всегда так. Вопрос -- найти критерии, когда это выполнено, а когда нет. Это очень полезно, например, в задаче про поисковик. Ведь в этой задаче $n$ уравнений на $n$ неизвестных, но она, очевидно, имеет нулевое решение. Хочется доказать, что нулевое решение  не единственное.
\item Часто решение системы нужно найти не обязательно точно. Например в задаче про поисковик. Никто не умрёт, если ранжирование чуть-чуть поедет. Разрешив неточное решение хочется выиграть на времени его нахождения. Это актуально для больших матриц. Что как раз  и имеет место в задаче о поисковой системе. Размер матрицы там равен числу вершин графа, то есть числу страниц в интернете. Сейчас в интернете более миллиарда страниц.

\item В задаче о разложении числа на множители нам попалась матрица над $\mb Z/2$. Заметим, что каждая строка этой матрицы соответствует разложению $x_i^2 \mod n < n $ на простые. но число не может раскладываться более чем на $\log$ от своего размера различных простых множителей. Поэтому в каждой строке всего $O(\log n)$ ненулевых чисел.  Всего $O(h \log n )$ ненулевых элементов. При выборе $h=e^{\sqrt{\ln n \ln\ln n}}$ получаем, что $(\log h)^2 = \ln n \ln\ln n$. Итого, получаем, что в матрице  $O(h \log^2 h)$ ненулевых элементов, что заметно меньше, чем возможные $h(h+1)$. Матрицы, в которых мало ненулевых элементов называются разреженными. Хочется иметь алгоритмы, быстро решающие системы с разреженными матрицами.
\eenm


\section{Операции над матрицами}

Отвлечёмся на время от ситуации в случае полей и посмотрим ещё на общие конструкции связанные с системой линейных уравнений. Пусть нам дано одно линейное уравнение $a_1x_1+\dots+ a_n x_n =b$. Матрица этой системы это строка $(a_1,\dots,a_n)$. Набор переменных мы будем считать столбцом высоты $n$. Мы хотим ввести операцию, которая восстанавливала бы выражение $a_1x_1+\dots+ a_n x_n$ по строке $a$ и столбцу $x$. До поры будем работать в контексте произвольного кольца.

\dfn Пусть $R$ -- кольцо. Определим произведение строки $a\in M_{1\times n}(R)$ на столбец $x\in M_{n\times 1}(R)$ в виде
$$ax= a_1x_1+\dots+a_nx_n.$$
\edfn 

Условимся обозначать множество столбцов $M_{n\times 1}(R)$ просто как $R^n$. Определение произведения мгновенно переносится на ситуацию, когда $A$ матрица. 

\dfn Пусть $R$ -- кольцо, $A\in M_{m\times n}(R)$, $x\in R^n$. Определим произведение $Ax$ как столбец из $R^m$, каждый элемент которого есть произведение строки из $A$ на столбец $x$,
$$Ax= \pmat a_{11} & \dots & a_{1n}\\
\vdots & \ddots & \vdots \\
a_{m1}&\dots& a_{mn} \epmat \pmat x_1\\ \vdots \\ x_n \epmat = \pmat a_{11}x_1+\dots+a_{1n}x_n\\ \vdots \\ a_{m1}x_1+\dots+a_{mn}x_n \epmat .$$
\edfn


Это определение позволяет нам записать систему линейных уравнений в компактном виде
$$Ax=b.$$

\rm Заметим, что задав операцию произведения, мы для каждой матрицы $A$ задали отображение $R^n \to R^m$, переводящее столбец $x\to Ax$.
\erm

Допустим мы хотим вычислить значение этого отображения одновременно на $k$ столбцах $e_i \in \mb R^n$. Организуем эти столбцы в матрицу $B=\pmat e_1 & \dots & e_k \epmat \in M_{n\times k}(R)$. Определим произведение матрицы $A\in M_{m\times n}(R)$ на матрицу $B$ как матрицу $C \in M_{m\times k}(R)$, чья $j$-ый столбец имеет вид $Ae_j$. Распишем это

\dfn Пусть $A\in M_{m\times n}(R)$ $B \in M_{n\times k}(R)$. Тогда произведение $AB$ это такая матрица $C \in M_{m\times k }(R)$, что 
$$C_{ij} = \sum_{k=1}^n A_{ik}B_{kj}$$
при $1\leq i\leq m$ и $1\leq j \leq k$.
\edfn

Основным свойством этой операции является ассоциативность

\utv Пусть $A\in M_{m\times n}(R), B \in M_{n\times l}(R), C\in M_{l\times s}(R)$. Тогда
$$(AB)C= A(BC).$$
\eutv

\noindent В некоторых задачах особенно важно научиться возводить матрицу в степенью.\\
\exm\\
1) Вспомним про задачу с популяцией. Пусть дано $n$ городов и в начальный момент времени. Количество жителей, которые находятся в городе $i$ обозначим за $x_i$. Пусть известно, что ежегодно доля жителей переезжающих из города $j$ в город $i$ -- это $a_{ij}$. Составив из $a_{ij}$ матрицу $A\in M_{n\times n}(\mb R)$ получаем, что для того, чтобы узнать число жителей в городах через год нам надо найти столбец $Ax$. Если же мы хотим понять, сколько, при фиксированных тенденциях будет жителей в городах через $k$ лет, то необходимо понять, как ведёт себя выражение 
$$A^k x \text{ -- популяция через $k$ лет}.$$
2) Пусть $A$ -- матрица смежности графа. Тогда $(A^k)_{ij}$ -- это количество путей длины $k$ из вершины $j$ в вершину $i$. В частности, $(A^3)_{ii}/2$ -- это количество треугольников  содержащих вершину $i$. Всего же, число треугольников в графе есть 
$$\frac{1}{6}\sum_{i=1}^n (A^3)_{ii}.$$
Если научиться быстро считать умножение матриц, то можно быстро посчитать число треугольников (или других циклов).\\

Однако кроме операции произведения есть и другие, более привычные и простые.

\dfn Пусть $A,B\in M_{m\times n}(R)$. Тогда определим сумму матриц как 
$$(A+B)_{ij}=A_{ij}+B_{ij}.$$
\edfn

\dfn Пусть $r\in R$, $A\in M_{m\times n}(R)$. тогда определим $rA$ как 
$$(rA)_{ij}=rA_{ij}.$$
\edfn

\utv Пусть $R$ -- кольцо. Тогда\\
1) $M_{m\times n}(R)$ относительно сложения это абелева группа.\\
2) $(A+B)C=AC+BC$\\
3) $C(A+B)=CA+CB$.\\
4) $r(AB)=(rA)B=A(rB)$.\\
5) $(r_1+r_2)A=r_1A+r_2A$.\\
6) $r(A+B)=rA+rB$.
\eutv

\crl Если $x_0\in R^n$, решение системы линейных уравнений $Ax=b$, где $A\in M_{m\times n}(R)$, $b\in R^m$, то любое другое решение $x'$ этой системы имеет вид $x'=x_0+y$, где $Ay=0$, -- есть решение однородной системы. Обратно, если $Ay=0$, то любое $x'=x_0+y$ есть решение  $Ax=b$.
\proof Определим $y=x'-x_0$. Тогда $$Ay=A(x'-x_0)=Ax'-Ax_0=b-b=0.$$
Если теперь $Ay=0$, то 
$$A(x_0+y)=Ax_0+Ay=b.$$

\endproof
\ecrl

\crl Пусть $y_1,y_2$ решение однородного уравнения $Ay=0$. Тогда $y_1+y_2$  и $ry_1$ решение уравнения $Ay=0$.
\ecrl

\crl Если уравнение $Ax=b$ разрешимо, то мощность множества его решений такая же как и у уравнения $Ax=0$. 
\ecrl





\section{Векторные пространства}

Базовым объектом линейной алгебры является векторное пространство.

\dfn[Векторное пространство]
Векторным пространством над полем $K$ называется множество $V$ вместе с отображениями $+\colon V\times V \to V$ и $\cdot \colon K \times V \to V$, удовлетворяющее свойствам:\\
1) $V$ относительно сложения -- это абелева группа\\
2) $\forall v \in V$ верно, что $1\cdot v=v$\\
3) $\forall v \in V$, $\forall \lambda, \mu \in K$ верно, что $(\lambda+\mu)\cdot v= \lambda\cdot v + \mu \cdot v$.\\
4) $\forall u,v \in V$, $\forall \lambda \in K$ верно, что $\lambda\cdot(u+v)= \lambda\cdot u + \lambda \cdot v$.\\
5) $\forall v \in V$ $\forall \lambda, \mu \in K$ верно, что $(\lambda\mu)\cdot v= \lambda\cdot(\mu \cdot v)$.
\edfn





\exm\\
0) Само поле $K$ вместе со сложением и умножением.\\
1) Пространство столбцов $K^n$. Умножение и сложение покомпонентное.\\
2) Обобщая. Пространство матриц $M_{m\times n}(K)$.\\
3) Пусть $X$ -- множество. Рассмотрим множество всех функций  из $K$ в $X$ , то есть $K^X$. Это векторное пространство над полем $K$ с поточечным сложением и умножением.\\
4) Рассмотрим множество непрерывных вещественнозначных функций на отрезке $[0,1]$. Это векторное пространство над $\mb R$.\\
5) Рассмотрим множество последовательностей над полем $K$, удовлетворяющих заданному линейному рекуррентному соотношению $a_k x_{n+k}+\dots+a_0x_n=0$. Это векторное пространство над $K$.\\
6) Рассмотрим множества всех многочленов $K[x]$\\
7) $\mb C$ является векторным пространством над $\mb R$\\



Однако обычно векторные пространства возникают в следующей ситуации.





\dfn[Подпространство] Пусть $V$ -- векторное пространство над полем $K$. Подмножество $U\subseteq V$ называется подпространством $V$, если\\
1) $U$ -- подгруппа $V$.\\
2) $\forall \lambda \in K$, $\forall u \in U$ верно, что $\lambda u \in U$.\\
По другому говоря,  операции на $V$ можно сузить на $U$, с тем, чтобы $U$ стало векторным пространством относительно этих операций.
\edfn

\exm\\
1) Множество решений уравнения $Ax=0$, где матрица $A \in M_{m \times n}(K)$ образует подпространство в $K^n$. Это один из самых интересных для нас примеров.\\
2) Рассмотрим множество непрерывных на отрезке $[0,1]$ функций, принимающих значение $0$  в точках $0, \frac{1}{2}, 1$. Это подпространство в $C([0,1])$.\\
3) Рассмотрим множество многочленов степени не выше $n$ от одной переменной $$K[x]_{\leq n}=\{ f \in K[x]\,|\, \deg f\leq n\}.$$ Это подпространство в  $K[x]$.\\
4) Рассмотрим множество правильных дробей $\frac{f}{g}\in K(x)$. Это подпространство в $K(x)$.\\

\utv[Простейшие свойства] Пусть $V$ -- векторное простраснтво над полем $K$. Тогда  \\
1) $0\cdot v =0 \in V$ для всех $v\in V$\\
2) $(-1)\cdot v=-v$ для всех $v\in V$\\
3) $\lambda \cdot 0 = 0$ для всех $\lambda\in K$.
\eutv


\rm Подпространства наследует с объемлющего пространства структуру и потому сами являются примерами векторных пространств.
\erm


Если дан некоторый набор векторов из пространства, то любое подпространство, их содержащее, должно содержать их всевозможные суммы с любыми коэффициентами. Для этой ситуации в линейной алгебре есть специальный термин.

\dfn(Линейная комбинация) Линейной комбинацией векторов $v_1,\dots, v_n$ с коэффициентами $\lambda_1, \dots, \lambda_n$, называется выражение
$$\lambda_1 v_1 +\dots + \lambda_n v_n.$$
Будем говорить, что элемент $v\in V$ представим в виде линейной комбинации векторов $v_1,\dots,v_n$, если
$$v=\lambda_1 v_1 +\dots + \lambda_n v_n.$$
Если хотя бы один из элементов $\lambda_1,\dots, \lambda_n $ не равен 0, то говорят, что линейная комбинация нетривиальна.
\edfn



\dfn Пусть $X \subseteq V$. Тогда линейная оболочка $\lan X \ran$ называется множество всех линейных комбинаций.
$$\lan X \ran = \{ v\in V\,|\, \exists\, v_1,\dots,v_k\in X\,\,  \lambda_1,\dots,\lambda_k \in K, \text{ что } v=\lambda_1v_1+\dots+\lambda_kv_k\}$$
\edfn

\dfn Будем говорить, что набор $v_{\alpha} \in V$ $\alpha \in I$ является порождающим для $V$, если $\lan \{v_{\alpha}\}_{\alpha \in I}\ran= V$. Иными словами, для любого $v \in V$ существуют  $\lambda_1,\dots,\lambda_n$, что $v=\sum \lambda_i v_i $.
\edfn

\lm Рассмотрим набор $v_1,\dots,v_n \in V$. Пусть $w_1=\mu_{11}v_1+\dots+\mu_{1n}v_n$, $\dots$, $w_m= \mu_{m1}v_1+\dots+\mu_{mn}v_n$. Рассмотрим набор $\lambda_1,\dots, \lambda_m$. Тогда вектор $w=\sum \lambda_i w_i$ является линейной комбинацией набора $v_i$.
\elm

\crl $\lan X \ran$ является подпространством в $V$, наименьшим среди содержащих $X$.
\ecrl


\dfn[Линейная зависимость] Набор векторов $v_1,\dots,v_n$ называется линейно зависимым, если 0 является нетривиальной линейной комбинацией $v_1,\dots, v_n$, то есть существуют  $\lambda_1, \dots, \lambda_n \in K$ не все равные 0, что
$$0=\lambda_1v_1+\dots+\lambda_n v_n.$$
\edfn

\dfn[Линейная независимость] Набор векторов $v_1,\dots,v_n$ называется линейно независимым, если он не является линейно зависимым, то есть если $\lambda_1, \dots, \lambda_n \in K$ такие, что $$0=\lambda_1v_1+\dots+\lambda_n v_n, \text{ то $\lambda_1=\dots=\lambda_n=0$}.$$
\edfn


\exm \\
0) Набор из одного нуля линейно зависим.\\
1) Пусть $v_1$ и $v_2$ два вектора из $V$. Они линейно зависимы тогда и только тогда, когда они пропорциональны.\\
2) Рассмотрим пространство $K^n$ и набор столбцов
$$e_1=\pmat 1\\0\\ \vdots\\ 0\epmat,\, \dots, e_n=\pmat 0\\ 0 \\ \vdots \\ 1 \epmat.$$
Это линейно независимая система векторов. \\
3) Аналогично в пространстве матриц $M_{m \times n}(K)$ имеется набор матриц $e_{ij}$ вида
$$ e_{ij}=
\bordermatrix{
 & &j&& \cr
 &0&\cdots&\cdots&0\cr
 &\vdots&\ddots && \vdots\cr
i&\vdots& 1 & \ddots& \vdots\cr
 &0&\cdots& \cdots&0
}
$$\\
4) Любой поднабор линейно независимого набора линейно независим.\\


\thrm[О линейной зависимости линейных комбинаций.] Пусть $u_1,\dots,u_m$ и $v_1,\dots,v_n$ два набора векторов. При этом все вектора $u_i$ являются линейными комбинациями $v_j$, то есть $u_i=\sum_{j=1}^n \lambda_{ij}v_j$. Тогда, если $m>n$, то $u_i$ обязательно линейно зависимы.
\ethrm
\proof Индукция по $n$. $n=1$. Все вектора $u_i$ кратны $v_1$ и пропорциональны друг другу, то есть линейно зависимы.

Будем поступать как в методе Гаусса. Запишем $u_i$ в виде
$u_i=\sum_{j=1}^n \lambda_{ij}v_j$. Если для некоторого индекса $j$ все $\lambda_{ij}=0$, то можно воспользоваться предположением индукции.

Рассмотрим вектор $u_i$, что $\lambda_{in}\neq 0$. Тогда перейдём к набору $$u_s'=u_s - \frac{\lambda_{sn}}{\lambda_{in}}u_i= \sum_{j=1}^{n-1} \mu_{sj} v_j, \,\,\,\, s\neq i.$$
Это набор из $m-1$ элемента, которые суть линейные комбинации $v_1,\dots,v_{n-1}$. Этот набор линейно зависим по индукционному предположению, то есть существуют $\nu_{s}$ не все равные нулю, что
$$0=\sum \nu_s u_{s}'= -\left(\sum\nu_s\frac{\lambda_{sn}}{\lambda_{in}}\right)u_i +\sum \nu_s u_s.$$
Заметим, что не все коэффициенты при $u_s$ равны нулю. Таким образом, мы получили нетривиальную линейную зависимость.
\endproof






\dfn[Базис] Набор векторов $v_{\alpha}$, $\alpha \in I$ называется базисом пространства $V$, если он является порождающей и линейно независимой системой векторов в $V$. \edfn

\exm\\
1) Набор векторов $e_i \in K^n$ является базисом. Этот базис называют стандартным.\\
2) Набор $e_{ij}$ является базисом $M_{m\times n}(K)$.\\
3) Мономы $1,x, \dots, x^n$ являются базисом $K[x]_{\leq n}$.\\
4) $1,i$ -- базис $\mb C$ над $\mb R$.\\
5) Базис $\mb C(x)$ над $\mb C$ это $\frac{1}{(x-\lambda)^n}$, где $\lambda \in \mb C$, $n\in\mb N$ и мономы $1,x,\dots, x^n, \dots$. Мощность этого базиса континуальна.








\lm[Переформулировки] Пусть $e_1,\dots,e_n$ -- набор элементов пространства $V$.
Тогда следующие свойства эквивалентны: \\
1) $e_1,\dots,e_n$ -- базис $V$.\\
2) $e_1,\dots,e_n$ -- минимальный по включению среди порождающих $V$ наборов векторов.\\
3) Для любого $v\in V$ существуют единственные $\lambda_1,\dots,\lambda_n \in K$, что $v=\lambda_1e_1+\dots+\lambda_ne_n$. Такие элементы $\lambda_1,\dots,\lambda_n$ называются координатами вектора $v$ в базисе $e$.\\
4) $e_1,\dots,e_n$ -- максимальный по включению набор среди линейно независимых наборов векторов из $V$.
\proof 1)  в 2). Пусть набор не минимален. Тогда в нём есть лишний вектор. Например, $e_1$. Выкинем его. Но тогда $e_1=\sum_{i\geq 2}\lambda_i e_i$. Это даёт нетривиальную линейную зависимость.

2) в 3). Существование разложения следует из порождаемости. Пусть $\sum \lambda_i e_i =\sum \mu_i e_i$. Тогда либо все $\mu_i=\lambda_i$, либо, скажем, $\mu_1-\lambda_1 \neq 0$. Тогда  $e_1=\frac{1}{\mu_1-\lambda_1}\sum_{i\geq 2} (\mu_i-\lambda_i) e_i$. Тогда из системы можно выкинуть $e_1$ и всё равно будет порождающая система.

3) в 4). Единственность разложения для 0 означает линейную независимость. Покажем максимальность. Пусть система не максимальна. Тогда к ней можно добавить вектор $v\neq 0$ и она останется независимой. Но такого не может быть, так как $v=\sum \lambda_i e_i$, что очевидно даёт линейную зависимость.

4) в 1). Пусть система не порождает $V$. Тогда добавим к ней вектор $v \in V\setminus \lan e_1,\dots,e_m \ran$. Если система зависима, то есть $\lambda v+ \sum \lambda_i e_i=0$, то либо $\lambda=0$ и тогда получаем, что $e_i$ зависимы, что не так, либо $v=-\lambda^{-1} \sum \lambda_i e_i$, что противоречит определению $v$. Таким образом, система $v,e_1,\dots, e_n$ независима, что противоречит максимальности.
\endproof
\elm

\dfn Пространство $V$ называется конечномерным, если оно порождено конечной системой векторов $e_1,\dots,e_n$.
\edfn

\thrm[Теорема о дополнении до базиса] Пусть $V$ -- конечномерное пространство. Тогда любой набор линейно независимых векторов $e_1,\dots,e_m$ можно дополнить до базиса при помощи элементов заданного конечного порождающего множества $v_1,\dots,v_n$. Число $m$ может быть равно нулю.
\proof Будем добавлять элементы из порождающего множества к линейно независимой системе до тех пор, пока полученный набор будет оставаться линейно независимым.
Индукция по числу элементов из порождающего множества не лежащих в линейной оболочке $\lan e_1,\dots, e_m\ran$. База. Если все элементы задействованы, то
$$ V=\lan v_1,\dots,v_n \ran \subseteq \lan e_1, \dots, e_m \ran \subseteq V.$$
Значит в цепочке всюду равенства. Значит, система $u_i$ является порождающей, и, следовательно, базисом.

Теперь рассмотрим порождающий набор $v_1,\dots,v_n$. Пусть элемент $v_i$ не лежит в $\lan e_{i}\ran$. Рассмотрим новую систему $ e_1,\dots,e_m,v_i$. Эта система линейно независима и, очевидно, в её линейной линейной лежит дополнительно вектор $v_i$ из порождающей системы. Тогда по индукционному предположению её можно дополнить до базиса.
\endproof
\ethrm

\crl В любом конечномерном пространстве есть конечный базис.
\proof Пустое множество векторов линейно независимо. Дополним его до базиса при помощи конечной порождающей системы.
\endproof
\ecrl


\thrm[Теорема о равномощности базисов] Пусть $V$ -- конечномерное пространство. Тогда любые два базиса $V$  равномощны.
\proof Можно считать, что один базис $e_1,\dots,e_n$ конечен. Рассмотрим другой базис $X \subseteq V$. Если $X$ бесконечно, то есть различные $f_1,\dots,f_{n+1}\in X$. Заметим, что $f_i$ линейно независимы как элементы базиса $X$. Но это противоречит  теореме о линейной зависимости линейных комбинаций так как $f_i$  выражаются через $e_j$. Пусть теперь $X$ конечно. Тогда, применяя тот же аргумент получаем $|X|\leq n$. Заметим, что, наоборот,  $e_j$ выражаются через элементы $X$. Тогда $n\leq |X|$. То есть $|X|=n$.
\endproof
\ethrm

\rm В такой постановке эта теорема верна для любого векторного пространства. Однако общее доказательство требует некоторой специфической техники из теории множеств.
\erm



\dfn[Размерность] Пусть $V$ -- конечномерное векторное пространство. Размерностью $V$ называется количество элементов в каком-то базисе $V$.
\edfn

\rm В дальнейшем будем считать, что все пространства, кроме тех, что встречаются в примерах -- конечномерны.
\erm

\lm Число элементов в любой линейно независимой системе $v_1,\dots,v_k$ в $V$ меньше или равно размерности $V$. Более того, равенство $k=\dim V$ достигается тогда и только тогда, когда $v_1,\dots,v_k$ -- базис $V$.
\elm
\proof Если $v_1,\dots,v_k$ -- линейно независимы, то их можно дополнить до базиса $V$. Но тогда их количество меньше или равно числу элементов в базисе $V$, то есть размерности $V$.
Аналогично, если $k=\dim V$, то мы не можем дополнить эту систему ни одним вектором до линейно независимой, так как получится линейно независимая система слишком большого размера. Таким образом, мы имеем максимальную линейно независимую систему, то есть базис $V$. Обратное утверждение вытекает из определения размерности.
\endproof

Сформулируем аналогичную лемму для порождающих систем

\lm Число элементов в любой конечной порождающей системе $v_1,\dots,v_k$ в $V$ больше или равно размерности $V$. Более того, равенство $k=\dim V$ достигается тогда и только тогда, когда $v_1,\dots,v_k$ -- базис $V$.
\elm



\thrm Любое подпространство $U$ конечномерного пространства $V$ конечномерно и $\dim U \leq \dim V$. Равенство достигается только в случае $U=V$.
\ethrm
\proof Рассмотрим линейно независимую систему $u_1,\dots,u_m$ в $U$ самого большого размера. Такая есть, так как линейно независимая система в $U$ лежит в $V$ и, следовательно, ограничена размерностью $V$. Заметим, что к линейно независимой системе векторов $ u_1,\dots,u_m $, нельзя добавить элемент из $U$, так чтобы она осталась линейно независимой. Таким образом, это максимальная по включению линейно независимая система в $U$, то есть конечный базис $U$. Как мы уже заметили, число $m$ меньше или равно, чем размерность $V$ и, следовательно, $m=\dim U \leq \dim V$. Заметим теперь, что если $m=\dim V$, то $u_1,\dots,u_m$ -- базис $V$ и, следовательно, $U=V$.
\endproof



\exm\\
1) Размерность пространства $M_{m\times n}(K)$ равна $mn$. Базисом является $e_{ij}$. Такой базис называется стандартным\\
2) Размерность пространства $K[x]_{\leq n}=n+1$. Базис $1,x,\dots,x^n$.\\
3) Размерность $\mb C$ над $\mb R$ равна 2.\\

Понятие размерности можно использовать для того, чтобы находить уравнения которым удовлетворяют различные числа.

\dfn Число $\alpha \in \mb C$ называется алгебраическим, если существует такой многочлен $p(x)\in \mb Q[x]$, что $p(x)\neq 0$ и $p(\alpha)=0$.
\edfn

\utv Пусть $\alpha \in \mb C$ -- алгебраическое число, а $f(x)\in \mb Q[x]$ -- некоторый многочлен. Тогда $\beta=f(\alpha)$ -- тоже алгебраическое число.
\eutv
\proof Рассмотрим $\mb Q$-векторное подпространство в $\mb C$ вида $V=\lan 1,\alpha,\dots,\alpha^k,\dots\ran \leq \mb C$. Это подпространство конечно порождено. А именно, если взять многочлен $p(x)\in \mb Q[x]$ из определения алгебраичности для $\alpha$, то 
$$V=\lan 1,\alpha,\dots, \alpha^{n-1}\ran,$$
где $n=\deg p(x)$. Значит $\dim V\leq n$ (при некотором ограничении на $p(x)$ будет и равенство). Значит любой набор из $n+1$ вектора из $V$ линейно зависим. Заметим, что $1,\beta,\dots,\beta^n$ лежат в $V$. Значит они линейно зависимы. Но тогда  есть такие коэффициенты $a_0,\dots,a_n\in \mb Q$ не все равные нулю, что $$a_0+a_1\beta+\dots+a_n\beta^n=0.$$
Но это и значит, что $\beta$ -- алгебраическое число. 
\endproof

\section{Сумма подпространств}

Мы уже обсудили, что пересечение подпространств это подпространство. Есть и другая конструкция, которая является аналогом объединения подпространств.

\dfn[Сумма] Пусть $U_1$ и $U_2$ -- подпространства в пространстве $V$. Тогда определим сумму $U_1$ и $U_2$ как 
$$U_1+U_2=\{u_1+u_2\,|\, u_1\in U_1, \,u_2\in U_2\}.$$
\edfn

\rm Сумма $U_1+U_2$ -- подпространство в $V$. \erm

\rm Кроме суммы подпространств можно рассмотреть и их теоретико множественное пресечение $U_1\cap U_2$. Несложно заметить, что это тоже подпространство. 
\erm

\thrm[Формула Грассмана] Пусть $U_1,U_2$ подпространства конечномерного пространства  $V$. Тогда 
$$\dim (U_1+U_2) + \dim U_1\cap U_2 = \dim U_1 + \dim U_2.$$
\ethrm
\proof Доказательство будет основано на подходящем выборе базиса. Начнём с пересечения $U_1\cap U_2$. Выберем некоторый базис $e_1,\dots,e_k$ в пересечении. Дополним его элементами $f_1,\dots,f_l$ до базиса $U_1$ и элементами $g_1,\dots,g_m$ до базиса $U_2$. Покажем, что $e_1,\dots,e_k, f_1,\dots,f_l, g_1,\dots,g_m $ это базис $U_1+U_2$. 


Понятно, что это порождающая система. Покажем, что этот набор линейно независим. Рассмотрим линейную комбинацию
$$\lambda_1e_1+\dots+\lambda_ke_k +\mu_1 f_1+\dots+\mu_lf_l+ \nu_1g_1+\dots+\nu_m g_m=0.$$
Перепишем это как 
$$\lambda_1e_1+\dots+\lambda_ke_k +\mu_1 f_1+\dots+\mu_lf_l=-(\nu_1g_1+\dots+\nu_m g_m).$$
Левая часть лежит в $U_1$, а правая часть в $U_2$. Значит обе суммы лежат в пересечении. Посмотрим на сумму $\nu_1g_1+\dots+\nu_m g_m\in U_1\cap U_2$. Как вектор из пересечения она расписывается через $e_i$. Но такое может быть только если $\nu_i=0$ так как набор $e_1,\dots,e_k,g_1,\dots,g_m$ линейно независим.  Значит
$$\lambda_1e_1+\dots+\lambda_ke_k +\mu_1 f_1+\dots+\mu_lf_l=0.$$
Но набор $e_1,\dots,e_k,f_1,\dots,f_l$ тоже линейно независим. Откуда оставшиеся коэффициенты так же равны нулю.

Посчитаем теперь размерности $\dim U_1+\dim U_2= k+l+k+m=2k+l+m$. С другой стороны $\dim (U_1+U_2) + \dim U_1\cap U_2= k+l+m+k=2k+l+m$. Что и доказывает нужное равенство.
\endproof

Покажем некоторые эвристические следствия из формулы Грассмана. Представим себе, что мы случайно выбираем подпространства $U_1$ и $U_2$ фиксированной размерности $k$ и $m$. Какова ожидаемая размерность суммы $U_1+U_2$ и размерность пересечения $U_1\cap U_2$?  Для этого, конечно, неплохо бы сказать, что такое случайное подпространство. Посмотрим на вещественную картинку.  Будем просто случайно выбирать вектора (на единичной сфере) и смотреть, какое пространство они порождают. Если присмотреться, то становится понятно, что случайно выбранные $k$ векторов с вероятностью 1 порождают $k$-мерное пространство, если, конечно $k\leq \dim V=n$. 

Итак, что мы можем сказать про сумму случайно выбранных подпространств размерности $k$ и $m$? Исходя из нашего определения это просто будет подпространство порождённое $k+m$ случайно выбранными векторами. Соответственно, если $k+m\leq n$, то ожидаем, что $\dim (U_1+U_2)=k+m$, а если $k+m > n$, то $\dim (U_1+U_2)=n$. Заметим, что в любой ситуации есть неравенство
$$\dim (U_1+U_2)=\dim U_1+\dim U_2 - \dim U_1\cap U_2 \leq \max(k+m,n).$$

Посмотрим на пересечение. Тут удобно воспользоваться формулой Грассмана. Получаем, что ожидаемая размерность будет $\dim U_1 + \dim U_2 - \dim (U_1+U_2)$ то есть $0$ при $k+m \leq n$ и $k+m-n$ при $k+m>n$. Удобно ввести понятие коразмерности 

\dfn Пусть $U\leq V$, причём размерность $V$ равна $n$, а размерность $U$ равна $m$. Тогда коразмерностью $U$  в $V$ называется число  
$$\codim U =n-m.$$
\edfn

В этих терминах получаем, что ожидаемая коразмерность пересечения $\codim U_1\cap U_2$ есть $\codim U_1 + \codim U_2$, если $\codim U_1 + \codim U_2 \leq n$ и $n$ иначе. То есть мы ожидаем, что коразмерности складываются. Воспользовавшись формулой Грассмана в общем случае получаем неравенство
\begin{align*} \codim U_1 \cap U_2 = n -\dim U_1 \cap U_2=& n+\dim (U_1+U_2)-\dim U_1-\dim U_2=\\
=&\codim U_1+\codim U_2 - \codim (U_1 +U_2) \leq \codim U_1+\codim U_2.
\end{align*}

Попробуем посмотреть, можно ли обобщить формулу Грассмана. Сама формула появилась у нас как аналог формулы включения-исключения. Может быть она верна и для большего числа подпространств? 

Ответ: <<Нет>>. Дело в том, что для доказательства формулы включения-исключения используются правила де Моргана $U\cap (V \cup W)=(U\cup V)\cap (U\cup W)$. Оказывается, что аналоги этих формул не верны в общей ситуации. Точнее, возьмём в качестве объемлющего пространства  $\mb R^2$, а в качестве $U,V,W$ три различный прямые проходящие через 0. В этой ситуации $U + (V \cap W)=U+0=U$, а $(U+V)\cap(U+W)=\mb R^2 \cap \mb R^2= \mb R^2$. 


Часто бывает необходимо разделить информацию на важный кусок и неважный. Если информация -- это вектор в $n$ мерном пространстве, то это обычно означает, что мы представляем эту точку в виде суммы двух векторов, первый из которых лежит в одном подпространстве, а второй в другом. Такое разложение должно быть единственным. Эта ситуация формализуется при помощи понятия о прямой сумме.

\dfn Будем говорить, что $V$ раскладывается в прямую сумму своих подпространств $U_1$ и $U_2$ и записывать это как
$$V=U_1\oplus U_2,$$
если $\forall v\in V$ существуют единственные $u_1\in U_1$, $u_2\in U_2$, что $v=u_1+u_2$.
\edfn

Вектор $u_1$ будем называть проекцией $v$ на $U_1$ вдоль $U_2$ и обозначать $u_1=pr_{U_1}v$.

\thrm[Критерий разложения в прямую сумму] Пусть $U_1$ и $U_2$ -- подпространства в пространстве $V$. Тогда следующие условия эквивалентны\\
1) $V=U_1\oplus U_2$\\
2) $V=U_1+U_2$ и $U_1\cap U_2=\{0\}$.\\
3) $U_1\cap U_2=\{0\}$ и $\dim U_1+\dim U_2=\dim V$.\\
4) Для любых базисов $e_1,\dots,e_k$   и $f_1\dots,f_l$ пространств $U_1$ и $U_2$ соответственно, что  $e_1,\dots,e_k,f_1,\dots,f_l$ -- базис $V$\\
5) Существуют базисы $e_1,\dots,e_k$   и $f_1\dots,f_l$ пространств $U_1$ и $U_2$ соответственно, что $e_1,\dots,e_k,f_1,\dots,f_l$ -- базис $V$.
\ethrm
\proof Из 1) в 2). Условие про сумму очевидно. Покажем, что пересечение нулевое. Пусть вектор $u \in U_1\cap U_2$. Запишем равенства $u=u+0$ и $u=0+u$. Это два разложения для $u$ при помощи элементов $u_1$ и $u_2$. По условию прямой суммы такое разложение единственно. Значит $u=0$.


Покажем переход 2) в 3). Надо сравнить размерности. Однако это напрямую следует из формулы Грассмана. Действительно имеем $\dim U_1 + \dim U_2=\dim (U_1+U_2) + 0 = \dim V$.


Из 3) в 4). Пусть $e_1,\dots,e_k$ -- базис пространства $U_1$ и $f_1\dots,f_l$ -- базис пространства $U_2$. Заметим, что по условию $k+l=\dim V$. Значит достаточно показать, что объединённая система векторов  линейно независима. Пусть 
$$\lambda_1 e_1+\dots+\lambda_ke_k+\mu_1f_1+\dots+\mu_lf_l=0.$$
Перенесём 
$$\lambda_1 e_1+\dots+\lambda_ke_k=-(\mu_1f_1+\dots+\mu_lf_l).$$
Получаем, что обе части равенства одновременно лежат в пересечении. Но пересечение состоит только из нуля. Значит обе части равенства нулевые. Так как $e_i$ и $f_j$ базисы получаем, что $\lambda_i=0$ и $\mu_j=0$.\\


Из 4) в 5) переход очевиден. Из 5) в 1). Рассмотрим вектор $v\in V$. Тогда вектор $v$ раскладывается по базису
$$v=\sum \lambda_i e_i + \sum \mu_j f_j.$$
Первая сумма -- это вектор из $U_1$, а второй из $U_2$. Покажем единственность. Возьмём разложение, возможно отличное от того, которое получили: $v=u_1+u_2$. Разложим в свою очередь $u_1=\sum \lambda_i'e_i$, а $u_2=\sum \lambda_j' f_j$. Но тогда это даёт другое разложение $v$. Но разложение по базису единственно. Значит $\lambda_i=\lambda_i'$ и $\mu_j=\mu_j'$. Но тогда и разложения одинаковые.

\endproof



\section{Линейные отображения}

\dfn[Линейное отображение] Пусть $U,V$ -- два векторных пространства над полем $K$. Отображение $L\colon U \to V$ называется линейным, если\\
1) $\forall a,b \in U$ верно, что $L(a+b)=L(a)+L(b)$.\\
2) $\forall a \in U$, $\lambda \in K$ верно, что $L(\lambda a)=\lambda L(a)$.
\edfn



\exm\\
0) Пусть $U$ и $V$ два пространства. Тогда отображение $U \to V$ вида $x \to 0$ является линейным.\\
1) Пусть задана матрица $A \in M_{m\times n}(K)$. Тогда отображение $K^n \to K^m$, заданное как $x \to Ax$ линейно.\\
2) Пусть задан многочлен $p(x) \in K[x]$. Тогда отображение $K[x] \to K[x]$ $f(x)\to p(x)f(x)$ линейно.\\
3) Пусть $p(x) \in K[x]$ -- многочлен. Тогда отображение $K[x] \to K[x]$, заданное как  $f(x)\to f(p(x))$ линейно.\\
4) Предыдущая конструкция работает и в другом контексте. Пусть $g(x)\in C(\mb R)$. Тогда отображение $C(\mb R)\to C(\mb R)$ вида $f(x)\to f(g(x))$ является линейным.\\
5) Рассмотрим пространство непрерывных функций $C([0,1])$. Тогда отображение $f(x) \to \int_{0}^1 f(x)dx$ линейно  как отображение $C([0,1])\to \mb R$.\\
6) Пусть $V=U_1\oplus U_2$. Тогда отображение $pr_{U_1} \colon V \to U_1$ заданное правилом $v \to u_1$, где $v=u_1+u_2$ разложение в сумму элементов из  $U_1$ и $U_2$.\\


Линейное отображение -- частный случай гомоморфизма групп. Поэтому здесь возникают стандартные понятия:






\dfn[Ядро и образ] Пусть $L\colon U \to V$ -- линейное отображение. 
$$\Ker L=\{x\in U\,|\, Lx=0\}\subseteq U.$$
$$\Im L=\{y\in V\,|\, \exists x\in U\, Lx=y\}\subseteq V.$$
\edfn

\rm Линейное отображение, как частный случай гомоморфизма групп, инъективно тогда и только тогда, когда $\Ker L=\{0\}$.
\erm

Итак, благодаря примерам, мы поняли, что отображение $K^n \to K^m$, заданное с помощью матрицы $A$ это частный случай линейного отображения. Таким образом, решение системы линейных уравнений можно понимать как нахождение прообраза элемента при линейном отображении.

Сюръективность или инъективность такого линейного отображения имеет непосредственное отношение к вопросам о разрешимости системы линейных уравнений. А именно, система $Ax=y$ разрешима для любого $y$, тогда и только тогда, когда отображение $x\to Ax$ сюръективно. Так же, система $Ax=0$ имеет единственное решение в случае инъективности $x\to Ax$.

В дальнейшем мы увидим, что все вычисления с  линейными отображениями может быть сведено к матрицам. Тем самым, линейное отображение между пространствами столбцов есть  модель для всех задач про линейные отображения. Но прежде посмотрим, на общие качественные свойства линейных отображений.



\lm[Базовые свойства] Для линейных отображений имеют место следующие свойства:\\
0) Пусть $L$ -- линейное отображение. Тогда $L(0)=0$.\\
1) Пусть $L_1$ и $L_2$ -- два линейных отображения $U \to V$. Тогда $L_1+L_2$ -- их поточечная сумма -- тоже линейное отображение $U\to V$.\\
2) Пусть $\mu \in K$ и $L\colon U \to V$ -- линейное отображение. Тогда $\mu L$ -- тоже линейное отображение.\\
3) Пусть $L_1\colon V_1\to V_2$, а $L_2\colon V_2 \to V_3$. Тогда $L_1\circ L_2$ -- линейное отображение $V_1 \to V_3$.\\
4)  Выполнена дистрибутивность для композиции. А именно, если $L_1,L_2 \colon V \to W$, а $L_3 \colon W \to U$, то
$$ L_3 \circ (L_1+  L_2 ) =  (L_3 \circ L_1)+  (L_3 \circ L_2).$$
5) Так же, если $L_1 \colon V \to W$, а $L_2,L_3 \colon W \to U$,  то
$$( L_3+  L_2 ) \circ L_1 =  (L_3 \circ L_1)+ (L_2 \circ L_1).$$\\
6) Пусть $L\colon U \to V$. Тогда $\Ker L$ подпространство $U$, а $\Im L$ -- подпространство $V$.\\
7) Множество всех линейных отображений $\Hom(U,V)=\{ L\colon U \to V\,|\, L \text{ линейно}\}$ является векторным пространством.
\elm
\proof Упражнение. Отметим только, что доказательства пунктов 4) и 5) принципиально разные.
\endproof





Оказывается, что все линейные отображения устроены довольно просто.

\thrm
Пусть $V_1$, $V_2$ --- векторные пространства над полем $K$. Пусть $e_1,\dots,e_n$ - базис $V_1$, а $f_1,\dots,f_n$ --- набор каких-то векторов из $V_2$. Тогда существует единственное линейное отображение $L\colon V_1 \to V_2$, что $L(e_i)=f_i$. Более того, если вектор $v\in V_1$ раскладывается по базису $v=\sum \lambda_i e_i$, то $L(v)=\sum \lambda_i f_i$.
\ethrm
\proof Очевидно, что отображение должно быть задано  формулой $L(v)=\sum \lambda_i f_i$. и это показывает единственность.
Осталось показать, что указанная формула даёт линейное отображение. Действительно, если два вектора $u=\sum \lambda_i e_i$ и $v=\sum \mu_i e_i$, то $u+v= \sum (\lambda_i +\mu_i) e_i$ есть разложение для суммы. Осталось посчитать $L(u+v)$ и раскрыть скобки. Аналогично делается вычисление для $L(\lambda u)$.
\endproof


\crl
Все линейные отображения $L\colon K^n \to K^m$ имеют вид $L(x)=Ax$, где $A$ --- матрица $m\times n$
\proof Если задано линейное отображение $L$, то по нему определяется матрица $A$ составленная из столбцов $L(e_i)$, где $e_i$ -- это стандартный базис $K^n$. Тогда отображения $x \to L(x)$ и $x \to Ax$ оба переводят $e_i \to L(e_i)$, то есть совпадают на базисе и, значит, совпадают везде.
\endproof
\ecrl

\dfn Линейное отображение называется моно-, эпи- или изоморфизмом, если оно инъективно, сюръективно или биективно.
\edfn

\rm Если линейное отображение изоморфизм, то есть обратимо, то тогда обратное -- тоже изоморфизм.
\erm

\crl Если $L$ переводит некоторый базис $V$ в базис $W$, то $L$ обратимо. Обратно, если линейное отображение $L \colon V \to W$ является изоморфизмом, то $L$ переводит любой базис $V$ в базис $W$.
\proof
Пусть $e_1,\dots,e_n$ -- базис $V$ и $L(e_1),\dots,L(e_n)$ -- базис $W$. Тогда есть единственное линейное отображение $T \colon W \to V$, которое переводит $L(e_i)$ в $e_i$. Заметим, что композиция $T\circ L$ переводит $e_i \to e_i$. Тогда по единственности такая композиция тождественна. Аналогично композиция в другом порядке тождественна. Тогда $T$ -- обратное отображение к $L$.
 В другую сторону. Пусть $e_1,\dots,e_n$ -- базис $V$. Покажем, что $L(e_1),\dots,L(e_n)$ -- базис $W$.  Пусть есть линейная зависимость  $$0=\lambda_1 L(e_1)+\dots+\lambda_n L(e_n).$$
Тогда
$$0=L^{-1}(0)=L^{-1}(L(\lambda_1 e_1+\dots+\lambda_n e_n))=\lambda_1 e_1+\dots+\lambda_n e_n.$$
Тогда все $\lambda_i$ равны 0. Покажем, что $L(e_i)$ порождают $W$. Рассмотрим вектор $w\in W$. Тогда $$w=L(v)=L\left(\sum \lambda_i e_i\right)=\sum \lambda_i L(e_i).$$
\endproof
\ecrl

Это позволяет нам установить следующее

\crl Пусть $V$ -- конечномерное пространство размерности $n$. Пусть $f_1,\dots,f_n$ -- базис $V$. Тогда  отображение $V\to K^n$, сопоставляющее вектору его столбец координат в базисе $f$ линейно и обратимо. Более того, задание базиса равносильно заданию обратимого линейного отображения $V\to K^n$.
\proof
Рассмотрим пространство $K^n$ и его стандартный базис $e_1,\dots,e_n$. Тогда по теореме существует единственное  линейное отображение переводящее $f_i \to e_i$. Это отображение берёт вектор $v \in V$, раскладывает его как $v=\sum_{i=1}^n \lambda_i f_i$ и переводит его в столбец координат $\lambda_i$. То есть это и есть отображение из условия теоремы.

Обратно, если есть изоморфизм $L\colon V \to K^n$, то можно рассмотреть обратное отображение $L^{-1}$. Тогда $L^{-1}(e_i)$ -- это базис $V$.
\endproof
\ecrl




\dfn
Изоморфизм  $V \to K^n$ называется линейной системой координат на пространстве $V$. Мы знаем что любая линейная система координат происходит из некоторого базиса. Отображение, сопоставляющее вектору $v$ его $i$-ую координату, называется $i$-ой координатной функцией. 
\edfn



\exm(Дополнительно) \\
1) Линейное отображение $$\pmat x\\y\\z \epmat \to \pmat x+y+z\\ y+z\\z \epmat $$
является линейной системой координат на $\mb R^3$.  Какому базису эта система координат  соответствует?\\
2) А вот это -- система координат на $\mb R^2$, соответствующая  повёрнутому на угол $\ffi$ против часовой стрелки стандартному базису:
$$\pmat x\\ y \epmat \to \pmat \cos \ffi\, x + \sin \ffi\, y\\ -\sin\ffi\, x+ \cos \ffi\, y \epmat 
$$


Посмотрим на качественные характеристики линейных отображений. Самой важной такой характеристикой является ранг.





\dfn Рангом линейного отображения $L\colon V \to W$ называется размерность его образа $\rk L= \dim \im L$. Рангом матрицы $A\in M_{m\times n}(K)$ называется ранг соответствующего ей линейного отображения $K^n \to K^m$. Иными словами, ранг матрицы это размерность пространства, порождённого столбцами этой матрицы.
\edfn


\thrm[О подходящем выборе базиса]
Пусть $L\colon V \to W$ --- линейное отображение между конечномерными пространствами. Тогда существует $e_1,\dots,e_n$ -- базис $V$, такой что $e_1,\dots,e_k$ -- базис $\Ker L$, а $L(e_{k+1}),\dots,L(e_n)$ -- базис $\Im L$.
\ethrm
\proof Выберем $e_1,\dots, e_k$ -- базис $\Ker L$ и дополним его элементами $e_{k+1},\dots,e_n$ до базиса всего $V$. Я утверждаю, что $L(e_{k+1}),\dots,L(e_n))$ являются базисом образа. Действительно, образ $L$ порождён $L(e_1),\dots, L(e_n)$, но первые $k$ элементов этого набора нули. Поэтому их можно исключить, что означает, что $L(e_{k+1}), \dots, L(e_n)$ порождают образ $L$.

Пусть сумма $\sum_{i=k+1}^n \lambda_i L(e_i)=0$. Тогда элемент $\sum_{i=k+1}^n \lambda_i e_i $ лежит в ядре $L$. Отсюда получаем, что имеет место равенство $$\sum_{i=k+1}^n \lambda_ie_i = \sum_{i=1}^k \lambda_i e_i$$
которое приводит к нулевой линейной комбинации всех $e_i$. Но $e_i$ -- базис $V$ и поэтому все коэффициенты, в частности, $\lambda_i$ при $i\geq k+1$ равны нулю.
\endproof

\crl Пусть $L\colon V \to W$ --- линейное отображение между конечномерными пространствами. Тогда
$$\dim V= \dim \Ker L +  \dim \im L= \dim \Ker L +\rk L.$$
\ecrl
\proof В обозначениях предыдущей теоремы размерность ядра равна $k$, размерность образа равна $n-k$, что в сумме и даёт $n$.
\endproof



\crl[Принцип Дирихле для линейных отображений] Пусть $V$ и $W$ два пространства размерности $n$ и $L \colon V \to W$ -- линейное отображение между ними. Тогда $L$ -- сюръективно тогда и только тогда, когда $L$ -- инъективно.
\ecrl

\thrm Любое подпространство $U\leq K^n$ коразмерности $d$ задаётся $d$ уравнениями. При этом меньшим числом уравнений задать $U$ нельзя. 
\ethrm
\proof 
Пусть $e_1,\dots,e_k$ -- базис $U$. Здесь $k=n-d$. Дополним его до базиса всего пространства $K^n$ векторами $e_{k+1},\dots,e_n$. Рассмотрим такое отображение $L\colon K^n \to K^d$, что $L(e_i)=0$, если $i\leq k$ и $L(e_i)=f_{i-k} \in K^d$, где $f_j$ -- $j$-ый стандартный базисный вектор.

Тогда $U=\Ker L$. Но $L$ представляется в виде $L(x)=Ax$ для матрицы $A\in M_{d\times n}(K)$. Тогда вектора из $U$ есть в точности решения системы $Ax=0$. 
\endproof

\rm Меньшим числом уравнений задать подпространство нельзя. В самом деле, если $d<\codim U$, и $U=\{ x\in K^n\,|\, Ax=0\}$ где $A\in M_{d\times n}$, то $\dim \Ker A\geq n- d > \dim U$. Противоречие.
\erm


\subsection{Число параметров в методе Гаусса}

Как мы уже выяснили, множество решений системы $Ax=b$ либо пусто, либо имеет вид $x_0 + \Ker A$, где $x_0$ -- некоторое решение.

В свою очередь, множество всех решений описывается при помощи метода Гаусса. В качестве ответа даётся формула, которая по произвольному набору параметров -- независимых переменных -- восстанавливает значения остальных переменных линейным образом. Качественная характеристика такой системы -- это число параметров, при помощи которых описываются все её решения. Нам надо показать, что это число параметров не зависит от хода метода Гаусса и от возможной замены переменных. Сделаем это следующим образом:

\thrm Пусть дана система линейных уравнений $Ax=b$ с матрицей $A\in M_{m\times n}(K)$ и непустым множеством решений. Тогда число параметров, задающих общее решение системы после использования метода Гаусса, равно $\dim \Ker A$.
\ethrm
\proof Пусть независимые переменные в методе Гаусса это $x_{i_1}, \dots,x_{i_s}$.
Заметим, что номера независимых переменных при решении однородной системы $Ax=0$ такие же, как и для системы $Ax=b$. Поэтому можем смотреть на однородную систему.

В случае однородной системы зависимые переменные  выражаются линейно через независимые в виде
$$x_i= \sum c_{ij} x_{i_j}, \text{ при всех} i \notin \{i_1,\dots, i_k\}.$$
Сопоставление набору $(x_{i_1},\dots, x_{i_s})$, решения системы $Ax=0$, задаёт линейное отображение $K^s \to K^n$. Точнее, это отображение есть взаимооднозначное линейное отображение $K^s \to \Ker A$, то есть изоморфизм. Следовательно, $\dim \Ker A = s$, что и есть число параметров.
\endproof




Если мы хотим найти все решения системы $Ax=b$, то, прежде всего, для этого нужно найти частное решение $x_0$. Для этого надо решить систему методом Гаусса и подставить конкретные значения для свободных переменных. Проще всего подставить $x_{i_1}=\dots=x_{i_s}=0$. Любое решение имеет вид $x_0+\Ker A$.
Теперь надо описать ядро $A$. Конкретно, нас будет интересовать  базис $\Ker A$. Заметим, что рассмотренное в доказательстве отображение $K^s \to \Ker A$ есть изоморфизм. При изоморфизме базис переходит в базис, следовательно взяв стандартный базис $K^s$ в качестве его образов получим базис ядра.

\subsection{Дополнительно: пример}
Пусть $$A=\pmat
1 & 2 & -1 & 2\\
0& 1& 0 & 1\\
1& 0 & -1 & 0\epmat, \,\,b= \pmat -1\\ -1\\ 1 \epmat.$$
Решаем систему
$$\pmat
1 & 2 & -1 & 2 & -1\\
0& 1& 0 & 1 & -1\\
1& 0 & -1 & 0 & 1\epmat \sim
\pmat 1& 0 & -1 & 0 & 1 \\
0& 1& 0 & 1 & -1\\
0 & 2 & 0 & 2 & -2
\epmat \sim \pmat 1& 0 & -1 & 0 & 1 \\
0& 1& 0 & 1 & -1\\
0 & 0 & 0& 0 & 0
\epmat.$$
Параметра два -- это $x_3,x_4$. Берём $x_3=x_4=0$. Тогда $x_2=-1$, $x_1=1$. Итого получаем частное решение $$v_0=\pmat 1\\ -1\\0 \\ 0 \epmat.$$
Ищем базис ядра. Для этого переходим к однородной системе. 
$$\pmat
1& 0 & -1 & 0 & 0 \\
0& 1& 0 & 1 & 0\\
0 & 0 & 0& 0 & 0
\epmat.$$
Подставляем стандартные вектора из $K^2$. Берём $x_3=1$, $x_4=0$. Тогда $x_2=0,x_1=1$. Берём $x_3=0$, $x_4=1$. Тогда $x_1=0,x_2=-1$. Итого базис состоит из двух векторов
$$v_1=\pmat 1\\0\\1\\0 \epmat, \, v_2=\pmat 0 \\ -1 \\ 0 \\ 1 \epmat.$$
Общее решение описывается как
$$\pmat 1\\ -1\\0 \\ 0 \epmat+ \lan \pmat 1\\0\\1\\0 \epmat, \pmat 0 \\ -1 \\ 0 \\ 1 \epmat \ran.$$


При рассмотрениях выше мы столкнулись с двумя разными заданиями подпространств в пространстве столбцов -- параметрическим, $U=\lan u_1,\dots, u_k \ran$, где по набору параметров $\lambda_1,\dots,\lambda_k$ легко построить вектор из $U$ взяв $\lambda_1 u_1+\dots+\lambda_k u_k$.

Другой способ -- это задание подпространства уравнениями $U=\Ker A = \{ x\in K^n \,|\, Ax=0\}$. При таком подходе очень легко проверить, что данный вектор лежит или не лежит в подпространстве. Достаточно вычислить $
Ax$ и сравнить с нулём. Только что мы показали, как от задания подпространства уравнениями перейти к заданию этого подпространства параметрически. 

В свою очередь, мы знаем, что есть и обратный переход -- подпространство коразмерности $d$ задаётся $d$ уравнениями. 


\section{Матрица линейного отображения}


Итак, у нас есть модельная задача: пусть $A \colon K^n \to K^m$ линейное отображение и элемент $y \in K^m$. Если нужно описать все прообразы элемента $y$, то это равносильно решению системы линейных уравнений $Ax=y$. Про эту задачу мы много понимаем всё.
Наша текущая задача -- научиться сводить задачу про общее линейное отображение к этой модельной. Однако тут возникает нюанс -- это можно сделать разными способами. Точнее:

\dfn
Пусть $V_1$, $V_2$ - векторные пространства над полем $K$ с базисами $e_1,\dots, e_n$ и $f_1,\dots, f_m$ соответственно. Пусть $L\colon V_1\to V_2$ - линейное отображение. Составим матрицу $A$, такую, что её $i$-ый столбец состоит из координат $L(e_i)$ в базисе $f$. Такая матрица называется матрицей линейного отображения $L$ в базисах $e$ и $f$. Будем обозначать её как $[L]^e_f$.
\edfn




\thrm Пусть $L \colon V_1 \to V_2$ -- линейное отображение, а $e$ и $f$ -- базисы  $V_1$  и $V_2$. Тогда матрица отображения $L$ -- это единственная такая матрица $A$, что $Ax=y$, где $x$ -- координаты вектора $v\in V_1$, а $y$ -- координаты его образа в $V_2$.
\ethrm
\proof Пусть $A$ -- это матрица $L$ в соответствующих базисах. Пусть $u=x_1e_1+\dots+x_ne_n$. Тогда $L u =x_1Le_1+\dots+x_n Le_n$. Тогда координаты $[Lu]_f$  вычисляются как $x_1[Le_1]_f+\dots+x_n[Le_n]_f$. Но это и есть $Ax$.
Обратно, если взять $u=e_i$, то координатный столбец $u$ будет равен $i$-ому стандартному столбцу и $[Le_i]_f=Ax$ будет как раз $i$-ым столбцом $A$.
\endproof


\utv Пусть $L_1,L_2\colon U \to V$ и $e$ -- базис $U$, а $f$ -- базис $V$. Тогда матрица $L_1+L_2$ в базисах $e$ и $f$ есть сумма матриц $L_1$ и $L_2$. Аналогично про домножение на скаляр.
\eutv
\proof Пусть $L_1 e_j= \sum A_{ij}f_i$, а $L_2=\sum B_{ij}f_i$. Тогда $L_1(e_j)+L_2(e_j)=\sum (A_{ij}+B_{ij})f_i$, что и доказывает требуемое.
\endproof

\crl Пусть пространства $U$ и $V$ имеют размерности $n$ и $m$. Допустим выбраны базисы $e_1,\dots,e_n$ и $f_1,\dots,f_m$ пространств $U$ и $V$ соответственно. Тогда сопоставление линейному отображению $L \in \Hom(U,V)$ его матрицы в указанных базисах есть изоморфизм
$$\Hom(U,V) \stackrel{\sim}{\longrightarrow} M_{m\times n}(K).$$
\ecrl





Пусть есть две матрицы $A\in M_{n\times m}(K)$ и $B\in M_{m\times l}(K)$. Какая матрица соответствует композиции линейных отображений, построенных по $A$ и $B$?



\thrm Пусть $U$, $V$, $W$ пространства с базисами $e_1,\dots,e_n$, $f_1,\dots,f_m$, $g_1,\dots,g_l$. Пусть $L_1\colon U \to V$ и $L_2\colon V \to W$ два линейных отображения. Пусть $A$ и $B$ матрицы $L_1$ и $L_2$ в парах базисов $e$, $f$  и $f$, $g$. Тогда матрица $L_2 \circ L_1$ в базисах $e$, $g$ равна произведению $BA$.
\ethrm
\proof Пусть $u\in U$, а $x$ -- его столбец координат. Подействовав последовательно на $x$ сначала $A$ и затем $B$ получим столбец координат образа $u$ под действием $L_2\circ L_1$ с одной стороны, а с другой -- столбец $B(Ax)$.  По ассоциативности произведения матриц имеем $$B(Ax)=(BA)x$$
Таким образом, домножение на матрицу $BA$ переводит координаты вектора $u$ в координаты его образа относительно $L_2 \circ L_1$. Но это и значит, что $BA$ есть матрица композиции.
\endproof




Какая матрица соответствует обратимому линейному отображению? 



\dfn Матрица $A \in M_{m\times n}$ называется обратимой, если существует $B \in M_{n\times m}$, что $AB=E_m$, $BA=E_n$.  Матрица $B$ называется обратной к $A$ и обозначается $A^{-1}$.
\edfn

\crl Если матрица $A \in M_{n\times m}$ обратима, то $n=m$. Обратная матрица единственна.
\proof Обратимая матрица осуществляет изоморфизм пространства $K^n$ и $K^m$.  Но если пространства изоморфны, то их размерности равны, то есть $n=m$. Обратное к биективному отображению единственно и, следовательно, единственна матрица, его задающая.
\endproof
\ecrl


\crl Матрица $A \in M_n(K)$  обратима тогда и только тогда, когда $\rk A=n$. Ещё такие матрицы называют невырожденными.
\ecrl
\proof Следует из принципа Дирихле для линейных отображений.
\endproof

\rm Если матрицы $A,B\in M_n(K)$ обратимы, то $AB$ -- тоже обратима и $(AB)^{-1}=B^{-1}A^{-1}$.
\erm

Знание обратной матрицы может помочь при решении системы линейных уравнений. А именно, если $A$ -- обратима, то тогда решение системы $Ax=b$ может быть найдено по правилу $x=A^{-1}b$.

Как найти обратную матрицу? В принципе, можно написать условие $AB=E_n$ и найти матрицу $B$, удовлетворяющую этому условию. Это система линейных уравнений на коэффициенты матрицы $B$. Однако, решать систему  относительно $n^2$ переменных не греет нам душу. 

Чтобы упростить жизнь вспомним посмотрим как найти столбец матрицы $B=A^{-1}$. Если $e_1,\dots,e_n$ -- это стандартный  базис $K^n$, то $u_i=Be_i$ и есть $i$-ый столбец $B$. Но это же равенство можно переписать как $Au_i=e_i$. Таким образом, для того, чтобы найти $u_i$ нужно решить систему уравнений с матрицей $A$. 

При решении такой системы методом Гаусса последовательность преобразований зависит от матрицы $A$. Значит все эти системы можно решать одновременно. А именно, возьмём матрицу $n\times 2n$ вида $(A|E_n)$. Приведём её при помощи элементарных преобразований к виду $(E_n|B)$. Тогда $B=A^{-1}$. Этот метод принято называть методом Гаусса-Жордана.



\subsection{Замена координат}


В одном и том же пространстве $U$ можно выбрать разные базисы и, соответственно, получаются разные системы координат. Как пересчитать координаты вектора из одной системы координат в другую?  Как меняются координаты вектора при переходе из одного базиса в другой?

Пусть на пространстве $U$ заданы два базиса $e$  и $f$. Рассмотрим $\id_U \colon U \to U$. Что даёт его матрица в базисах $e$  и $f$? По характеристическому свойству это такая матрица $A$, что $Ax=y$, где $x$ -- это координаты вектора $u$ в базисе $e$, а $y$ -- это координаты того же вектора в базисе $f$.

\dfn Рассмотрим пространство $V$ размерности $n$ и два базиса $e_1,\dots,e_n$ -- старый и $f_1,\dots,f_n$ -- новый. Тогда матрицу 
$[\id_U]^e_f$ назовём матрицей замены координат из базиса $e$ в базис $f$. Она пересчитывает координаты вектора из старого базиса в новый.

В том же контексте есть другое, похожее определение. Матрица $C_{e\to f}=[\id_V]^f_e$  называется матрицей перехода из базиса $e$ в базис $f$.
\edfn

Стандартным определением является именно матрица перехода. В чём причина? Оказывается матрицу перехода часто проще искать. Действительно, как найти матрицу $C_{e\to f}$? Для этого нужно взять элементы $f_i$, расписать их $f_j = \sum_{i=1}^n c_{ij} e_i$ через базис $e$, взять их координатные столбцы в базисе $e$ и составить из них матрицу. Это иногда записывают как 
$$(f_1,\dots,f_n)=(e_1,\dots,e_n)C_{e\to f}.$$ 

В одной из самых распространённых ситуаций, когда $e_1,\dots, e_n$ -- это стандартный базис $K^n$, а $f_1,\dots, f_n$ -- какой-то другой базис матрица перехода $C_{e\to f}$ просто составлена из столбцов $f_1,\dots,f_n$.




\rm Для матриц перехода выполнены следующие свойства:\\
1) $C_{e\to e}=E_n$\\
2) $C_{e\to f}C_{f\to g} = C_{e \to g}$\\
3) $C_{f\to e}=C_{e\to f}^{-1}$.
\erm


Теперь можно разобраться с тем, как меняется матрица линейного отображения при замене базиса.  




\thrm Пусть даны пространства $U$, $V$ и линейное отображение $L\colon U\to V$. Рассмотрим в пространстве $U$ два базиса $e_1,\dots, e_n$ (старый) и $e_1',\dots, e_n'$ (новый). Аналогично в $V$ --- $f_1,\dots, f_m$ (старый) и $f_1',\dots, f_m'$ (новый).
Пусть  $A$ -- матрица $[L]^e_f$ и $A'=[L]^{e'}_{f'}$. Тогда 
$$A'=C^{-1}AD,$$
где $C$ -- матрица перехода $f\to f'$, а $D$ -- матрица перехода $e\to e'$
\ethrm
\proof Посмотрим на равенство $L=\id_V \circ L \circ \id_U$. Тогда вычисляя матрицу композиции получаем
$$A'=[L]^{e'}_{f'}=[\id_V]_{f'}^{f}\, [L]_{f}^{e}\,[\id_U]_e^{e'}=C^{-1}AD,$$
что и требовалось.
\endproof



\thrm[Канонический вид матрицы линейного отображения] Пусть $L\colon U \to V$. Тогда существуют такие базис $e_1,\dots,e_n$ в $U$ и  $f_1,\dots,f_m$ в $V$, что матрица линейного отображения $L$ в этих базисах имеет вид 
$$\pmat E_r& 0\\
0&0\\
\epmat .$$
\ethrm
\proof
По теореме о подходящем выборе базиса  существует $e$ -- такой базис $U$, что $f_1=L e_1,\dots,f_r=L e_r$ -- это базис $\Im L$ и $e_{r+1},\dots,e_n$ -- базис $\Ker L$. Дополним набор $f_1,\dots,f_r$ до базиса $V$ элементами $f_{r+1},\dots,f_m$.  Посмотрим на матрицу отображения $L$ в этих базисах. Это в точности матрица  
$$\pmat E_r& 0\\
0&0\\
\epmat.$$
\endproof




\crl Для любой матрицы $A \in M_{m \times n}$ ранга $r$ существуют обратимые матрицы $C\in M_m(K)$ и $D\in M_n(K)$, что 
$$A=C\pmat E_r& 0\\
0&0\\
\epmat D.$$
\proof 
По предыдущей теореме, применённой к отображению $Lx =Ax$ есть $e$ -- базис $K^n$ и $f$ -- базис $K^m$, что матрица $L$ имеет вид  $$\pmat E_r& 0\\
0&0\\
\epmat.$$
Тогда по теореме о замене координат получаем, что 
$$A=C \pmat E_r& 0\\
0&0\\
\epmat D$$
где $C$ -- матрица перехода из стандартного базиса в $f$, а $D$ -- матрица перехода из $e$ в стандартный базис.
\endproof
\ecrl

\crl[Ранговое разложение] Для любой матрицы $A\in M_{m\times n}(K)$ ранга $r$ существуют матрицы $T\in M_{m\times r}$ и $S\in M_{r\times n}(K)$, что $A=TS$.
\proof 
Представим $A$ в виде $$A=C \pmat E_r& 0\\
0&0\\
\epmat D$$
Заметим, что  $$\pmat E_r& 0\\
0&0\\
\epmat =\pmat E_r\\
0\\
\epmat \pmat E_r& 0\\
\epmat.$$
Тогда 
$$A=C 
\pmat E_r\\
0\\
\epmat 
\pmat E_r& 0
\epmat D= TS, \,\text{ где }\, T=C 
\pmat E_r\\
0 \epmat \text{ и }  S=\pmat E_r& 0\\
\epmat D.$$

\endproof
\ecrl















\section{Свойства ранга}
Ранг линейного отображения $L$ отвечает за его обратимость и за размерность множества решений уравнения $Lx=y$. Ранг линейного отображения совпадает с рангом его матрицы при любом выборе базисов. Ранг линейного отображения есть единственный инвариант линейного отображения с точностью до замены координат. Наша задача посмотреть на базовые свойства, которыми обладает ранг. Прежде всего разберёмся, как оценить ранг суммы линейных отображений.


\utv Пусть $L_1, L_2 \colon U \to V$. Тогда $\rk (L_1 + L_2) \leq \rk L_1 + \rk L_2.$
\eutv
\proof Заметим, что $\Im(L_1 + L_2) \subseteq \Im L_1 + \Im L_2$. Тогда 
$$\rk (L_1 + L_2) =\dim \Im(L_1 + L_2) \leq \dim(\Im L_1 + \Im L_2) \leq \dim\Im L_1 + \dim\Im L_2 = \rk L_1 + \rk L_2.$$
\endproof

Разберёмся с оценкой на ранг композиции линейных отображений.

\utv Пусть $S \colon V \to W$, $T \colon W \to U$ -- линейные отображения. Тогда $$\rk T \circ S \leq \min(\rk T, \rk S).$$
\eutv 
Необходимо доказать два неравенства. Для начала покажем, что если есть $L\colon V_1 \to V_2$ и $V' \leq V_1$, то $\dim L(V') \leq \dim V'$. Действительно $$\dim V'= \dim \im L|_{V'} + \dim \Ker L|_{V'} \geq \dim L(V').$$
Теперь $\im T \circ S \subseteq  \im T$, поэтому $\rk T \circ S \leq \rk T$.
Наконец $$\rk T \circ S= \dim \im T \circ S = \dim T (S(V)) \leq \dim S(V)= \rk S.$$
\endproof


\crl Пусть даны линейные отображения $U \stackrel{T}{\to} V \stackrel{L}{\to} W \stackrel{S}{\to} U'$, причём отображения $S$ и $T$ -- обратимые. Тогда $\rk SLT=\rk L$.
\ecrl
\proof С одной стороны ясно, что $\rk SLT \leq \rk LT \leq \rk L.$ С другой стороны $$\rk L = \rk S^{-1}SLTT^{-1} \leq \rk SLT,$$ что и требовалось.
\endproof







Однако, в методе Гаусса удобнее смотреть на строчки, чем на столбцы. Это замечание приводит нас к определению строчного ранга.

\dfn Пусть $A\in M_{m\times n}(K)$. Определим $\rk_{row}$ -- строчный ранг матрицы $A$ как размерность пространства, порождённого её строками.
\edfn

\dfn Пусть  $A\in M_{m\times n}(K)$. Определим матрицу $A^{\top}\in M_{n\times m}(K)$ как $A^{\top}_{ij}=A_{ji}$.
\edfn

\rm Очевидно равенство $\rk_{row} A= \rk A^{\top}$.
\erm

Поэтому посмотрим на свойства операции транспонирования матрицы.

\lm Пусть $A,B$ -- матрицы. Тогда\\
0) ${A^{\top}}^{\top}=A$.\\
1) $(A+\lambda B)^{\top}= A^{\top}+\lambda B^{\top}$.\\
2) $(AB)^{\top}=B^{\top}A^{\top}$.\\
3) $(A^{\top})^{-1}= (A^{-1})^{\top}$.
\elm
\proof Покажем, например, второй пункт. Имеем
$$(AB)^{\top}_{ij}= (AB)_{ji}=\sum_k A_{jk}B_{ki}=\sum_k B^{\top}_{ik}A^{\top}_{kj}=(B^{\top}A^{\top})_{ij}.$$
Теперь покажем пункт 3. Пусть $AA^{-1}=E_n$. Тогда $E_n=E_n^{\top}=(AA^{-1})^{\top}= (A^{-1})^{\top}A^{\top}$. Аналогично из равенства $A^{-1}A=E_n$ следует $E_n=A^{\top}(A^{-1})^{\top}$, что завершает доказательство.
\endproof

\rm Заметим, что, вообще говоря, проверять второе условие не обязательно.
\erm

\thrm Пусть $A\in M_{m \times n}(K)$. Тогда ранг по строчкам и ранг по столбцам совпадают.
\ethrm
\proof Представим матрицу $A$ в виде произведения
$$A=C \pmat E_r & 0\\
0 & 0 \epmat D,$$
где $C \in M_m(K)$, а $D \in M_n(K)$ -- обратимы, а $r$ -- ранг матрицы. Тогда
$$A^{\top}=D^{\top} \pmat E_r & 0\\
0 & 0 \epmat C^{\top}.$$
Так как ранг не меняется при домножении на обратимые матрицы, то $\rk A^{\top} = \rk \pmat E_r & 0\\
0 & 0 \epmat = r = \rk A$.
\endproof


Приведём пример использования свойства ранга. Для этого посмотрим на пример с поисковой системой. Пусть $G$ граф на $n$ вершинах. Тогда рассмотрим квадратную матрицу $P(G)$ размера $n$
$$P(G)_{ij}= \begin{cases} \frac{1}{d_j^{out}}, \text{ если есть ребро $j \to i$} \\
1, \text{ если $i=j$ и из вершины $i$ нет исходящих рёбер}\\
0, \text{ иначе }
\end{cases}.$$
Эта матрицу еще называется матрицей случайного блуждания на графе $G$. Смысл состоит в том, что она моделирует следующую ситуацию -- если мы с вероятностями $w_1,\dots,w_n$ находимся в вершинах графа и на следующем шаге хотим из $j$-ой вершины перейти в $i$-ую по ребру графа, при этом проход по каждому ребру равновероятен, то после такого действия новый набор вероятностей будет иметь вид $P(G)w$. Матрица $P$ обладает тем свойством, что сумма элементов в каждом столбце равна 1.

Теперь систему для нахождения <<важностей>> вершин графа можно переписать в виде $E_n w= P(G)w$ или
$$(E_n - P(G))w=0.$$
Нам хочется показать, что у этой системы есть нетривиальное решение, то есть, что ранг матрицы $E_n- P(G)$ строго меньше $n$. Для этого можно показать, что ранг
$$E_n - P(G)^{\top}$$
меньше $n$, а это легко сделать -- достаточно заметить, что матрица $E_n - P(G)^{\top}$  имеет столбец $(1,1,\dots, 1)^{\top}$ в своём ядре. Заметьте -- это не помогает найти решение исходной системы, а лишь доказывает его существование.



\section{Матрицы специального вида и $LUP$ разложение}


При решении систем линейных уравнений методом Гаусса нам пригодились элементарные преобразования. Покажем, что они не влияют на ранг матрицы.

Для этого покажем, что элементарное преобразование можно реализовать домножив исходную матрицу на некоторую обратимую матрицу. Точнее, рассмотрим матрицу
$$ E_{ij}(\lambda)=E_n +\lambda e_{ij}=
\bordermatrix{
 & &j&& \cr
 &1&\cdots&\cdots&0\cr
 &\vdots&\ddots && \vdots\cr
i&\vdots& \lambda & \ddots& \vdots\cr
 &0&\cdots& \cdots&1
}.
$$\\
Домножение на матрицу $E_{ij}(\lambda)$ приводит к тому, что матрица
$$A'=E_{ij}(\lambda)A.$$
получается из матрицы $A$ прибавлением $j$-ой строки к $i$-ой с коэффициентом $\lambda$.
Действительно, домножение матрицы $A$ на матрицу $e_{ij}$ выделяет из матрицы $A$ $j$-ую строку и записывает её на позицию $i$.

\dfn Матрица $E_{ij}(\lambda)$ называется матрицей элементарного преобразований первого типа или просто элементарной матрицей.
\edfn

Что соответствует замене местами двух строк матрицы $A$? Для этого надо поставить $i$-ую строчку в позицию $j$ и наоборот, а остальные строки оставить на месте. Итого имеем матрицу
$$P_{(ij)}=\,\bordermatrix{
 & & &j& & i& \cr
 &1& & & & & &0\cr
 & &\ddots & & & & \cr
j\,& && 0 & & 1 & &\cr
\,& &&  & &  & &\cr
i\,& && 1 & &0 & &\cr
 & & & & & & \ddots& \cr
 &0& & & & & &1
}
$$


\dfn Матрица $P_{(ij)}$ называется матрицей элементарного преобразований второго типа или матрицей транспозиции.
\edfn

А что вообще происходит при перестановке строк? Каждой перестановке $\sigma \in S_n$ однозначно соответствует матрица $P_{\sigma}$ заданная правилом
$$(P_{\sigma})_{ij}= \begin{cases}
1, \text{ если $\sigma(j)=i$}\\
0, \text{ иначе }
\end{cases}.$$


\dfn Матрица $P_{\sigma}$ называется матрицей перестановки $\sigma$.
\edfn


\rm Заметим, что произведение $P_{\sigma} P_{\tau}=P_{\sigma\tau}$.  Любая матрица, у которой в каждом столбце ровно один единичный элемент, а все остальные 0 является матрицей перестановки.
\erm

Наконец, преобразованию третьего типа соответствует матрица
$$ D_{i}(\lambda)=
\bordermatrix{
 & &&i&& \cr
 &1&&&&0\cr
 &&\ddots &&& \cr
i&&  & \lambda && \cr
&&  &  &\ddots& \cr
 &0&& &&1
}.
$$

\dfn Матрица $D_{i}(\lambda)$, $\lambda \in K^*$ называется матрицей элементарного преобразований третьего типа.
\edfn

\rm Так выглядят обратные к этим матрицам\\
1) $E_{ij}^{-1}(\lambda)= E_{ij}(-\lambda)$,\\
2) $D_i^{-1}(\lambda)= D_i(\lambda^{-1})$,\\
3) $P_{\sigma}^{-1}=P_{\sigma^{-1}}$. В частности, $P_{(ij)}^{-1}=P_{(ij)}$.
\erm






Композиция элементарных преобразований, это конечно не элементарное преобразование, но можно кое-что заметить:

\dfn Пусть $A \in M_{n}(K)$. Тогда матрица $A$ называется нижнетреугольной, если $\forall  i<j$, то $A_{ij}=0$. Аналогично определяются верхнетреугольные матрицы. 
\edfn

\dfn Множество всех верхнетреугольных матриц размера $n$ обозначается как $UT_n(K)$, а нижнетреугольных $LT_n(K)$. 
\edfn

\rm Матрица $E_{ij}(\lambda)$ является верхнетреугольной, если $i<j$ и нижней унитреугольной если $i>j$. 
\erm

\lm  Пусть $A,B \in LT_n(K)$. Тогда\\
1) $A+B \in LT_n(K)$\\
2) $\lambda A \in LT_n(K)$ где $\lambda \in K$.\\
3) $AB \in LT_n(K)$.\\
4) $A^{-1} \in LT_n(K)$, если $A$ -- обратима.\\
Аналогично для верхнетреугольных матриц.
\proof[Первое доказательство] Свойства 1) и 2) легко проверяются. Свойство 3) проверяется непосредственно. Покажем свойство 4). Пусть $A$ -- обратимая нижнетреугольная матрица. Тогда на диагонали $A$ стоят ненулевые элементы $d_1,\dots,d_n$. Заметим теперь, что все преобразования в методе Гаусса будут иметь вид $E_{ij}(\lambda)$, где $i<j$. Это означает, что не будет использоваться перестановки  строк, элементы на диагонали не изменятся, а наддиагональные элементы останутся нулевыми. В свою очередь матрицы $E_{ij}(\lambda)$ будут нижнетреугольными. Метод Гаусса приведёт матрицу $A$ к виду $$D=\pmat d_1 &&\\
&\ddots & \\
&& d_n \epmat.$$ 
Это означает, что 
$$ L_1\dots L_k A =D,$$
где $L_i$ -- матрицы указанных элементарных преобразований. Но тогда 
$$A^{-1}=(L_k^{-1}\dots L_1^{-1}D)^{-1}=D^{-1}L_1\dots L_k.$$
Следовательно, $A^{-1}$ -- нижнетреугольная, как произведение нижнетреугольных матриц. 
\endproof
\proof[Второе доказательство] Как и раньше на диагонали $A$ стоят ненулевые элементы $d_1,\dots,d_n$. Рассмотрим диагональную матрицу $D$ с $d_1,\dots, d_n$ на диагонали. Определим $B=D^{-1}A$. Это нижнетреугольная матрица с единицами на диагонали. Тогда $B=E_n+N$, где 
$$ N=\pmat 
0&&&\\
*&0&&\\
\vdots&\ddots&\ddots&\\
*&\dots & *&0
\epmat.$$
Заметим, что $N^n=0$. Действительно, верно, что $Ne_n=0$ и $Ne_i \in \lan e_{i+1},\dots,e_n\ran$. Отсюда применяя $n-1$ раз матрицу $N$ к любому вектору мы скатываемся в $\lan e_n \ran$ откуда благополучно отправляемся в $0$. Тогда заметим, что
$${B}^{-1}=E_n-N+N^2-N^3+\dots+ (-1)^{n-1}N^{n-1}$$
является нижнетреугольной. Значит и обратная к $A$ является нижнетреугольной.
\endproof
\elm

Обращение верхнетреугольных и нижнетреугольных матриц проводится за $O(n^2)$ операций. Это обстоятельство означает, что решать систему линейных уравнений $Lx=b$ для, например, нижнетреугольной матрицы $L$ довольно легко. Это можно использовать и для решения произвольных систем при помощи следующей теоремы.


\thrm[$LUP$ разложение] Пусть $A \in M_{n}(K)$ -- обратимая матрица. Тогда существует  две обратимые матрицы $L \in LT_{n}(K)$ -- нижняя треугольная с единицами на диагонали, $U\in UT_n(K)$ и матрица перестановки $P_{\sigma}$, что 
$$PA=LU.$$ 
\ethrm
\proof Предъявим алгоритм построения такого разложения. После шага $k$ будем предполагать, что построена матрица $L\in LT_n$ с единицами на диагонали и нулями под диагональю в столбцах с номерами начиная с $k+1$; матрица $U$ -- блочная верхнетреугольная, причём верхний блок $k\times k$ -- верхнетреугольный
$$L=\,\bordermatrix{
& & &k&k+1 & \cr
&1& & & & &\cr
&*&\ddots & & && \cr
k&\vdots &\ddots& 1 &  & &\cr
&\vdots && *      &1 & &\cr
&\vdots && \vdots & & \ddots& \cr
&*&\dots & *& & &1
} \text{ и }
U=\,\bordermatrix{
& & &k&k+1 & \cr
&*&\dots &\dots &\dots &\dots &*\cr
&0&\ddots & & &&\vdots \cr
&\vdots &\ddots& * &  & &\vdots\cr
&\vdots && 0 &* & &\vdots\cr
&\vdots && \vdots &\vdots & &\vdots \cr
&0& \dots&0 &* &\dots &*
};$$
и матрица перестановки $P$, такие что 
$$L^{-1}PA=U.$$
При таких условиях матрица $U$, необходимо, невырождена. Попробуем произвести шаг алгоритма. Если элемент $U_{k+1,k+1}\neq 0$, то $k+1$ строчку можно прибавить к нижним с тем, чтобы элементы $U_{i,k+1}$ занулились при $i>k+1$. Это элементарные преобразования строк и они эквивалентны домножению на элементарные матрицы. Назовём эти матрицы $L_i$. Посмотрим, что происходит при отдельном элементарном преобразовании. Домножим равенство $L^{-1}PA=U$ на $L_i$ 
$$L_iL^{-1}PA=L_i U.$$
В правой части появился дополнительный ноль в $i$-ой позиции $k+1$-го столбца. В то же время $L_iL^{-1}=(LL_i^{-1})^{-1}$. Откуда видим, что матрица $LL_i^{-1}$ играет роль $L$. Заметим, что $L_i^{-1}$ -- нижнетреугольная и, следовательно, произведение $LL_i^{-1}$ -- тоже. Осталось заметить, что у $LL_i^{-1}$ добавились нули лишь в $k+1$ столбце. Но домножение на матрицу  $L_i^{-1}=E_{j,k+1}(\lambda)$ справа прибавляет к столбцу $k+1$ столбец $i$ c коэффициентом $\lambda$. Так как $i$-ый столбец содержит единственный ненулевой элемент на позиции $i$,то такое преобразование запишет  $\lambda$ вместо нуля на позиции $i$ в столбце $k+1$.
Применив все преобразования $L_i$ получим новые подходящие матрицы $L,U$.

Осталось разобрать ситуацию, когда $U_{k+1,k+1}=0$. В этом случае, благодаря невырожденности матрицы $U$ найдётся такое $i$, что элемент $U_{i,k+1}\neq 0$. Переставим строчку $i$ и строчку $k+1$ местами. Это происходит при помощи домножения на матрицу перестановки $P_i$. Посмотрим, как поменяются матрицы $L,U,P$ при таком преобразовании. Домножим на $P_i$ основное соотношение. 
$$P_iU=P_iL^{-1}PA=P_iL^{-1}P_i(P_iP)A=(P_iLP_i)^{-1}(P_iP)A.$$
Новая матрица $P$ равна $P_iP$. Осталось проверить, что $P_iLP_i$ -- нижнетреугольная с единицами на диагонали и нулями под диагональю в столбцах с номерами начиная с $k+1$. Но в этом несложно убедиться ведь матрица $P_iLP_i$ получается из $L$ перестановкой строк и столбцов с номерами $i$ и $k+1$.
\endproof

\rm Как видно из доказательства, матрица $P$ далеко не единственна. Она определяется тем, какие строки мы переставляем в методе Гаусса. Для того, чтобы избежать сложностей связанных с ошибками округления, даже если возможно не переставлять строки, принято ставить на первое место строку с наибольшим по модулю числом.
Поэтому часто используется именно $LUP$ разложение.
\erm

\rm  Диагональные элементы матрицы $L$ всегда единицы и их можно отдельно не хранить. Элементы под диагональю определяют $L$. Их столько же, сколько заведомо нулевых элементов в $U$. Таким образом, матрицы $L$ и $U$ удобно хранить в виде одной матрицы $n\times n$ убрав всю избыточную информацию про них.
\erm


\rm Можно провести смешное доказательство этой теоремы. Применим метод Гаусса к столбцам $A$. Метод Гаусса равносилен домножению на матрицы $E_{ij}(\lambda)$. При этом, вначале, мы прибавляем $i$-ую строку к $j$-ой, где $i>j$. Такие элементарные матрицы являются верхнетреугольными. Таким образом,если мы всегда домножаем справа на верхнетреугольные матрицы и в итоге получили нижнетреугольную $L$, то мы победили. 

Но в процессе, на шаге $k$   может  получиться, что элемент $a_{kk}=0$. Но в этом случае, в столбце $k$ есть ненулевой элемент. Переставим его с помощью преобразования строк -- это приведёт к домножению матрицы слева на матрицу перестановки. Но это нам и нужно!
\erm



\chapter{Группы}

\section{Немного примеров}

Мы уже обсуждали примеры групп раньше. Вспомним некоторые из них  и посмотрим, что к ним добавилось.

\exm \\
1) Если $R$-кольцо, то группу $R$ относительно сложения будем обозначать просто как $R$\\
2) Если $R$-кольцо, то $R^*$ -- это группа обратимых элементов.\\
3) Группа биекций $S_X$ на множестве $X$. В частности, $S_n$ -- группа биекций на $\{1,\dots,n\}$. Каждая такая биекция $\sigma \in  S_n$ однозначно задаётся последовательностью $\sigma(1),\dots,\sigma(n)$. В этой последовательности перечислены все элементы множества $\{1,\dots,n\}$, но, возможно, в другом порядке. Мы будем обозначать такую перестановку $\sigma=[\sigma(1),\dots,\sigma(n)]$. Например, обозначение тождественной перестановки будет $[1,\dots,n]$. А вот перестановка $[2,1,3,4,\dots,n]$ меняет местами $1$ и $2$, но все остальные элементы оставляет на месте.\\ 
4) Рассмотрим группу $D_n$. Это подгруппа $\Isom_{\mb R^2}$ состоящая из самосовмещений правильного $n$-угольника на плоскости. Покажем, что в этой группе $2n$ элементов. Прежде всего предъявим их, а потом покажем, что других нет. Здесь стоит различать два случая: $n$ -- чётное, $n$ -- нечётное. Если $n$ -- нечётное, то есть повороты на углы $\ffi=\frac{2\pi k}{n}$, где $k\in \ovl{0,1}$ и $n$ симметрий относительно прямых проходящих через какую-то вершину и центр $n$-угольника. Если же $n$ -- чётное, то элементами $D_n$ так же являются повороты на  $\ffi=\frac{2\pi k}{n}$, но симметрии делятся на два типа -- $\frac{n}{2}$ симметрий относительно прямых, проходящих через пары противоположенных вершин и $\frac{n}{2}$ симметрий относительно прямых, проходящих через середины противоположенных сторон $n$-угольника.

Для того, чтобы показать, что в этой группе не более $2n$ элементов воспользуемся следующими фактами:

\fct Любая изометрия плоскости определяется своими значениями в трёх точках, не лежащих на одной прямой (определяется тем, куда переводит вершины невырожденного треугольника). 

Любое самосовмещение правильного $n$-угольника переводит его вершины в вершины и центр в центр.
\efct

Пусть $f\in  D_n$, $x_0$ -- некоторая вершина $n$-угольника, $x_1$ -- соседняя вершина с $x_0$. Заметим, что образом точки $x_0$ может служить любая вершина $n$-угольника $y_0$. Итого для образа $x_0$ есть $n$ вариантов. Пусть $x_0$ перешёл в $y_0$. Тогда для $x_1$ есть не более двух вариантов, а именно $x_1$ обязан перейти в соседа $y_0$, которых ровно 2. 

Заметим, что отображение $f$ задаётся образами точек $x_0$ и $x_1$. Действительно, ведь центр $n$-угольника переходит в центр и поэтому фактически мы знаем куда переходят 3 точки, не лежащие на одной прямой -- $x_0, x_1$ и центр. Но для значений $x_0,x_1$ не более $2n$ вариантов, как мы только что поняли. Значит, элементов из $D_n$ не более и, следовательно, ровно $2n$ штук.\\
5) Если $V$ -- векторное пространство над полем $K$, то $\GL(V)$ -- это группа всех обратимых линейных отображений из $V$ в $V$. В частности, если $V=K^n$, то элементы группы $\GL_n(V)$ соответствуют обратимым матрицам с операцией умножения. Группа таких матриц обозначается $\GL_n(K)$.\\

Группа $D_n$ является подгруппой группы $GL_2(\mb R)$. Действительно, любая изометрия переводит параллелограмм в параллелограмм, и, следовательно, сумму векторов в сумму векторов.\\
6) Определим группу аффинных преобразований $n$-мерного пространства как
$$\Aff_n(K)=\AGL_n(K)=\{ f\colon K^n\to K^n\,|\, f(x)=Ax+b, \text{ где } A\in\GL_n(K), \, b \in K^n\}.$$




\section{Подгруппа, порождённая множеством. Циклические группы}

Прежде всего сделаем техническое замечание:

\rm Пусть дан набор подгрупп $H_{\alpha}$, где элементы $\alpha$ пробегают множество индексов $I$. Тогда пересечение $$\bigcap_{\alpha \in I}H_{\alpha}$$
тоже подгруппа $G$.
\erm


\dfn
Пусть $G$ - группа, а $X$ некоторое подмножество $G$. Тогда подгруппой, порождённой $X$, называется наименьшая подгруппа $H\leq G$, содержащая $X$. Будем обозначать эту подгруппу за $\langle X\rangle$. Эта подгруппа всегда существует и совпадает с подгруппой, равной пересечению подгрупп содержащих $X$
$$\langle X\rangle=\bigcap\limits_{ X\subseteq L \leq G} L.$$
Если множество $X$ состоит из конечного числа элементов, $X=\{ x_1,\dots, x_n\}$,  то мы будем писать просто $\langle x_1 ,\dots, x_n\rangle$, а не $\lan\{x_1,\dots,x_n\}\ran$.
\edfn


\utv
Подгруппа $\langle X\rangle$ допускает альтернативное определение $$\langle X\rangle=\{x_1^{\eps_1}\cdots x_n^{\eps_n} \,|\, \text{ где $x_i\in X $, $\eps_i\in \{\pm 1\}$ }\}.$$
Если взять $n=0$, то положим такое произведение равным $1$.
\eutv
\proof Понятно, что элементы вида $x_1^{\eps_1}\cdots x_n^{\eps_n}$ обязаны лежать в подгруппе, содержащей $X$, а значит и в $\lan X\ran$. Это даёт включение $ \{x_1^{\eps_1}\cdots x_n^{\eps_n} \,|\, \text{ где $x_i\in X $, $\eps_i\in \{\pm 1\}$ }\} \subseteq \langle X\rangle.$ Осталось показать включение в другую сторону. Для этого покажем, что множество $\{x_1^{\eps_1}\cdots x_n^{\eps_n} \,|\, \text{ где $x_i\in X $, $\eps_i\in \{\pm 1\}$ }\}$ --- подгруппа в $G$. Так как это множество содержит $X$, то, являясь подгруппой, оно необходимо должно содержать $\lan X\ran$, как наименьшую подгруппу с этим свойством.

Для проверки того, что это подгруппа, надо всего лишь заметить, что произведение выражений $x_1^{\eps_1}\dots x_n^{\eps_n}$  и $y_1^{\gamma_1}\dots y_m^{\gamma_m}$ есть $x_1^{\eps_1}\dots x_n^{\eps_n}y_1^{\gamma_1}\dots y_m^{\gamma_m}$, то есть снова выражение такого вида. 
Так же обратный $(x_1^{\eps_1}\dots x_n^{\eps_n})^{-1}=x_n^{-\eps_n}\dots x_1^{-\eps_1}$ есть снова выражение такого вида. Единица, как мы отметили, соответствует случаю $n=0$.
\endproof

\dfn
Пусть $G$ группа, а $g\in G$ --- некоторый элемент. Подгруппа вида $$\langle g \rangle=\{g^n\,|\, n\in \mb Z\}$$
называется циклической подгруппой группы $G$, порождённой $g$.
\edfn

\dfn Будем говорить, что группа $G$ порождена множеством $X$, если $\lan X\ran =G$. Если множество $X$ конечно и состоит из элементов $x_1,\dots,x_n$, то будем писать $G=\lan x_1,\dots,x_n \ran$ и называть $x_1,\dots,x_n$ образующими $G$. Если такое конечное множество есть, то  будем говорить, что $G$ конечно порождена. 
\edfn

\dfn Группа $G$ называется циклической, если она порождена одним элементом. То есть, если существует такой $g\in G$, что $G=\lan g \ran$.
\edfn

\exm
\enm
\item Группа $\mb Z$ -- циклическая. В качестве её образующих можно взять элементы $1$ или $-1$.
\item Рассмотрим группу $\GL_n(K)$ -- какие у неё образующие? Ответ прост -- это матрицы элементарных преобразований. Простое упражнение состоит в том, что матрицы, соответствующие транспозиции можно исключить из системы образующих.
\item Группа $\mb Z/n$ -- тоже циклическая -- в качестве её образующих можно взять любой элемент $l$ взаимно простой с $n$.
\item Рассмотрим группу $D_n$. Может ли она быть порождена одним элементом? Ответ: нет. Самое простое объяснение заключается в том, что группа $D_n$ не абелева. Ведь поворот на угол $\ffi$ относительно точки $x_0$ и симметрия относительно прямой, проходящей через эту точку не коммутируют (если $\ffi \neq 0, \pi $). С другой стороны, элементы циклической группы $\lan g\ran$ -- степени одного элемента $g$ -- коммутируют между собой, ведь $g^ig^j=g^{i+j}=g^jg^i$.

Чем же можно породить группу $D_n$? Оказывается, двух элементов уже достаточно.

\utv Пусть $f_{\ffi}$ -- поворот на угол   $\ffi=\frac{2\pi}{n}$ и $f_l \in D_n$ -- симметрия относительно какой-нибудь прямой.
Тогда $\lan f_{\ffi},f_l\ran =D_n$.
\eutv
\proof
Совершенно понятно, как получить поворот на угол $\frac{2\pi k}n$ -- это есть $f_{\ffi}^k$. Но как получить произвольную симметрию относительно прямой? Это вопрос чуть сложнее и мы не станем на него отвечать а приведём косвенное доказательство того, что такое представление есть. А именно, рассмотрим набор $f_l^{\eps} f_{\ffi}^k$, где $\eps\in \ovl{0,1}$, а $k\in \ovl{0,n-1}$.  Мы покажем, что все $2n$ элементов этого набора различны и, следовательно, пробегают все элементы группы $D_n$. 

Пусть $f_l^{\eps_1}f_{\ffi}^{k_1}=f_l^{\eps_2}f_{\ffi}^{k_2}$. Значит, $f_l^{\eps_1-\eps_2}f_{\ffi}^{k_1-k_2}=\id$.
Если $\eps_1=\eps_2$, то имеем $f_{\ffi}^{k_1-k_2}=\id$, что возможно только если $k_1=k_2$. Если же $\eps_1\neq \eps_2$, то будем считать $\eps_1-\eps_2>0$. Тогда $f_{l}=f_{\ffi}^{k_2-k_1}$. То есть поворот равен симметрии относительно прямой. Но такого не бывает, так как множество точек, остающихся на месте при симметрии -- это прямая. А множество точек, остающихся на месте относительно поворота: это либо точка, относительно которой поворачивают, либо всё пространство (в случае поворота на угол $0$).
\endproof
\eenm

Как мы уже отметили, группы $\mb Z$ и $\mb Z/n$ --- циклические. Посмотрим ещё в этом направлении. Что происходит, когда мы берём подгруппу, порождённую произвольным элементом $g \in G$? 


\dfn Порядок элемента $g\in G$ --- это количество элементов в подгруппе $\lan g \ran$.
\edfn

\lm Пусть $g$ -- элемент $G$. Тогда если $\ord g$ конечен, то $\ord g=n$, где $n$ -- такое наименьшее натуральное число, что $g^n=e$. Если же порядок $g$ бесконечен, то не существует такого элемента $n\in \mb N$, что $g^n=e$. 
\elm
\proof Пусть $n$ --- такое, что $g^n=e$. Покажем, что $\ord g \leq n$. Рассмотрим элементы $e,g,g^2,\dots,g^n, \dots$. Заметим, что, начиная с $g^n$, все элементы этой последовательности будут повторяться. Точнее, если поделить с остатком $m=nq+r$, где $0\leq r<n$, то
$$g^m=g^{nq+r}=g^r.$$
Так как подгруппа $\lan g \ran$ в точности состоит из элементов вида $g^i$, то мы установили, что различных среди них не более $n$ штук и все они имеют вид $g^r$, где $0\leq r < n$. 

Теперь если  $\ord g= \infty$ и одновременно $g^n=e$ при $n \in \mb N$, то предыдущее рассуждение приводит нас к противоречию. Действительно, условие $g^n=e$ означает, что в $\lan g\ran$ не более чем $n$ различных элементов. В частности, она конечна, что противоречит  $\ord g= \infty$.

Пусть теперь  $m=\ord g < \infty$. Рассмотрим набор элементов $e,\dots,g^m$. Их $m+1$ штука и они лежат в $\lan g \ran$ где ровно $m$ элементов. Значит, среди них есть одинаковые. Пусть это $g^i=g^j$, где $0\leq j <i\leq m$. Домножим на $g^{-j}$ и получим, что $g^{i-j}=e$. Но тогда в $\lan g \ran $ не более $i-j$ элементов по предыдущему рассуждению. Такое бывает только если $i=m$ и $j=0$.  Отсюда видно, что $g^m=e$. К тому же это рассуждение показывает, что элементы $g^i$ при $0<i<m$ не могут быть равны $e$. Значит $m$ -- минимальное. 
\endproof

Здесь в теореме мы снова пользовались тем, что если $g^n=e$, то в подгруппе, порождённой $g$, не более чем $n$ элементов. Сформулируем более точную версию этого утверждения, которая понадобится нам:

\utv Пусть $g\in  G$ и $g^n=e$ для $n\in \mb N$. Тогда $n \di \ord g$.
\eutv
\proof Пусть $m=\ord g$. Тогда $g^m=e$. Поделим с остатком: $n=mq+r$, где $0\leq r < m$. Тогда $e=g^n=g^{mq+r}=g^r$. Но если $r\neq 0$, то это противоречит определению $m$. Значит, $r=0$, что и требовалось.
\endproof









\thrm Пусть $g\in G$ -- элемент порядка $n\in \mb N$. Тогда циклическая группа $\lan g \ran $ изоморфна группе $\mb Z/n$. Если же $\ord g = \infty$, то $\lan g \ran$ изоморфна $\mb Z$.
\ethrm
\proof[Доказательство теоремы] Разберём сначала второй случай. Для этого докажем лемму: 
\lm Пусть $G$ группа, $g\in G$. Тогда существует такой единственный гомоморфизм $f \colon \mb Z \to G$, что $f(1)=g$.
\elm
\proof Для доказательства единственности заметим, что $f(n)$ должно быть равно $f(1)^n=g^n$. 

Итак, осталось показать, что заданное этой формулой отображение --  гомоморфизм групп. Но это равносильно базовым свойствам возведения в степень, которые мы уже обсуждали.
\endproof
\proof[Продолжение доказательства теоремы]
Пусть $\ord g=\infty$. Тогда построим гомоморфизм $\mb Z \to \lan g \ran$, переводящий $1$ в~$g$. В этом случае образ элемента $n \in \mb Z$ будет равен $g^n$. Значит, указанный гомоморфизм сюръективен, так как группа $\lan g \ran$ состоит ровно из степеней $g$. Осталось проверить инъективность. Предположим, что ядро $\Ker f \neq \{0\}$. Тогда есть число $0\neq n \in\Ker f $. Можно считать, что $n$ -- натуральное. Тогда $g^n=e$. Но это означает, что $\lan g \ran$ --- конечная подгруппа в $G$. Что противоречит условию. Значит, ядро тривиально.


Для доказательства в первом случае можно было бы сформулировать аналогичную лемму, но я не буду этого делать, а докажу напрямую.

Пусть $n=\ord g$. Построим отображение $f \colon\mb Z/n \to \lan g\ran $ по правилу $f(\ovl{k})=g^k$ (я тут специально напомнил, что в $\mb Z/n$ живут классы эквивалентности целых чисел).  Прежде всего необходимо проверить корректность такого определения, ведь выбор представителя $k$ может повлиять, априори, на $g^k$. 

Пусть $k_1\equiv k_2 \mod n$. Это значит, что $k_1=k_2+ns$. Но тогда $g^{k_1}=g^{k_2+ns}=g^{k_2}$. Значит, отображение определено корректно. Так же как и в случае с целыми числами, из свойств степени следует, что это гомоморфизм групп.   Осталось проверить, что это биекция. Для этого заметим, что элемент $g^k$ есть образ класса $k$. Это показывает сюръективность. Инъективность теперь следует из принципа Дирихле.
\endproof

Прервёмся на секунду и обсудим примеры подгрупп, порождённых подмножеством: прежде всего разберёмся с "модельной" циклической группой $\mb Z/n$.

\lm Пусть $k\in \mb Z/n$. Тогда $\ord k = \frac{n}{(n,k)}$. 
\elm
\proof Перепишем всё на языке сравнений. Нам надо найти наименьшее натуральное $d$, что $dk\equiv 0 \mod n$. Все решения этого сравнения имеют вид $d=\frac{n}{(n,k)}t$. Наименьшее натуральное решение получается при $t=1$, что и ожидалось.
\endproof

Изоморфизм сохраняет все свойства, которые можно выразить через групповую операцию. В частности, элементы порядка $n$ он переводит в элементы порядка $n$. Если $g$ -- это элемент порядка $n$, то циклическая подгруппа, порождённая им, изоморфна $\mb Z/n$. Отсюда автоматически следует: 

\crl Пусть $G$ -- группа, а $g\in G$ имеет порядок $n$. Тогда элемент $g^k$ имеет порядок $\frac{n}{(n,k)}$ 
\ecrl

А что будет, если взять целые числа? Понятно, что любой элемент внутри $\mb Z$, кроме $0$, имеет бесконечный порядок. Можно спросить поглубже, а чем вообще порождены подгруппы $\mb Z$? Оказывается, ответ довольно прост.

Для элементов группы $D_n$ легко посчитать их порядок: если это поворот, то он лежит в циклической группе, порождённой поворотом на $\frac{2\pi}{n}$, а если это симметрия, то порядок равен 2. Но как проверить, что порядок элемента  $g\in G$ равен $n$ для группы, которая задана не так явно, например, для группы $(\mb Z/n)^*$? Теоретически, можно возводить элемент $g$ подряд во все степени и смотреть, когда же $g^k=1$. Если перебирать таким образом, то может случиться, что вам придётся перебирать $|G|=n$ элементов (например, когда $G$ циклическая группа, а $g$ её порождает). Если $n=2^{2048}$, то такой перебор может и не закончиться. Следующая теорема говорит нам, как можно сэкономить в вычислениях:

\lm Пусть $g \in G$ такой, что $g^n=e$ для числа  $n=p_1^{\alpha_1}\dots p_k^{\alpha_k}$. Тогда если $g^{\frac{n}{p_i}}\neq e$ для всех $i\in\ovl{1,k}$, то $n=\ord g$. 
\elm
\proof Пусть $m=\ord g$. Из условия $g^n=e$ мы знаем, что $m | n$. Пусть $m<n$. Тогда существует такой простой делитель $p_i$ числа $n$, что $n\di p_i^{\alpha_i}$, но $m\ndi p_i^{\alpha_i}$. Заметим тогда, что $n/p_i \di m$. То есть $n/p_i=mk$. Но тогда $g^{n/p_i}=g^{mk}=e$, что противоречит условию теоремы. Значит, $n=m$, что и требовалось.
\endproof

Посмотрим простейшие примеры того, как это утверждение работает. Рассмотрим элемент $2\in (\mb Z/13)^*$. Прежде всего я утверждаю, что $2^{12}=1$. Это следует из теоремы Эйлера. Простые делители $12$ -- это $2$ и $3$. Значит, для того, чтобы показать, что порядок элемента $2$ в группе $(\mb Z/13)^*$ равен 12 нам осталось проверить, что $2^{6}\neq 1$ и $2^4\neq 1$ в $\mb Z/13$. Что довольно легко делается.

Заметим, что так как в группе $(\mb Z/13)^*$ итак 12 элементов, то подгруппа $\lan 2\ran =(\mb Z/13)^*$. То есть $(\mb Z/13)^*$ -- циклическая.










\subsection{Подгруппы циклических групп}



\thrm Пусть $H$ -- подгруппа в циклической группе $G$, тогда $H$ -- циклическая. Более того, если $|G|=n$, то для любого $d|n$ существует единственная подгруппа $H\leq \mb Z/n$, что $|H|=d$. 
\ethrm
\proof

Рассмотрим сначала случай $G=\mb Z$. Это докажет нашу теорему в случае произвольной бесконечной циклической группы.

\lm Пусть $H$ подгруппа в $\mb Z$. Тогда $H$ -- циклическая.
\elm
\proof Мы уже проделывали это доказательство, когда говорили про наибольший общий делитель. Нам надо понять, что $H=\lan n\ran =n\mb Z$. Если $H=\{0\}$, то $n=0$. Иначе рассмотрим в $H$ наименьший натуральный элемент $n$. Покажем, что $H=n\mb Z$. Так как $n \in H$, то и $kn \in H$. Следовательно, получаем включение $n\mb Z \subseteq H$. Покажем обратное включение. Пусть $m\in H$. Поделим с остатком: $m=nq+r$, где $0\leq r<n$. Заметим, что тогда элемент $r=m-nq$ тоже лежит в $H$. Но если $r$ натуральный, то $r<n$ и мы приходим к противоречию с определением $n$. Значит,  $r=0$. То есть $m=nq \in n \mb Z$, что и требовалось. 
\endproof

Если $G$ -- бесконечная циклическая группа, то $G \simeq \mb Z$ и предыдущее утверждение закрывает доказательство этого случая. Пусть теперь $|G|=n$. Тогда $G\simeq \mb Z/n$ и утверждение достаточно доказать для $\mb Z/n$. Вместо прямого доказательства, сведём указанное утверждение к уже известному аналогу для $\mb Z$. Для этого рассмотрим гомоморфизм $\pi\colon \mb Z \to \mb Z/n$ переводящий $x\to \ovl{x}$. Сведение теперь будет основано на лемме:

\lm Пусть $f\colon G_1\to G_2$ гомоморфизм групп и $H\leq G_2$. Тогда $f^{-1}(H)$ -- подгруппа в $G_1$.
\elm
\proof Проверим: $e \in f^{-1}(H)$, то есть, по определению, что $f(e)\in H$. Это так, потому что $f(e)=e\in H$. Аналогично, если $a=f^{-1}(x)$, где $x \in H$, то $f(a^{-1})=x^{-1}\in H$. Если же $f(a), f(b) \in H$, то $f(ab)=f(a)f(b)\in H$. 
\endproof

Итак, пусть $H\leq \mb Z/n$. Тогда, воспользовавшись леммой, получаем, что  $\pi^{-1}(H)$ ---  подгруппа в $\mb Z$. Но такая подгруппа циклическая, то есть $\pi^{-1}(H)=\lan k \ran$. У любого элемента $x\in H$ есть прообраз $a\in \pi^{-1}(H)=\lan k \ran$, так как $\pi$ сюръективно. Но тогда $x\in \lan \ovl{k} \ran$. То есть $H\subseteq \lan \ovl{k}\ran$. Осталось проследить, что верно и обратное включение. Откуда вытекает, что $H=\lan \ovl{k}\ran$ -- циклическая.

Покажем существование и единственность подгруппы порядка $d$ для всех $d|n$.

Вначале предъявим подгруппу порядка $d$. Как мы уже показали, такая подгруппа должна быть циклической. То есть нужно предъявить элемент порядка $d$. Проще всего взять элемент $\frac{n}{d}\in \mb Z/n$.

Покажем, что любая другая подгруппа $H$ порядка $d$ равна $\lan \frac{n}{d} \ran$. Пусть $H$ порождена элементом $x$ порядка $d$. Если отождествить $x$ с соответствующим целым числом, то условие на порядок записывается как  $d=\frac{n}{(n,x)}$, откуда $\frac{n}{d}=(n,x)$. Значит, $x$ кратен $\frac{n}{d}$. Но тогда $H \subseteq \lan\frac{n}{d}\ran$. Так как в обеих группах ровно $d$ элементов, то мы получаем равенство.
\endproof




\section{Классы смежности и теорема Лагранжа}

Из предыдущего раздела осталось несколько вопросов: откуда же взять изначальное условие $g^n=e$ для нахождения порядка элемента $g$? Почему в теореме про подгруппы циклической группы мы ограничились только теми подгруппами, порядок которых делит размер объемлющей группы? Оказывается это всё проявление некоторого общего принципа свойственного всем группам.


\dfn Пусть $H$ --- подгруппа $G$. Определим отношение эквивалентности $\sim_H$ на $G$ следующим образом: $g_1\sim_H g_2$ если $\exists h \in H$, что $g_1=g_2 h$.
\edfn

\utv Это отношение эквивалентности.
\eutv
\proof Действительно. Для того, чтобы показать, что $g\sim_H g$ возьмём $h=e$. 

Симметричность: если $g_1 \sim_H g_2$, то $g_1=g_2h$ для некоторого $h\in H$. Тогда $g_2=g_1h^{-1}$. Осталось заметить, что $h^{-1}\in H$.

Покажем рефлексивность. Пусть $g_1\sim_H g_2 \sim_H g_3$.  То есть $g_1=g_2h_1$, $g_2=g_3h_2$. Но тогда подставив в первое равенство второе получим $g_1=g_3h_2h_1$. Значит, $g_1\sim_H g_3$. Что и требовалось. 
\endproof

Посмотрим как выглядят классы эквивалентности относительно $\sim_H$.

\dfn Пусть $G$ -- группа, $H$ -- подгруппа и задан некоторый элемент $g\in G$. Тогда множество $gH=\{ gh\,|\, h \in H\}$ является классом эквивалентности относительно $\sim_H$. Будем называть $gH$ левым смежным  классом элемента $g$ по подгруппе $H$. 
\edfn

\dfn Множество всех левых смежных классов будем обозначать $G/H$. Количество элементов в $G/H$ называется индексом $H$ в $G$  и обозначается $[G:H]$. 
\edfn

Благодаря тому, что отношение $\sim_H$ -- это отношение эквивалентности получаем, что:


\crl Группа $G$ разбивается в дизъюнктное объединение левых смежных классов $$G=\coprod_{ gH \in G/H} gH.$$
\ecrl




\rm Аналогично определяется правый смежный класс $Hg$ для элемента $g$. Группа $G$ так же разбивается в дизъюнктное объединение правых смежных классов. Множество правых смежных классов обозначается как $H\setminus G$.
\erm 

Это не всё, что нам нужно от смежных классов:

\utv Пусть $H$ -- подгруппа $G$ и $g\in G$ -- некоторый элемент. Тогда отображение $H \to gH$, заданное по правилу $h \to gh$, является биекцией.
\eutv
\proof Построим обратное отображение. Оно берёт элемент $x=gh\in gH$ и отправляет его в $g^{-1}x=h \in H $. Несложно проверить, что это взаимно обратные отображения.
\endproof

\dfn Пусть $G$ -- группа. Тогда число элементов в $G$ называют порядком $G$.
\edfn

\thrm[Лагранжа]  Пусть $G$ -- группа, $H$ -- подгруппа. Пусть порядок  $H$ конечен и индекс $[G:H]$ конечен. Тогда $G$ -- конечная группа и 
 $$|G|=|H|[G:H].$$
\ethrm
\proof По следствию из того, что $\sim_H$ -- отношение эквивалентности, получаем, что группа $G$ разбивается в дизъюнктное объединение левых смежных классов $$G=\coprod_{gH \in G/H} gH.$$
Таких смежных классов по определению $[G:H]$ штук. В каждом смежном классе $gH$ элементов столько же, сколько в $H$, то есть $|H|$. Но тогда число элементов в $G$ конечно и равно $|H|[G:H]$.
\endproof

\crl Пусть $G$ -- конечная группа, а $H$ -- её подгруппа. Тогда $|G| \di |H|$.
\ecrl
\proof В теореме Лагранжа даже сказано откуда берётся дополнительный множитель.
\endproof

\crl Пусть $G$ -- конечная группа. Тогда порядок элемента $g\in G$ делит $|G|$.
\ecrl
\proof Порядок $\ord g$ равен $|\lan g\ran|$. Но $\lan g\ran$ -- это подгруппа в $G$. Применим предыдущее следствие.
\endproof

\crl Пусть $G$ -- конечная группа порядка $n$, а $g$ -- её элемент. Тогда $g^n=e$.
\ecrl  
\proof По предыдущему следствию $n= \ord g \cdot m$, для $m\in \mb N$. Тогда $g^n=(g^{\ord g})^m=e^m=e$. 
\endproof

\crl Пусть $G$ -- конечная группа порядка $p$. Тогда $G$ циклическая и $G \simeq \mb Z/p$. 
\ecrl

\crl Пусть $G$ -- группа порядка $4$. Тогда либо $G \simeq \mb Z/2 \times \mb Z/2$, либо $G \simeq \mb Z/4$.
\ecrl



Некоторые старые факты удобно воспринимать в контексте теоремы Лагранжа.


\crl[Теорема Эйлера] Пусть $n$ -- натуральное число и $a\in \mb Z/n^*$. Тогда $a^{\ffi(n)}=1$.
\ecrl
\proof
Применим следствие из теоремы Лагранжа к группе $\mb Z/n^*$ и элементу $a\in \mb Z/n^*$.
\endproof

\rm Теорема Лагранжа может упростить доказательство того, что $D_n$ порождена поворотом на угол $2\pi/n$ и любой симметрией. Действительно: подгруппа порождённая только поворотом состоит из $n$ элементов. Если мы добавим ещё и симметрию, то должна получиться подгруппа в которой больше элементов. Её порядок должен делиться на $n$, то  есть должен быть не меньше $2n$. Значит, все элементы из $D_n$ мы точно получим.
\erm 




\section{Строение мультипликативной группы поля}


Из предыдущей темы мы знаем, что кольцо $\mb Z/n$ для составного $n$ раскладывается в произведение множителей вида $\mb Z/p^{\alpha}$, и описывать группу обратимых элементов нам нужно только в этом случае. Начнём с самой простой ситуации: $\alpha=1$. В этом случае $\mb Z/p$ -- поле, а группа $\mb Z/p^*$ состоит из $p-1$ элемента. Утверждается, что эта группа циклическая. Для этого нам понадобятся несколько лемм. 

\lm Пусть $n$ --- натуральное число. Тогда $n = \sum_{d|n}\varphi(d)$.
\proof  Рассмотрим циклическую группу $\mb Z/n$. Тогда если $d$ делит $n$, то в этой группе есть единственная подгруппа из $d$ элементов. Все элементы порядка $d$ лежат в этой подгруппе. Эта подгруппа циклическая. Следовательно, их $\ffi(d)$ штук. Тогда, сгруппировав все элементы одинакового порядка, получаем
$$n= |\mb Z/n |= \sum_{d|n} |\{ x \in \mb Z/n \,| \text{ $x$ элемент порядка $d$  }\}| = \sum_{d|n}\varphi(d).$$
\endproof
\elm

\lm Пусть $H$ --- такая конечная группа, что число элементов, удовлетворяющих равенству $x^d= e$, не больше $d$. Тогда $H$ --- циклическая.
\elm
\proof Посчитаем число элементов в $H$. Обозначим его за $n$. Тогда
$$ n = \sum_{d|n} |\{ x \in H \,|\text{  $x$ элемент порядка $d$ в $H$} \}|.$$

Пусть  $x\in H$ порядка $d$. Тогда все элементы из подгруппы $\lan x \ran$ удовлетворяют тождеству $y^d=e$. Их $d$ штук. С другой стороны, по условию в $H$ не более $d$ элементов удовлетворяющих $y^d=e$. Рассмотрим $z \in H$ порядка $d$. Тогда он удовлетворяет $z^d=e$ и, значит, лежит в $\lan x \ran$. Но в $\lan x \ran$ как и в любой циклической группе элементов порядка $d$ ровно $\ffi(d)$. В частности, число элементов порядка ровно $d$ либо $0$, либо $\varphi(d)$, то есть всегда меньше или равно $\varphi(d)$. Тогда
$$n = \sum_{d|n} |\{x \in H \,| \text{  $x$ элемент порядка $d$ в $H$ } \}|\leq \sum_{d|n}\varphi(d) =n$$

Значит, неравенство обращается в равенство для каждого слагаемого. В частности, для того, которое соответствовало элементам порядка $n$. Значит, элементов в $H$ порядка ровно $n$ в точности $\varphi(n)$ штук. В частности, они
есть. Тогда группа $H$ порождена любым из них.\endproof


\thrm[Конечные подгруппы в мультипликативной группе поля] Пусть $H$ -- конечная подгруппа в $K^*$, где $K$ -- поле. Тогда $H$ циклическая.
\proof Решений уравнения $x^d-1 = 0$ в $K$ не более $d$ штук. Значит, их не более $d$ штук в подгруппе $H$. Применим предыдущую лемму.
\endproof
\ethrm

Применяя предыдущую теорему мы сразу же можем описать структуру мультипликативной группы $\mb Z/ p^*$, где $p$ -- простое.

\crl Пусть $p\neq 2$ простое число. Тогда группа $\mb Z/p^*$ изоморфна циклической группе $\mb Z/(p-1)$.
\ecrl

\dfn Если $n\in \mb N$, то число $a$, такое что $\lan a\ran =\mb Z/n^*$ называется первообразным корнем по модулю $n$. Мы показали, что по модулю простого числа есть первообразные корни. Однако это редкость. Для большинства $n$ таких первообразных корней нет.  
\edfn


\section{Проблема дискретного логарифма и алгоритм Диффи-Хеллмана}

Обсудим, как наличие первообразного корня может быть  применено в криптографии. Основную задачу криптографии можно сформулировать так: передать сообщение от одного адресата другому, так, чтобы в случае доступа к каналу связи третьего человека, он не смог получить текст исходного сообщения. 

Точнее, третье лицо может получить только зашифрованное сообщение, по которому, по идее, не должно иметь возможность за разумное время восстановить исходное сообщение. С другой стороны, необходимо, чтобы получатель сообщения смог бы расшифровать полученное (не умерев от нетерпения).

Классические системы шифрования -- системы с закрытым ключом подразумевали, что участники заранее договариваются о закрытом ключе. При помощи этого ключа происходит шифрование и расшифровка. Однако часто у вас нет возможности договориться заранее. 

Решить эту проблему помогает алгоритм описанный в статье \href{https://www-ee.stanford.edu/~hellman/publications/24.pdf}{W. Diffie and M. E. Hellman, New Directions in Cryptography }. Этот алгоритм позволяет договориться об общем секретном ключе двум людям, используя только открытый канал связи.

\dfn
Пусть $G$ --- группа. Рассмотрим некоторый элемент $g\in G$. Тогда для любого $h\in \lan g\ran$ определено число $l$, что $g^l=h$ и $l\leq \ord g$. Такое число $l$ называется логарифмом $h$ по основанию $g$.
\edfn

Проблему дискретного логарифма можно поставить следующим образом. Пусть даны группа $G$, её элементы $g\in G$ и $h\in G$, про которые заведомо известно, что они лежат в $\lan g\ran$. Задача: найти логарифм $h$ по основанию $g$.


Рассмотрим некоторое конечное поле $K$. Нам известно, что группа $K^*$ является циклической. Пусть дан некоторый элемент $g\in K^*$, порождающий группу $K^*$. Тогда задача нахождения дискретного логарифма по основанию $g$ считается трудной. Это даёт возможность предложить  алгоритм для получения общего ключа.

Будем считать общеизвестными описание конечного поля $K$ и первообразный корень $g$ степени $m=|K|-1$. Например, $K=\mb Z/p$ и описание --- это просто задание простого числа $p$. Множество сообщений --- это множество чисел от 1 до $m$. Боб загадывает некоторое число $b\leq m$, а Алиса --- число $a\leq m$. Боб передаёт Алисе число $B=g^b$. Алиса передаёт Бобу $A=g^a$. Тогда оба они знают число 
$$A^b=g^{ab}=B^a,$$
которое и служит секретным ключом.
 

\section{Группа перестановок}

Попытаемся примерить все наши определения к группе перестановок. Прежде всего посчитаем порядок перестановки. Начнём со случая цикла


\dfn[Цикл] Пусть $a_1,\dots,a_k$ -- набор различных элементов из $\{1,\dots,n\}$. Тогда определим $c$ -- элемент из $S_n$, который мы будем называть циклом $(a_1,\dots,a_k)$, следующим образом:
$$c(x)=\begin{cases}
x,\, x \notin \{a_1,\dots,a_k\}\\
a_{i+1},\, x=a_i,\, 1\leq i < k\\
a_1,\, x=a_k
\end{cases}$$
\edfn

\utv Порядок цикла $(a_1,\dots,a_k)$ равен $k$.
\eutv
\proof Для того, чтобы цикл $c=(a_1,\dots,a_k)$ имел порядок $d$ нужно, чтобы $d$ было минимальным таким, что $c^d=\id$, то есть, чтобы после $d$ применений $c$ точки возвращались в себя. Точка $a_1$ возвращается в себя в первый раз после $k$ итераций. Как и все остальные точки из цикла. Точки отличные от $\{a_1,\dots,a_k\}$ и так переходят себя. Значит, $d=k$. 
\endproof

Сведём вычисление порядка для произвольной перестановки к вычислению для циклов. Для этого будут полезны следующие определения.

\dfn[Неподвижная точка] Если $\sigma \in S_n$, то тогда неподвижной точкой называется такой $x\in \{1,\dots,n\}$, что $\sigma(x)=x$. Множество всех неподвижных точек относительно $\sigma$ обозначим как $\Fix(\sigma)$ 
\edfn

\dfn[Носитель] Носителем перестановки $\sigma \in S_n$ называется множество $\supp \sigma$ = $\{1,\dots,n\}\setminus \Fix(\sigma)$
\edfn

\dfn[Независимость] Перестановки $\sigma_1,\sigma_2\in S_n$ называются независимыми, если $\supp \sigma_1 \cap \supp \sigma_2=\varnothing$.
\edfn

\rm Две независимые перестановки коммутируют, то есть $\sigma_1\sigma_2=\sigma_2\sigma_1$.
\erm

\thrm Пусть $\sigma \in S_n$. Тогда существует единственный с точностью до порядка набор независимых циклов $c_1,\dots,c_k$, что $c_i\neq \id$ и 
$$\sigma=c_1\dots c_k.$$
\ethrm
\proof Постараемся привести строгое доказательство. Пример применения алгоритма, стоящего за этим доказательством, смотри внизу. Рассмотрим отношение эквивалентности $\sim$ такое, что $x\sim y$, если существует $k\in \mb Z$, что $\sigma^k(x)=y$. Покажем транзитивность. Если $y=\sigma^k(x)$, а $z=\sigma^l(y)$, то $z=\sigma^l(\sigma^k (x))= \sigma^{l+k}(x)$.

Будем называть класс эквивалентности точки $x$ относительно этого отношения орбитой точки $x$ под действием $\sigma$. Она состоит из всех элементов вида $\sigma^k(x)$, $k \in \mb Z$.

Пусть $\Omega_1,\dots,\Omega_s$ -- это все различные орбиты. Определим перестановки $c_i$, где $i\in\ovl{1,s}$ следующим правилом:
$$c_i(x)=\begin{cases} \sigma(x), x\in \Omega_i\\
x, x\notin \Omega_i
\end{cases}.$$
Я утверждаю, что $c_i$ -- это независимые циклы. Прежде всего заметим, что носитель $c_i \subseteq \Omega_i$. Отсюда следует независимость.

Далее, если $\Omega_i$ состоит из $l$ элементов, то $c_i$ -- это цикл длины $l$ вида 
$$c_i=(x,\sigma(x),\dots,\sigma^{l-1}(x)), $$ где $x$ -- произвольный элемент из $\Omega_i$. Для этого необходимо показать, что это все элементы $\Omega_i$, что они все различны и что $\sigma^l(x)=x$.

Для этого заметим, что если $\sigma^i(x)=x$, для $i>0$, то в орбите $x$ не более чем $i$ элементов, так как, начиная с номера $i$, элементы $\sigma^k(x)$ начинают повторяться.

Предположим теперь, что $\sigma^i(x)=\sigma^j(x)$, где $0\leq j<i<l$. Но тогда, подействовав  $\sigma^{-j}$, получаем, что $\sigma^{i-j}(x)=x$. Но тогда по сделанному замечанию в $\Omega_i$ не более чем $i-j <l$ элементов. Значит, все элементы различные. 

Рассмотрим элемент $\sigma^l(x)$. Прежде всего заметим, что он совпадает с одним из элементов в наборе $x, \sigma(x),\dots,\sigma^{l-1}(x)$ по принципу Дирихле. Пусть с $\sigma^{i}(x)$. Если $i\neq 0$, то $\sigma^{l-i}(x)=x$ и в $\Omega_i$ меньше чем $l$ элементов. Противоречие. Значит, $i=0$ и $\sigma^l(x)=x$.

Итак, $c_i$ -- цикл. Покажем, что 
$$\sigma=c_1\dots c_s.$$
Пусть $x\in \Omega_i$. Заметим, что $\sigma(x)$ тоже лежит в $\Omega_i$. Посчитаем левую часть
$$c_1\dots c_s(x)=c_1\dots c_{i-1} (c_i(x))=c_1\dots c_{i-1} (\sigma(x))=\sigma(x),$$
так как циклы $c_j$ не переставляют элементы из $\Omega_i$ при $i\neq j$. Осталось выкинуть те циклы $c_i$, которые равны тождественной перестановке.

Единственность. Пусть $\sigma= c_1\dots c_k$ -- произведение независимых циклов. Тогда $\supp c_i$ совпадает с некоторой орбитой $\sigma$. Порядок следования элементов в $c_i$ определяется действием $\sigma$ на этой орбите, так как остальные $c_j$ оставляют точки этой орбиты на месте в силу независимости. 
\endproof



\exm \,Перестановка $\sigma=[2,3,1,4,6,5]$ может быть представлена следующим образом. Посмотрим куда переходит $1$ под действием $\sigma^k$. Получаем следующие переходы:
$$1\to 2\to 3 \to 1$$ 
$$4 \to 4 $$
$$ 5\to 6\to 5.$$
Итого: 
$$[2,3,1,4,6,5]=(123)(4)(56)=(123)(56).$$

\utv Пусть перестановка $\sigma \in S_n$ разложилась в виде произведения  независимых циклов $\sigma=c_1\dots c_k$. Обозначим длину цикла $c_i$  как $d_i$. Тогда $\ord \sigma= \Nok(d_1,\dots,d_k)$.
\eutv
\proof Благодаря тому, что независимые перестановки коммутируют получаем
$$\sigma^d=c_1^d\dots c_k^d$$

Заметим, что $c_i^d$ -- тоже независимые перестановки (хотя и не обязательно циклы). Поэтому $\sigma^d$  есть произведение независимых перестановок. Для того, чтобы $\sigma =\id$, необходимо и достаточно, чтобы $c_i^d=\id$. Но это происходит только если $d\di d_i$. Наименьшее такое $d$ -- это $\Nok(d_1,\dots,d_k)$.
\endproof

\utv[Обратный в цикловой записи] Пусть дан цикл $c=(a_1,\dots,a_k)$. Тогда $c^{-1}=(a_k,\dots,a_1)$. Если перестановка   $\sigma= c_1c_2\dots c_s$ представлена в виде произведения непересекающихся циклов $c_i$, то
$$\sigma^{-1}=c_1^{-1}c_2^{-1}\dots c_s^{-1}.$$
\eutv
\proof Утверждение для одного цикла понятно. Далее заметим, что циклы $c_i^{-1}$ так же независимы. Отсюда
$$\sigma^{-1}=c_s^{-1}\dots c_1^{-1}=c_1^{-1}\dots c_s^{-1}.$$
\endproof



Мы разобрались с циклическими подгруппами в $S_n$. Но сама группа $S_n$ не циклическая (при $n\geq 3$) так как не абелева. Хотелось бы что-то сказать про образующие $S_n$. На текущий момент мы знаем, что множество всех циклов порождает $S_n$. Хочется уменьшить набор образующих.

\dfn[Транспозиция] Цикл вида $(ij)$ где $i\neq j$ называется транспозицией.
\edfn

\utv Любая перестановка представляется в виде произведения транспозиций.
\eutv
\proof Для этого достаточно научиться представлять циклы $(a_1,\dots,a_k)$. Но это легко сделать
$$(a_1 a_k)\dots( a_1 a_2)=(a_1,\dots,a_k)=(a_1a_2)\dots (a_{k-1} a_k)$$
\endproof
Здесь мы показали, что один и тот же цикл может быть двумя разными способами представлен в виде произведения транспозиций. Есть ли что-то общее у двух таких представлений?

\dfn[Инверсия] Будем говорить, что пара $i<j$  образует инверсию для перестановки $\sigma$, если $\sigma(i)>\sigma(j)$.
\edfn

\dfn[Чётность и знак] Чётностью перестановки будем называть чётность числа инверсий $Inv(\sigma)$ в ней. Знаком перестановки $\sigma$ будем называть 
$$\sgn(\sigma)=(-1)^{Inv(\sigma)}.$$
\edfn

\rm Знак перестановки можно задать другим способом: 
$$\sgn(\sigma)= \prod_{i>j} \frac{\sigma(i)-\sigma(j)}{i-j}.$$
Действительно, заметим, что знак выражения в правой части равен $(-1)^{Inv (\sigma)}$, так как сомножитель $\frac{\sigma(i)-\sigma(j)}{i-j}$ отрицателен, только если пара $i,j$ задаёт инверсию в $\sigma$.

Осталось показать, что получившееся выражение по модулю равно 1. Для этого перепишем его в виде 
$$\prod_{i>j} \frac{\sigma(i)-\sigma(j)}{i-j}= \frac{\prod_{i>j} (\sigma(i)-\sigma(j))}{\prod_{i>j}(i-j)}
$$
Сделаем в верхнем произведении <<замену переменной>>. Представим $i=\sigma^{-1}(k)$, $j=\sigma^{-1}(l)$. Тогда верхнее произведение превращается в произведение 
$$\prod_{\sigma^{-1}(k)>\sigma^{-1}(l)} (k-l)$$
Видно, что это тоже произведение всех попарных разностей, только не обязательно из большего элемента в паре вычитается меньший. Итого по модулю это произведение равно $\prod_{i>j}(i-j)$, что  и требовалось.
\erm

\rm Удобно представить это выражение, как произведение по двухэлементным подмножествам.
$$\prod_{i>j} \frac{\sigma(i)-\sigma(j)}{i-j}=\prod_{\substack{\{i,j\}\\ i\neq j}} \frac{\sigma(i)-\sigma(j)}{i-j}$$
\erm
 

\lm Отображение $\sgn \colon S_n \to \{\pm 1\}$ является гомоморфизмом групп.
\elm
\proof Пусть $\sigma, \tau \in S_n$. Покажем, что $\sgn \sigma \tau = \sgn \sigma \sgn \tau$. Представим левую часть в виде произведения:
$$\sgn \sigma \sgn \tau= \prod_{\substack{\{k,l\}\\ k\neq l}} \frac{\sigma(k)-\sigma(l)}{k-l} \prod_{\substack{\{i,j\}\\ i\neq j}} \frac{\tau(i)-\tau(j)}{i-j}.$$
Сделаем замену в первом произведении: представим $k=\tau(i),l=\tau(j)$ для единственных $i,j$. Тогда если $\{k,l\}$ пробегает  все двухэлементные подмножества в $\{1,\dots,n\}$, то $\{i,j\}$ тоже пробегает все двухэлементные подмножества. Итого
$$\sgn \sigma \sgn \tau=\prod_{\substack{\{i,j\}\\ i\neq j}} \frac{\sigma(\tau(i))-\sigma(\tau(j))}{\tau(i)-\tau(j)} \prod_{\substack{\{i,j\}\\ i\neq j}} \frac{\tau(i)-\tau(j)}{i-j}$$
Сокращая знаменатели из первого произведения с числителями из второго, получаем как раз $\sgn \sigma \tau$.
\endproof

Перейдём к связи чётности и транспозиций.

\rm Заметим, что чётность транспозиции $(1,2)$ равна $-1$.
\erm

\lm Пусть $g\in S_n$. Тогда имеет место равенство перестановок. $g(1,2)g^{-1}=(g(1),g(2))$.
\elm
\proof Пусть $X=\{1,\dots,n\}$. Нарисуем диаграмму из отображений:
\begin{center}
\begin{tikzpicture}
\node (A) at (2, 0) {$X$};
\node (B) at (0, 0) {$X$};
\node (C) at (2, 1) {$X$};
\node (D) at (0, 1) {$X$};
\path[->,font=\scriptsize,>=angle 60]
(C) edge node[left]{$g$} (A)
(D) edge node[right]{$g$} (B)
(C) edge node[above]{$(1,2)$} (D)
(A) edge node[below]{$g(1,2)g^{-1}$} (B);
\end{tikzpicture}
\end{center}
Перестановка $g\colon X \to X$ сопоставляет новую нумерацию элементам из $X$. Итак, пусть есть некоторый элемент $k\in X$. Число $k$ -- это номер элемента $g^{-1}(k)$ в новой нумерации. Применяя далее к $g^{-1}(k)$ транспозицию $(1,2)$ мы получаем образ $g^{-1}(k)$ под действием $(1,2)$ в старой нумерации. Действуя $g$ переходим обратно в новую нумерацию. 
Таким образом, действует так же как и перестановка $(1,2)$ только с поменянной нумерацией. То есть как перестановка $(g(1),g(2))$.
\endproof

\rm Возможно проще вычислить в лоб, то есть посмотреть, куда по определению переходят $g(1)$ и $g(2)$ и все остальные элементы.
\erm

\crl Знак любой транспозиции равен $-1$.
\ecrl
\proof Пусть дана транспозиция $(i,j)$. Рассмотрим такую перестановку $g\in S_n$, что $g(1)=i$ и $g(2)=j$. Тогда $$\sgn((i,j))=\sgn(g(1,2)g^{-1})=\sgn g \sgn (1,2) (\sgn g)^{-1}=\sgn (1,2)=-1.$$ 
\endproof


\thrm Чётность перестановки $\sigma=\tau_1\dots \tau_k$, где $\tau_i$ -- транспозиции, равна $(-1)^{k}$.
\ethrm
\proof
$$\sgn(\tau_1\dots \tau_k)=\underbrace{(-1)\cdots (-1)}_{k \text{ раз }}=(-1)^k.$$

\endproof

\rm $\sgn \sigma = \sgn \sigma^{-1}$
\erm

Как посчитать знак (чётность) перестановки? Для этого удобно использовать представление перестановки в виде произведения непересекающихся циклов.

\utv Пусть $\sigma=c_1\dots c_k$, где $c_i$ независимые циклы. Тогда
$$\sgn \sigma = (-1)^{\text{число $c_i$ чётной длины}}.$$
\eutv
\proof Цикл длины $k$ раскладывается в виде произведения $k-1$ транспозиции. Значит, циклы нечётной длины дают сомножитель $1$, а циклы чётной длины сомножитель $(-1)$.
\endproof

Есть ещё один эквивалентный способ.
\utv Пусть $\sigma\in S_n$. Тогда
$$\sgn \sigma = (-1)^{n-k},$$
где $k$ -- это число орбит $\sigma$.
\eutv
\proof Множество точек $\{1,\dots,n\}$ разбивается в дизъюнктное объединение  $k$ орбит $\Omega_i$. Пусть $\Omega_i$ состоит из $k_i$ элементов. Получаем, что $\sum_{i=1}^{k} k_i=n$. Каждой орбите соответствует один цикл $c_i$ длины $k_i$ в разложении $\sigma$. Он раскладывается на $k_i-1$ транспозицию. Значит всего в разложении $\sigma$ необходимо взять $\sum_{i=1}(k_i-1)=n-k$ транспозиций. 
\endproof

В качестве примера, в котором важную роль играет понятие чётности перестановки посмотрим на игру в пятнадцать. Выглядит игра следующим образом: дан квадратик $4\times 4$ в пятнадцати клетках которого написаны все числа от 1 до 15, а оставшаяся пуста. Разрешается пустую клетку поменять с любой соседней местами.

$$\begin{array}{|c|c|c|c|}
\hline
1 & 2 & 3 & 4\\
\hline
5 & 6 & 7 & 8\\
\hline
9& 10& 11& 12\\
\hline
13& 14& 15&\\
\hline
\end{array}
$$

Обычно стартовая или финишная позиция для игры -- это указанная выше расстановка чисел. Будем называть её базовой расстановкой.
Далее в разных вариантах игр спрашивают разное. В первоначальной постановке мистер Ной Палмер Чепмэн, создатель головоломки, предлагал при помощи указанных правил из базовой расстановки получить магический квадрат, то есть такую расстановку, что сумма чисел в любой строке и любом столбце была бы одинакова. В таком виде игра в пятнадцать увидела свет в 1874 году в США и к 1880 году стала общеизвестной. 

Другая её вариация состояла в следующем: дана расстановка, которая отличается от базовой только тем, что числа 14 и 15 поменяны местами

$$\begin{array}{|c|c|c|c|}
\hline
1 & 2 & 3 & 4\\
\hline
5 & 6 & 7 & 8\\
\hline
9& 10& 11& 12\\
\hline
13& 15& 14&\\
\hline
\end{array}
$$

Требовалось предъявить такую последовательность ходов, которая переводит эту расстановку в базовую. За решение этой задачи было обещано вознаграждение.

Мы покажем, что решить эту задачу нельзя. Сопоставим каждой расстановке чисел перестановку из $S_{16}$. Прежде всего, вместо пустой клетки поставим число 16. Потом, выпишем числа из всех ячеек, в том порядке, который задаёт базовая перестановка, то есть перечисляя их слева направо, сверху вниз.

Теперь заметим, что если на данном шаге мы находимся в положении, соответствовавшем перестановке $\sigma$, то на следующем шаге, мы получим перестановку $\tau \sigma$, где $\tau$ -- транспозиция.

Пусть мы за $k$ шагов получили из расположения в котором поменяны 14 и 15 местами базовую расстановку. Это означает, что есть некоторые транспозиции $\tau_1,\dots, \tau_k$, что 
$$\id= \tau_1\dots \tau_k\,(14,15).$$
Сравнивая знак справа и слева получаем, что $1=(-1)^{k+1}$, то есть, что $k$ нечётно.

С другой стороны посмотрим на путешествие числа 16 (пустой клетки). В результате число 16 должно вернуться на место. Это означает, что перемещений вверх было столько же, сколько и перемещений вниз, а перемещений вправо столько же, сколько и перемещений влево. Но тогда в сумме число перемещений чётно. Противоречие!


Важным математическим объектом является само множество, а точнее группа чётных перестановок.

\dfn Знакопеременной группой $A_n$ называется
$$A_n=\{\sigma \in S_n\,|\, \sigma \text{ чётна } \} = \Ker \sgn \leq S_n.$$
\edfn

\rm Группа чётных перестановок состоит из $\frac{n!}{2}$ элементов.
\erm

\subsection{Образующие $S_n$ и $A_n$}

Наше рассмотрение понятия чётности началось с того, что мы пытались понять, какие перестановки порождают $S_n$. Попытаемся построить удобные порождающие системы для групп $S_n$ и $A_n$.

Прежде всего сформулируем общий факт про образующие, которым мы уже один раз неявно воспользовались

\utv Пусть $g_1,\dots,g_k $ -- образующие $G$. Для того, чтобы набор $h_1,\dots,h_l\in G$ порождал  $G$ необходимо и достаточно, чтобы все $g_i$ выражались через $h_1,\dots,h_k$. 
\eutv

\utv Группа $S_n$ порождена перестановками $(12), (13),\dots, (1n)$
\eutv
\proof Мы уже знаем, что группа $S_n$ порождена транспозициями. Значит, надо при помощи транспозиций вида $(1i)$ получить произвольные транспозиции.  Имеем
$$(ij)=(1i)(1j)(1i).$$
Что и завершает доказательство.
\endproof

В этой системе образующих $n-1$ перестановка. Можно ли обойтись меньшим числом? Да. Но прежде докажем обобщение той конструкции, которую мы применили для вычисления знака транспозиции.

\utv Пусть $g\in S_n$ и $c=(a_1,\dots,a_k)\in S_n$. Тогда 
$$gcg^{-1}=(g(a_1),\dots,g(a_k)).$$
\eutv
\proof Достаточно дословно повторить аргумент с изменением нумерации.
\endproof

\utv Пусть $\sigma=c_1\dots c_k$ разложена в произведение независимых циклов. Тогда для любого $g\in S_n$
$$g\sigma g^{-1}= gc_1 g^{-1}\dots gc_kg^{-1},$$
есть произведение независимых циклов той же длины, что и у $\sigma$.
\eutv
\proof Равенство для $g\sigma g^{-1}$ очевидно. Так же ясно, что длины циклов $gc_i g^{-1}$ такие же как и у $c_i$. Осталось заметить, что если множества $\{a_1,\dots,a_k\}$ и $\{b_1,\dots,b_l\}$ не пересекались, то $\{g(a_1),\dots,g(a_k)\}$ $\{g(b_1),\dots,g(b_l)\}$ тоже не пересекаются. Это показывает, что $gc_i g^{-1}$ независимые циклы.
\endproof

В качестве завершающего аккорда покажем, что разложение на циклы одинаковой длины необходимо для существования такого $g$. Но сначала пара определений.
\dfn Пусть $\sigma \in S_n$. Тогда её цикленным (или цикловым) типом называется набор упорядоченных пар $(1,k_1),\dots, (n,k_n)$, где $k_i$ -- это число орбит размера $i$ относительно $\sigma$. В этом определении фигурируют именно орбиты, чтобы избежать неоднозначности связанной с циклами длины один.
\edfn

\dfn Пусть $G$ -- некоторая группа. Если $g,h\in G$, то элемент $ghg^{-1}$ называется сопряжённым к $h$ при помощи $g$. Два элемента $h_1$ и $h_2$ называются сопряжёнными, если существует $g \in G$, что $gh_1g^{-1}=h_2$.
\edfn 

\thrm Две перестановки $\sigma_1,\sigma_2\in S_n$ сопряжены тогда и только тогда, когда у них одинаковые цикленные типы.
\ethrm
\proof Надо лишь показать, что одинаковы цикленные типы для перестановок означают, что они сопряжены. Предъявим алгоритм построения перестановки $g$. Для этого выпишем все циклы в $\sigma_1$ в порядке возрастания их длины, не исключая циклы длины 1. Аналогично сделаем для $\sigma_2$. 
$$\sigma_1=(a_1)\dots(a_s)(a_{s+1}a_{s+2})\dots (a_{n-t}\dots a_n)$$
$$\sigma_2=(b_1)\dots(b_s)(b_{s+1}b_{s+2})\dots (b_{n-t}\dots b_n)$$
Под циклом длины $k$ в $\sigma_1$ расположен цикл длины $k$ в $\sigma_2$. Положим $g(a_i)=b_i$. Так как наборы из $a_i$ и из $b_i$ содержат каждый элемент из $\{1,\dots,n\}$ по одному разу, то получилась перестановка. Очевидно $g\sigma g^{-1}=\sigma_2$.
\endproof

Вернёмся к образующим симметрической группы.

\utv Группа $S_n$ порождена перестановками $(12), (1 \dots n)$. 
\proof Выразим перестановки $(1i)$ через данные. Вначале умножим $(12)(12\dots n)=(2\dots n)=\gamma$. Теперь
$$\gamma^{i-2}(12)\gamma^{-(i-2)}=(1i).$$
\endproof
\eutv

Теперь получим набор образующих для $A_n$. Доказательство так же будем вести по индукции.


\thrm Группа $A_n$ порождена циклами $(123),\dots,(12n)$. 
\ethrm
\proof При $n=3$ утверждение очевидно. Возьмём теперь $\sigma\in A_n$ и пусть $\sigma(n)=i$. Заметим, что перестановка $(12n)^2(12i)$ переводит $i\to n$. Тогда $\sigma'=(12n)^2(12i) \,\sigma$ лежит в  $A_{n-1}$, так как она $n$ переводит в $n$ и является чётной, как произведение чётных перестановок. Но по индукционному предположению $\sigma'$ выражается через $(123),\dots,(12\,n-1)$. Значит исходная перестановка выражается через $(123),\dots,(12n)$.
\endproof


\utv Группа $A_n$ порождена циклами $(123),(12\dots n)$, если $n$ нечётно  и  $(123),(2\dots n)$, если $n$ чётно.
\proof В случае нечётного $n$ рассмотрим произведение $(123)^{-1}(12\dots n)=(3\dots n )= \gamma$. Теперь при помощи $\gamma$ получим 
$$\gamma^{i-3}(123)\gamma^{-(i-3)}=(12i).$$
Для чётных $n$:
$$\gamma_i= (2\dots n)^{i-2} (123)(2\dots n)^{-(i-2)}=(1, i,i+1).$$
Теперь 
$$\delta_i=\gamma_{i+1}\gamma_i \gamma_{i+1}^{-1}= (1, i+1,i+2)(1, i,i+1)(1, i+1,i+2)^{-1}=(i+1,i,i+2)=(i,i+2,i+1).$$
Теперь, если мы получили $(12i)$, то получим и $(1,2, i+1)$. Точнее
$$\delta_i^2 (12i)\delta_i^{-2}=(1,2,i+1).$$
При $i+1=n$ стоит воспользоваться $\delta_{i-1}$. Заметим, что это доказательство не работает для $A_4$. Здесь нужно повозиться руками.
\endproof
\eutv


\upr Придумайте доказательство, которое работает в чётном случае лучше (В частности, в случае $A_4$).
\eupr



\section{Разложение в произведение}
Представим себе, что вам дана группа, например как подгруппа в $S_n$ заданная некоторыми образующими. Можно ли как-то упростить её представление? Главный вопрос здесь, что значит упростить. Я предлагаю считать упрощением тот факт, что группа изоморфна прямому произведению. Действительно, пусть 
$$G\cong G_1 \times G_2.$$
Тогда, если  $G$ конечная, то $|G|=|G_1|\cdot |G_2|$. В частности, если обе группы $G_1$ и $G_2$ нетривиальны, то $|G_1|< |G|$ и $|G_2|<|G|$, что уже свидетельствует о некотором упрощении. Покажем ещё несколько результатов в этом направлении.

\utv Пусть $(g,h)\in G_1\times G_2$. Тогда $\ord (g,h)= \Nok(\ord g, \ord h)$.
\eutv
\proof По определению произведения $(g,h)^d=(g^d,h^d)$. Эта пара равна $(e,e)$ только если $g^d=e$ и $h^d=e$. Первое условие означает, что $d\di \ord g$, а второе, что $d\di \ord h$. То есть $d\di \Nok(\ord g, \ord h)$. Наименьшее такое $d$ и есть $\Nok(\ord g, \ord h)$.
\endproof

Произведение $G_1\times G_2$ содержит внутри себя подгруппы $G_1\times \{e\}$  и $\{e\}\times G_2$. Первая -- это фактически $G_1$, а вторая -- это $G_2$. Будем обозначать эти подгруппы просто как $G_1$ и $G_2$. Такое замечание позволяет построить образующие $G_1\times G_2$, если известны образующие для $G_1$ и $G_2$.

\thrm Пусть $g_1,\dots,g_k$ образующие $G_1$, а $h_1,\dots,h_l$ образующие $G_2$. Тогда $(g_1,e), \dots, (g_k,e), (e,h_1),\dots,(e,h_l)$ -- это образующие $G_1\times G_2$.
\ethrm
\proof Рассмотрим пару $(g,h)$. Она представлена в виде произведения двух пар $(g,h)=(g,e)(e,h)$. Представим элемент $g$ в виде $g=g_{i_1}^{\eps_1}\dots g_{i_s}^{\eps_s}$ -- это даст аналогичное разложение для $(g,e)$. Так же для $(e,h)$.
\endproof

Осталось только понять как извлечь из группы $G$ информацию про то, изоморфна ли она прямому произведению каких-то групп. Предположим, что так произошло и $G\cong G_1\times G_2$. Внутри $G_1\times G_2$ есть подгруппы $G_1\times \{e\}$ и $\{e\}\times G_2$, изоморфные $G_1$ и $G_2$ соответственно. Тогда внутри $G$ должны быть подгруппы изоморфные $G_1$ и $G_2$. Это сразу означает, что кандидаты на сомножители мы должны искать среди подгрупп в $G$. На самом деле, можно заметить, что так как в произведении произвольная пара $(g,h)=(g,e)(e,h)$ есть произведение элементов из $G_1\times \{e\}$ и $\{e\}\times G_2$, то аналогичное условие выполнено и для $G$. Это приводит к определению

\dfn Будем говорить, что группа $G$ раскладывается в виде произведения своих подгрупп $G_1$ и $G_2$, если отображение
$$f\colon G_1\times G_2 \to G$$
$$\quad (g,h)\to gh$$
является изоморфизмом. Будем записывать этот факт как $G=G_1\times G_2$.
\edfn

Какие свойства необходимо наложить на подгруппы $G_1$ и $G_2$, чтобы $G$ разложилось в их прямое произведение.Заметим, что свойства этих подгрупп должны быть такие же, как и у $G_1\times \{e\}$ и $\{e\}\times G_2$ в $G_1\times G_2$. Сформулируем их:\\
1) $G_1\times \{e\} \cap \{e\}\times G_2 =\{(e,e)\}$. Действительно, у элемента пересечения каждая компонента равна $e$.\\
2) Если $(g,e)\in G_1\times \{e\}$, а $(e,h)\in \{e\}\times G_2$, то $(g,e)(e,h)=(e,h)(g,e)$. Это верно, потому что оба произведения равны $(g,h)$.\\
3) Любой элемент из $G_1\times G_2$ есть произведение элемента из $G_1\times \{e\}$  и элемента из $\{e\}\times G_2$. Запишем это в ослабленном виде $\lan G_1\times \{e\},\{e\}\times G_2\ran = G_1\times G_2$.\\

Итак, если группа $G$ изоморфна произведению двух подгрупп, то в ней должны быть подгруппы с указанными свойствами. Оказывается, что этих свойств и достаточно.

\thrm Пусть даны две подгруппы $G_1,G_2 \leq G$. Тогда $G_1\times G_2=G$, тогда и только тогда, когда\\
1) $G_1\cap G_2 =\{1\}$.\\
2) Если $g\in G_1$, а $h\in G_2$, то $gh=hg$.\\
3) $\lan G_1,G_2\ran = G$.
\ethrm
\proof Проверим, что указанное отображение есть гомоморфизм. Пусть $g_1,g_2 \in G_1$, а $h_1,h_2\in G_2$. Тогда образ $$f((g_1,h_1)(g_2,h_2))=g_1g_2 h_1h_2.$$
С другой стороны перемножая образы произведения получаем 
$$f((g_1,h_1))f((g_2,h_2))=g_1h_1 g_2h_2.$$
Осталось заметить, что центральные элементы можно переставить благодаря условию 2).

Покажем, то что $f$ -- мономорфизм. Для этого заметим, что пара $(g,h)\in \Ker f$, если $gh=e$, то есть $g=h^{-1}$. Но правая часть лежит в $G_1$, а левая -- в $G_2$. Благодаря условию на пересечение мы знаем, что это единичный элемент. То есть $g=h=e$. Значит ядро  тривиально. 

Покажем сюръективность. Заметим, что образ при гомоморфизме  -- это подгруппа. В данном случае, это подгруппа в  $G$ и она содержит $G_1$ и $G_2$. Но тогда по третьему условию образ равен  $G$.
\endproof

\rm Условие 3) равносильно тому, что любой элемент из $G$ раскладывается в виде произведения $gh$, где $g\in G_1$, а $h\in G_2$ (если есть второе условие). Так же условия 1) и 3') можно заменить на условие, что разложение вида $gh$ единственно.  
\erm

\exm \enm
\item Пусть $H=\lan (123) (56), (124)(67)\ran \leq S_7$. Покажем, что $H$ раскладывается в произведение двух групп. Прежде всего поменяем набор образующих у $H$. Пусть $h=(123) (56)$, а $g=(124)(67)$. Тогда $h^3=(56)$ И, значит, циклы $(56)$, $(123)$ по отдельности тоже лежат в $H$. Аналогично
с $(124)$ и $(67)$. Значит $H$ порождена
$$H=\lan (123),(56), (124), (67)\ran.$$
Теперь легко заметить две подгруппы в $H$, в произведение которых она раскладывается: $H_1=\lan (123), (124)\ran$ и $H_2=\lan (56), (67)\ran$. Действительно, так как образующие из $H_1$ независимы с образующими $H_2$, то все возможные элементы из $H_1$ и $H_2$ коммутируют  между собой, а все элементы из пересечения обязаны совпадать с тождественной перестановкой. Понятно, что $H$ порождена $H_1$ и $H_2$.

Теперь, неплохо бы понять, чему изоморфны $H_1$ и $H_2$. Заметим, что $H_1$ это в точности $A_4$. Проще всего это понять, увидев в образующих $H_1$ образующие $A_4$. Но можно это сделать используя косвенные аргументы: заметим, что порядок $H_1$ делится на $3$ по теореме Лагранжа так как группа содержит элемент порядка $3$. С другой стороны, произведение $(123)(124)=(13)(24)$ и сопряжение $(123) (13)(24)(123)^{-1}=(12)(34)$ дают две образующих подгруппе $V_4=\{\id, (12)(34), (13)(24), (14)(23)\}$.  Это означает, что $|H_1|\di 4$ и отсюда $|H_1|\di 12$. Но $H_1 \leq A_4$, в которой итак 12 элементов. Значит имеет место равенство

В свою очередь $H_2\simeq S_3$, как группа перестановок на элементах $5,6,7$. Отсюда $H\cong A_4 \times S_3$.
\item Не только независимость приводит к тому, что перестановки в $S_n$ коммутируют. Пусть $$H=\lan (12)(34)(567), (13)(24)(5678)\ran=\lan (12)(34), (567), (13)(24)(5678)\ran ,$$
то перестановки $g=(12)(34)$ и $h=(13)(24)(5678)$ коммутируют, что проверяется непосредственно. Теперь несложно показать, что $$H\cong \lan (12)(34) \ran \times \lan (13)(24)(5678), (567) \ran.$$
Первая группа изоморфна циклической группе $\mb Z/2$. А про  вторую можно утверждать, что она изоморфна $S_4$. Для этого покажем, что элемент из $\sigma \in H_2$, действующий тождественно на $5,6,7,8$ должен быть тривиальным. Посмотрим на чётность. Если записать $\sigma$ как произведение образующих, то образующая $(13)(24)(5678)$ входит в это разложение чётное число раз. Но тогда на элементах $1,2,3,4$ перестановка $\sigma$ действует тривиально. Итак, перестановка из $H_2$ определяется своим действием на $5,6,7,8$. Но несложно понять, что любая перестановка этих элементов реализуется (так как что-то является образующими чего-то).
\eenm



\subsection{Структура $\mb Z/n^*$}

Если $n=p_1^{\alpha_1}\dots p_k^{\alpha_k}$, то мы уже знаем, что 
$$(\mb Z/n)^* \cong \mb (Z/p_1^{\alpha_1})^{*}\times \dots \times \mb (Z/ p_k^{\alpha_k})^{*}.$$
Итого надо разобраться со степенью простого. Для этого нам понадобится техническая лемма.



\lm Пусть $p$ простое и при нечётном $p$ верно, что $s\geq 1$, а при $p=2$ -- что $s\geq 2$. Тогда, если $$x \equiv 1+cp^s \mod p^{s+1}, \text{ то } x^p\equiv 1+cp^{s+1} (\mod p^{s+2}).$$
\elm
\proof Мы знаем по предположению, что $x=1+p^s(c+rp)$. Тогда $$x^p=(1+p^s(c+rp))^p=1+pp^s(c+rp)+C_p^2p^{2s}(c+rp)^2+\dots+p^{ps}(c+rp)^{p}\equiv 1+cp^{s+1} \mod p^{s+2}.$$
\endproof



\utv Пусть  $p$ -- нечётное простое. Тогда ${\mb Z/p^{\alpha}}^*$ изоморфна циклической группе 
$$\mb Z/p^{\alpha-1} (p-1) \cong \mb Z/(p - 1) \times \mb Z/p^{\alpha-1}.$$
Если же $p=2$, то если $\alpha = 1$, то группа $\mb Z/p^{\alpha}$ тривиальна.\\
2) если $\alpha \geq 2$, то $\mb Z/^*p^{\alpha}$ изоморфна произведению $ \mb Z/2 \times \mb Z/2^{\alpha-2}$.
\eutv
\proof
Пусть $p$ -- нечётное. Рассмотрим подгруппу $H_1=\{ x\equiv 1 \mod p\}$. Её порядок $p^{\alpha -1}$. Я утверждаю, что она порождена $1+p$. Для этого проверим, что порядок $1+p$ не меньше $p^{\alpha-1}$. Для этого надо проверить, что $(1+p)^{p^{\alpha-2}} \not\equiv 1 \mod p^{\alpha}$. Но это так -- последовательно используя лемму получаем, что
$$(1+p)^{p^{\alpha-2}}\equiv 1+p^{\alpha-1}\not\equiv 1 \mod p^{\alpha}.$$
Значит $H$ -- циклическая. Посмотрим теперь, откуда берётся циклическая подгруппа из $p-1$ элемента. Для этого рассмотрим $g$ -- первообразный корень степени $p-1$ в поле $\mb Z/p$. В частности, $g$ удовлетворяет уравнению $g^{p-1}-1=0$. По лемме Гензеля у решения этого уравнения есть подъём до решения $\hat{g}$ по модулю $p^{\alpha}$. Это значит, что $\hat{g}^{p-1}-1=0$ в $\mb Z/p^{\alpha}$ и $\hat{g}\equiv g \mod p$. Первое условие говорит, что порядок $g$ есть делитель $p-1$, а второе -- что это ровно $p-1$.

Итак в $\mb Z/^*p^{\alpha}$ есть две подгруппы $H_1=\lan 1\ran$ и $H_2=\lan \hat{g} \ran$. Так как порядки этих подгрупп взаимно просты, то сами подгруппы пересекаются только по нейтральному. Порядок подгруппы, их содержащей  должен делиться на $p-1$ и $p^{\alpha-1}$, то есть на $(p-1)p^{\alpha-1}$, откуда следует, что $H_1$ и $H_2$ порождают всю группу. Коммутативность так же имеет место. Итого, группа разложилась в произведение.

С $\mb Z/2^{\alpha}$ поступим так же. Случай $\alpha=1,2$ понятны. Рассмотрим подгруппу $H_1=\{\pm 1\}$ в $(\mb Z/2^\alpha)^*$. Если $\alpha\geq 3$, то в $(\mb Z/2^\alpha)^*$ есть нетривиальная подгруппа 
$$H_2=\{ x\in \mb Z/2^\alpha\,|\, x \equiv 1 \mod 4\}.$$
Покажем, что $(\mb Z/2^\alpha)^* =H_1 \times H_2$. Действительно, так как $-1\not\equiv 1 \mod 4$, то пересечение состоит из единичного элемента. Все элементы коммутируют так как мы находимся внутри абелевой группы. Для того, чтобы проверить третье условие заметим, что если $x\in (\mb Z/2^\alpha)^*$, что $x\equiv 1\mod 4$, то он и так лежит в $H_2$, а если $x\equiv -1 \mod 4$, то $x$ лежит в $(-1)H_2$.

Осталось понять, что $H_2$ -- циклическая. Рассмотрим элемент $5=1+4\in H_2$. Используя лемму получим, что $\ord 5 = 2^{\alpha-2}$, что и требовалось. 

\endproof

Подведём итог: 

\crl[Ответ в зависимости от разложения] Пусть $n=2^kp_1^{\alpha_1}\dots p_s^{\alpha_s}$. Тогда\\
1) если $k = 0,1$, то  $\mb Z/n^*$ изоморфна $$\prod_{i=1}^s \mb Z/p_i^{\alpha_i-1}(p_i -1).$$
2) если $ k\geq 2$, то $\mb Z/n^*$ изоморфна $$\mb Z/2 \times \mb Z/2^{k-2} \times \prod_{i=1}^s \mb Z/p_i^{\alpha_i-1}(p_i -1).$$
\ecrl

\subsection{Доказательство теоремы Рабина}

\thrm[Рабин] Пусть $n$ нечётное составное число,  $n>9$. Тогда $S(n)\leq \frac{\varphi(n)}{4}$, где $S(n)$ -- множество свидетелей простоты в тесте Миллера-Рабина.
\ethrm
\proof Пусть $n=p_1^{\alpha_1}\dots p_k^{\alpha_k}$, а $n-1=2^sd$, где $d$ -- нечётно. Покажем, что $S(n)$ лежит в некоторой подгруппе внутри $\mb Z/n^*$ индекса по крайней мере $4$.

Для этого рассмотрим  такое наибольшее $l$, что для любого $p_i$ существует $b$, что $b^{2^l}=-1 \mod p_i^{\alpha_i}$ (понятно, что такое есть). Рассмотрим теперь $m=2^ld$ и подгруппы
$$H=\{ x\,|\, x^m\equiv \pm 1 \mod n\}, \quad
H_1=\{ x\,|\, x^m \equiv \pm 1 \mod p_i^{\alpha_i} \,\,\forall i\}.$$
Очевидно, $H \leq H_1$. Заметим, что $S(n) \subseteq H$. Действительно, если $a\in S(n)$, то $(a^d)^{2^r}= -1$ (либо $a^d=1$, что нас и так устраивает).  Но тогда, взяв $b=a^d$ получим, что $l\geq r$, так как $l$ -- наибольшее.

Посчитаем индекс $H \leq H_1$. Удобно посмотреть на $H_0= \{ x^m \equiv 1 \mod n$. Понятно, что $H_0 \leq H$ -- подгруппа индекса 2. Посчитаем индекс $H_0$ в $H_1$. Для этого заметим, что в $H_1$ реализуется любая из $2^k$ комбинаций знаков в сравнениях (благодаря определению числа $l$). Каждая такая комбинация соответствует смежному классу. Теперь получаем, что индекс $$[H_1:H]=[H_1:H_0]/[H:H_0]=2^{k-1}$$
Итого: при $k\geq 3$ теорема доказана.


Пусть $k=2$. Заметим, что любой элемент из $H_1$ удовлетворяет свойству $x^{n-1}=1$. Для этого надо показать, что $n-1 \di 2m=2^{l+1}d$. Заметим, что все простые $p_i$ имеют вид $p_i=1+c_i2^{l+1}$, потому что по модулю каждого простого есть элемент порядка $2^{l+1}$. Но тогда и $n=1+c2^{l+1}$. Тогда $n-1\di 2^{l+1}$ и $n-1\di d$. Отсюда получаем, что $n-1 \di 2m$. Теперь  $x=p_1^{\alpha_1}p_2^{\alpha_2}$. Рассмотрим случай, когда в этом разложении есть кратные множители, то есть, скажем, $\alpha_1\geq 2$. В этой ситуации в $\mb Z/n^*$ есть элемент порядка $p_1^{\alpha_1-1}$. Но элемента такого порядка нет в $H_1$, так как порядки элементов $H_1$ делят $2m$. Итого $H_1 \neq \mb Z/n^*$ и общий индекс $[\mb Z/n^*: H]=[\mb Z/n^*:H_1][H_1:H]\geq 4$.

Пусть теперь $n=p_1p_2$.  Заметим, что в $Z/n^*$ есть элемент порядка $p_2-1$. Покажем, что его нет в $H_1$ так как он не удовлетворяет условию $x^{n-1}=1 \mod n$. Посмотрим на $n-1 \mod p_2-1$. Имеем $$n-1=p_1p_2-1\equiv p_1-1 \mod p_2-1.$$
Значит $(n-1,p_2-1)=(p_1-1,p_2-1)<p_2-1$. То есть в $H_1$ нет элемента порядка $p_2-1$. Что приводит к оценке индекса $H_1$ в $\mb Z/n^*$.

Последний случай: $n=p^{\alpha}$. Тогда индекс $H_1$ в $\mb Z/n^*$  делит (на самом деле равен) $p^{\alpha-1}$. Действительно, порядки элементов из $H_1$ делят и $n-1$ и порядок группы $p^{\alpha-1}(p-1)$. Значит порядки этих элементов делят $p-1$. Но группа $(\mb Z/p^{\alpha})^*$ циклическая. Все элементы порядка делящего $p-1$ содержатся в подгруппе из $p-1$ элемента. То есть $p-1 \di |H_1|$.    Если $p\geq 5$, то это нам подходит. Как и в случае  $p=3$, так как по нашему условию из этого следует, что $\alpha\geq 3$.

\endproof


\upr $S(n)$ -- не подгруппа в $\mb Z/n^*$.
\eupr

\subsection{Дополнение: другое доказательство теоремы Рабина}

Вообще, коль скоро мы знаем строение $\mb Z/n^*$, то можно просто в лоб подсчитать число элементов в $S(n)$ и вывести отсюда теорему Рабина. Можно посмотреть, что из этого получается.

\begin{thmm}[Рабин] Пусть $n$ нечётное составное число,  $n>9$. Тогда $|S(n)|\leq \frac{\varphi(n)}{4}$.
\end{thmm}
\proof Пусть $n$ -- это $p_1^{\alpha_1}\dots p_k^{\alpha_k}$, а $n-1=2^rd$. Тогда группа $\mb Z/n^*$ изоморфна группе
$$ \prod_{i=1}^k \mb Z/p_i^{\alpha_i-1}(p_i -1).$$
Видно, что в задаче выделенную роль играет степень двойки. Пусть $p_i-1=2^{s_i}r_i$. Тогда группа $\mb Z/p_i^{\alpha_i-1}(p_i -1)$ изоморфна 
$$\mb Z/2^{s_i}\times \mb Z/p_i^{\alpha_i-1}r_i.$$
Итого имеем 
$$\mb Z/n^*\cong \prod \mb Z/2^{s_i} \times \mb Z/p_i^{\alpha_i-1}r_i.$$ 
Рассмотрим элемент $x\in \mb Z/n^*$ и посмотрим, что же означает условие, что $x^m=1$ или $x^{2^sm}=-1$ для его компонент. 
Для того, чтобы $y=-1$ необходимо и достаточно, чтобы все его компоненты в группах $\mb Z/2^{s_i}$ были единственным элементом порядка $2$, а все компоненты в группах $\mb Z/p_i^{\alpha_i-1}r_i$ были равны 0 (здесь используется аддитивная запись).

Для начала посмотрим на компоненты в группах $\mb Z/p_i^{\alpha_i-1}r_i$. Если $2^smb=0 (\mod p_i^{\alpha_i-1}r_i)$, то это означает, что порядок $b$ делит $2^{s_i}m$. Множество таких элементов образует подгруппу порядка $\Nod(2^{s_i}m,r_ip_i^{\alpha_i-1})=\Nod(m,r_i)$, так как $(p_i,2^{s_i}m)=1$ и $(r_i,2^{s_i})=1$.

Теперь рассмотрим произведение $\prod \mb Z/2^{s_i}$. То, что элемент $2^{s}mx$ имеет порядок 2 $(\mod \mb Z/2^{s_i})$ означает, что $x$ имеет порядок $2^{s+1}$. Таким образом $T$ принадлежат только те элементы чьи компоненты в $\mb Z/2^{s_i}$ имеют одинаковый порядок для всех $i$.

Сколько таких элементов в $\prod \mb Z/2^{s_i}$? Рассмотрим $l= \min_{i=1}^k s_i$. Тогда порядок каждой компоненты элемента не может превосходить $2^l$ -- он не может получиться одинаковым по всем компонентам. Сколько элементов, которые имеют порядок $2^l$ по каждой компоненте? Их $2^{(l-1)k}$ штук. Аналогично элементов, чьи компоненты порядка  $2^i$ ровно $2^{(i-1)k}$ штук. Итого
$$\sum_{i=1}^l 2^{(i-1)k}= \frac{2^{lk}-1}{2^k-1}.$$ Заметим, что ещё есть первое условие которое подходит только для нейтрального элемента. Итого из сомножителя $\prod \mb Z/2^{s_i}$  подходящих нам вариантов ровно $$\frac{2^{lk}-1}{2^k-1}+1\leq 2\cdot 2^{k(l-1)}.$$
В случае $k,l\neq 1$ можно поставить строгое неравенство. Объединяя, получаем, что всего элементов в $T$ не более
$$2\cdot 2^{k(l-1)}\prod_i\Nod(r_i,m).$$
Покажем, что 
$$\frac{|S(n)|}{\ffi(n)}\leq 2\frac{ 2^{k(l-1)}\prod_i\Nod(r_i,m)}{(p_i-1)p_i^{\alpha_i-1}}\leq \frac{1}{4}.$$
Прежде всего заметим, что выражение $2^{l-1}\Nod(r_i,m)$ делит $\frac{p_i-1}{2}$.
Тогда можно написать оценку
$$2\frac{ 2^{k(l-1)}\prod_i\Nod(r_i,m)}{(p_i-1)p_i^{\alpha_i-1}}\leq \frac{2}{2^k}\prod \frac{1}{p_i^{\alpha_i-1}}.$$
Получаем, что если простых сомножителей не меньше 3-ёх, то неравенство выполнено. Действительно -- в знаменателе возникает множитель $8$, который может сократиться только с одной двойкой сверху. 

Рассмотрим случай, когда различных простых ровно 2. Заметим, что, если есть кратные множители то в последнем множителе есть не единичный элемент в знаменателе. 

Итак остался случай, когда есть два  не кратных множителя $n=pq$ или один кратный $n=p^{\alpha}$. При $n>9$  последний случай очевидно подходит. Теперь $n=pq$, $p>q\geq 3$. Вернёмся к изначальной оценке и посмотрим на $\Nod (m,p-1)=\Nod(m,r)$. Заметим, что $2^t \Nod (m,p-1)= \Nod(n-1,p-1)$. Оценим последний $\Nod$. Заметим, что $n-1=pq-1\equiv q-1 (\mod p-1)$.
Тогда $\Nod(n-1,p-1)\leq q-1 < p-1$ и, так как является делителем $p-1$, то меньше, чем $\frac{p-1}{2}$. 
Итого $$\Nod (m,p-1)\leq \frac{1}{2}\Nod (n-1,p-1)\leq\frac{1}{4}(p-1),$$
что и даёт нужное неравенство.
\endproof


\section{Нормальные подгруппы и факторгруппа}

Однако далеко не всегда группа раскладывается в прямое произведение. Так, например, $\mb Z/p^2$, где $p$ простое не раскладывается нетривиальным образом в произведение двух подгрупп. Действительно, в ней всего одна нетривиальная подгруппа $\lan p \ran$. Найти непересекающуюся с ней нетривиальную подгруппу не представляется возможным. 

Однако мы довольно хорошо умеем упрощать работу по модулю $p^2$ переходя к сравнениям по модулю $p$. С точки зрения теории групп это означает, что мы берём образ всех элементов относительно сюръективного гомоморфизма 
$$\mb Z/p^2 \to \mb Z/p.$$
Возможность проконтролировать ситуацию так же гарантируется тем, что мы знаем, что любые два прообраза у элемента $\mb Z/p$ отличаются на элемент ядра.
Рассмотрим подробнее такую ситуацию. Пусть дан сюръективный гомоморфизм $f \colon G \to H$. Какую информацию про группу $G$ мы можем получить зная информацию про $H$ и $\Ker f$?

\utv Пусть дан сюръективный гомоморфизм $f \colon G \to H$. Пусть так же $g_1,\dots,g_k$ образующие $\Ker f$, а $h_1,\dots, h_l$ образующие $H$. Если взять $h_i'\in G$, такие, что $f(h_i')=h_i$, то группа $G$ будет порождена $h_1',\dots,h_l', g_1,\dots,g_k$.
\eutv
\proof  Воспользуемся простенькой леммой
\lm Пусть $f\colon G \to H$ -- гомоморфизм групп. Тогда $f(g_1)=f(g_2)$ тогда и только тогда, когда $g_1\in g_2 \Ker f$.
\elm
Пусть $g\in G$. Тогда $f(g)=h_{i_1}^{\eps_1}\dots h_{i_s}^{\eps_s}$. Тогда $g'={h_{i_1}'}^{\eps_1}\dots {h'_{i_s}}^{\eps_s}$ обладает свойством $f(g')=f(g)$. Тогда по лемме $g\in g'\Ker f$, то есть выражается через нужные образующие.
\endproof

\utv  Если $G$ конечна  и  $f\colon G \to H$ сюръективный гомоморфизм групп, то $|G|=|\Ker f| |H|$.
\eutv
\proof Каждый элемент $H$ соответствует ровно одному смежному классу $G/\Ker f$ по лемме.
\endproof

Здесь в рассмотрение попала подгруппа $\Ker f$ в $G$. Именно она в паре с $H$ позволила восстановить некоторую информацию про $G$. Таким образом, если мы ищем гомоморфизм $G$ в некоторую группу $H$, то сначала логично найти кандидата на ядро этого гомоморфизма, то есть подгруппу в $G$. Любая ли подгруппа может быть ядром гомоморфизма? 

Нет. Действительно, если $f \colon G \to G_1$, и есть $g \in G$ и $x\in \Ker f$, то  сопряжённый $gxg^{-1}$ тоже лежит в $\Ker f$. 
$$f(gxg^{-1})=f(g)f(x)f(g)^{-1}=f(g)f(g)^{-1}=e.$$
Это приводит нас к следующему определению.

\dfn[Нормальная подгруппа] Подгруппа $H\leq G$ называется нормальной, если для любого $g\in G$ и любого $h \in H$ выполнено, что $ghg^{-1}\in H$. То, что $H$ является нормальной подгруппой будем обозначать как $H \nrml G$
\edfn

\exm \enm
\item Пусть $G$ -- абелева группа. Тогда любая подгруппа в $G$ нормальна.
\item Пусть $f\colon G \to H$ -- гомоморфизм групп. Тогда $\Ker f$ -- нормальная подгруппа в $G$.
\item Подгруппа $V_4=\{\id, (12)(34), (13)(24), (14)(23)\}$ нормальна в $S_4$.
\item Подгруппа, порождённая поворотом на $\pi$ нормальна в $D_n$.
\item Если $G=G_1\times G_2$, то $G_1,G_2\nrml G$.
\eenm


Сформулируем некоторые переформулировки этого свойства.

\utv Пусть $H \leq G$. Тогда следующие утверждения эквивалентны:\\
1) $H$ -- нормальная подгруппа в $G$.\\
2) для любого $g\in G$ $gHg^{-1}\subseteq H$.\\
3) для любого $g\in G$ $gHg^{-1}= H$.\\
4) для любого $g\in G$ $gH=Hg$.\\
5) для любого $g\in G$ $gH\subseteq Hg$.
\eutv

Посмотрим, что нормальные подгруппы раньше попадались нам.\\


\exm \enm
\item Пусть $A$ -- абелева группа. Тогда любая подгруппа $H$ в $A$ нормальна. Действительно если взять $h\in H$, то в силу коммутативности $aha^{-1} =h \in H$.
\item В частности, $n \mb Z \nrml \mb Z$. 
\item Ядро любого гомоморфизма $f\colon G \to H$ является нормальной подгруппой в $G$.
\item В частности, $A_n \nrml S_n$.
\item В качестве примера, не подходящего под общую канву, рассмотрим группу $$V_4=\{\id, (12)(34),(13)(24),(14)(23)\} \nrml S_4.$$ 
\eenm

Пусть теперь нам дана нормальная подгруппа $H\nrml G$. Как же построить гомоморфизм из $G$ в некоторую группу $G_1$, так, чтобы ядром было в точности $H$. Предположим, что $f\colon G \to G_1$ имеет ядром $H$. Тогда заметим, что все элементы вида $xH$ переходят туда же, куда и $x$. Заметим и обратное, если $f(y)=f(x)$, то $y=xh$, где $h\in \Ker f =H$. Таким образом, элементам $G_1$ должны однозначно соответствовать смежные классы $G/H$. Это соображение и положим в основу определения.

\dfn[Факторгруппа] Пусть $H\nrml G$. Определим на множестве смежных классов $G/H$ структуру группы положив 
$$g_1 H g_2 H= g_1g_2 H.$$
\edfn

\utv Приведённая конструкция действительно задаёт группу. Более того отображение $G \to G/H$ переводящее $g \to gH$ является сюръективным гомоморфизмом групп с ядром $H$. 
\eutv 
\proof Прежде всего проверим корректность. Пусть $g_1,g_2\in G$ и $g_1h_1$ и $g_2h_2$ эквивалентны им. Тогда $$g_1h_1g_2h_2=g_1g_2(g_2^{-1}h_1 g_2) h_2 \in g_1g_2H,$$
Что и требовалось. Здесь один раз пришлось воспользоваться нормальностью. Проверка ассоциативности происходит точно так же как и для $\mb Z/n$. А именно, если даны три класса $g_1H, g_2H, g_3H$, то
$$(g_1H g_2H) g_3H=g_1g_2H g_3H=(g_1 g_2) g_3H=g_1 (g_2 g_3)H=g_1H (g_2 g_3H)=g_1H (g_2H g_3H).$$
Нейтральным элементом является класс $1\cdot H=H$, обратным для класса $gH$ является класс $g^{-1}H$.
\endproof

\thrm[Универсальное свойство] Пусть $G,G_1$ -- группы, $H$ -- подгруппа в $G$. Тогда для любого гомоморфизма $f \colon G\to G_1$, такого что $ H \leq \Ker f$, существует единственный гомоморфизм $\ffi\colon G/H \to G_1$ такой, что <<треугольник коммутативен>>:
\begin{center}
\begin{tikzpicture}
\node (A) at (0, 0) {$G$};
\node (B) at (2.5, 0) {$G_1$};
\node (C) at (0, -1) {$G/H$};
\path[->,font=\scriptsize,>=angle 60]
(A) edge node[above]{$f$} (B)
(A) edge node[right]{$\pi$} (C);
\path[dashed,->,font=\scriptsize,>=angle 60]
(C) edge node[below]{$\exists !\, \ffi$} (B);
\end{tikzpicture}
\end{center}
\proof Заметим, что необходимо, чтобы $\ffi(gH)=f(g)$. Это показывает единственность. Покажем, что отображение $\ffi$, заданное этой формулой, корректно определено и является гомоморфизмом. Если взять другого представителя класса $g_1\in gH\subseteq g \Ker f $, то образ его, будет равен образу $g$.

Проверим, что это гомоморфизм: $\ffi(g_1g_2H)=f(g_1g_2)=f(g_1)f(g_2)=\ffi(g_1H)\ffi(g_2H).$
\endproof
\ethrm

\thrm Пусть $f\colon G \to G_1$ гомоморфизм групп. Тогда имеет место изоморфизм 
$$G/\Ker f \simeq \Im f.$$
Этот изоморфизм переводит класс $g \Ker f \to f(g)$. 
\ethrm
\proof Без ограничения общности можно считать, что $G_1=\Im f$. По универсальному свойству существует отображение $\ffi : G/H \to G_1$, заданное как $\ffi(gH)=f(g)$.

Покажем его инъективность. Пусть $\ffi(gH)=e$. Значит $f(g)=e$. Значит $g\in \Ker f=H$. Но тогда $gH=H$. То есть в ядре $\ffi$ лежит только тривиальный элемент факторгруппы, что  и требовалось.

Покажем сюръективность. Пусть $a\in \Im f$. Тогда существует $g\in G$, что $f(g)=a$. Тогда $a=\ffi(gH)$. Значит, отображение $\ffi$ сюръективно.
\endproof

В частности, это означает, что все сюръективные гомоморфизмы устроены как гомоморфизм факторизации. Обычно, однако, эту теорему используют для описания факторгруппы $G/H$, c помощью построения сюръективного гомоморфизма $G \to G_1$, ядро которого есть $H$. 



\exm \\
1) $S_n/A_n\simeq \{\pm 1\}$ так $A_n$  -- это ядро отображения знака.\\
2) $D_n/C_n \simeq \mb Z/2$ так как повороты -- это ядро отображения <<ориентации>>.\\
3) $\AGL_n(K)/\{f(x)=x+b\}\simeq \GL_n(K)$. Сдвиги -- это ядро гомоморфизма, сопоставляющего преобразованию вида $f(x)=Ax+b$ матрицу $A$.\\
4) В теореме Рабина мы встретили несколько подгрупп в группе $\mb Z/n^*$ 
$$H_2=\{ x\,|\, x^m\equiv 1 \mod n\} \leq H=\{ x\,|\, x^m\equiv \pm 1 \mod n\} \leq H_1=\{ x\,|\, x^m \equiv \pm 1 \mod p_i^{\alpha_i} \,\,\forall i\}.$$
Для того, чтобы посчитать индекс $[H_1: H_2]$ заметим, что есть гомоморфизм $H_1 \to \{\pm 1\}\times \dots \times \{\pm 1\} $ ($k$ раз, где $k$ -- число простых в разложении $n$)  заданный формулой $x \to x^m \mod p_i^{\alpha_i}$. По выбору $m$ этот гомоморфизм сюръективен. $H_2$ -- его ядро. Значит индекс $[H_1:H_2]$ есть число элементов в группе $\{\pm 1\}\times \dots \times \{\pm 1\} $, то есть $2^k$. Аналогично считается индекс $[H:H_2]$.


Если в группе $G$ есть нетривиальная нормальная подгруппа $H$, то это позволяет <<упростить>> вашу группу при рассмотрении многих вопросов, перейдя к паре групп $H$ и $G/H$. Если группа $G$ конечная, то рано или поздно этот процесс сойдётся к группам, в которых нет нетривиальных нормальных подгрупп.


\dfn Группа $G$ называется простой, если в $G$ нет нормальных подгрупп отличных от $G$ и $\{e\}$.
\edfn

Таким образом, простые конечные группы -- это такие кирпичики из которых <<собраны>> все другие конечные группы. Большим достижением 20 века стала классификация всех конечных простых групп. Некоторые примеры таких групп мы уже определили, к другим -- приблизились:\\
1) Группа $\mb Z/p$, где $p$ -- простое, является простой группой. Других конечных простых абелевых групп не бывает.\\
2) Группа $A_n$ простая при $n\geq 5$.\\
3) Определим группу $$\PSL_n(K)=\{A \in \GL_n(K)\,|\, \det A=1\}/ \{ \lambda E_n\,|\, \lambda^n=1, \lambda\in K \}.$$
Это простая группа за исключением случаев, когда $K=\mb Z/2$, $n=2$ и  $K=\mb Z/3$, $n=2$.

Дальнейшее смотри в \cite{}


\section{Действие группы на множестве}


В этой лекции мы поговорим о геометрическом взгляде на группы. На группы, как группы симметрий некоторых множеств. Ещё до того как дать определение посмотрим на примеры такого появления групп. Пусть $X$ -- множество. Тогда все элементы в этом множестве равноправны, все их переставляет между собой группа биекций $S_X$. Это группа симметрий множества $X$, когда на нём не задано никакой дополнительной структуры. 

Рассмотрим граф $G$ -- множество вершин которого обозначим за $V$, а множество рёбер за $E$. Тогда группой автоморфизмов графа  $\Aut(G)$ называется подгруппа в группе перестановок вершин $S_V$, элементы которой переводят рёбра в рёбра. Это группа -- группа симметрий множества $V$ с дополнительной структурой -- структурой графа. Однако так же эта группа переставляет между собой рёбра графа, клики внутри графа и т.д.

Рассмотрим другой пример: возьмём обычную плоскость $\mb R^2$ и рассмотрим все биекции $f\colon \mb R^2 \to \mb R^2$ сохраняющие расстояние. То есть, должно быть выполнено $|x-y|=|f(x)-f(y)|$. Множество таких биекций образует подгруппу в $S_{\mb R^2}$, которая обозначается $\Isom(\mb R^2)$ и называется группой изометрий (движений, самосовмещений) плоскости. 

Рассмотрим правильный $n$-угольник c  центром в нуле. Тогда можно рассмотреть подгруппу в группе $\Iso (\mb R^2)$ состоящую из тех преобразований, которые оставляют этот $n$-угольник на месте. Это группа $D_n$.

Попытаемся сформулировать характерные для всех этих примеров черты.


\dfn
Действием группы $G$ на множестве $X$ называется отображение $\cdot \colon G\times X\to X$, удовлетворяющее аксиомам\\
1) $\forall x \in X$  выполнено, что $e\cdot x=x$.\\
2) $\forall x \in X$, $\forall g,h\in G$ выполнено, что $(gh)\cdot x= g\cdot (h\cdot x)$.\\
Если задано действие $G$ на $X$, то $X$ будем называть $G$-множеством. Тот факт, что группа $G$ действует на $X$ будем обозначать как $G \curvearrowright X$. 
\edfn


\exm \\
1) $S_n$ действует на множестве $\{1,\dots,n\}$.\\
2) Группа $G$ действует сама на себе домножениями слева.\\
3) Группа вращений пространства относительно нуля действует на точки пространства.\\
4) Группа $S_n$ действует на парах $\{1,\dots,n\}\times \{1,\dots,n\}$ (и на тройках тоже).\\
5) Группа $D_n$ самосовмещений правильного $n$-угольника действует на вершинах, диагоналях, парах диагоналей и т.д. в этом правильном $n$-угольнике.\\
6) И вообще, если группа $G\curvearrowright X$, то $G \curvearrowright X\times X\times \dots \times X$ , а так же $G \curvearrowright Y^X$, где $Y$ -- произвольное множество. Последнее действие задаётся формулой $g,f \to f(g^{-1}x)$.\\
7) Если $H\leq G$ и $G$ действует на $X$, то и $H$ действует на $X$. Более общо, если задан гомоморфизм $f\colon H \to G$ и $G$ действует на $X$, то и $H$ действует на $X$. Зададим это действие формулой $h,x \to f(h)\cdot x$.\\

Оказывается, что при помощи последней конструкции любое действие любой группы на любом множестве можно свести к действию перестановок на этом множестве.


\thrm
Пусть заданы группа $G$ и множество $X$. Тогда имеет место взаимооднозначное соответствие между различными действиями группы $G$ на $X$ и  гомоморфизмами $G \to S_X$. А именно, для каждого действия $G \curvearrowright X$ построим гомоморфизм, переводящий $g \to T_g$, где $T_g(x)=gx$ -- биекция заданная домножением на $g$. 

Обратно, по гомоморфизму $\psi \colon G \to S_X$ определим действие, такое что $gx=\psi(g)(x)$.
\ethrm
\proof
Пусть дано действие $G$ на $X$. Покажем, что отображение $g \to T_g$ есть гомоморфизм групп $G \to S_X$.  Прежде всего заметим, что $T_g$ биекция, так как у этого отображения есть обратное -- $T_{g^{-1}}$. 
Покажем, что $T_{g_1g_2}=T_{g_1}T_{g_2}$. Применим  левую часть к конкретному элементу $x$. Получаем $$T_{g_1g_2}(x)=g_1g_2x=T_{g_1}(g_2x)=T_{g_1}(T_{g_2}(x)),$$
что и требовалось. Мы уже отмечали, что $\psi$ -- гомоморфизм. Осталось показать, что два эти соответствия между действиями группы $G$ на $X$ и гомоморфизмами $G \to S_X$ взаимно однозначны. 

Построим по действию $G \curvearrowright X$ гомоморфизм $G \to S_X$ и потом обратно действие. Покажем, что получилось исходное действие. посмотрим куда переходит пара $(g,x)$
$$(g,x) \to T_g(x)=gx.$$
Что и требовалось. В другую сторону. Если дан некоторый гомоморфизм $\psi$, проверим, что для всякого элемента $g\in G$ результат гомоморфизма построенного по действию, определённому через $\psi$ такой же, как и у $\psi$. Возьмём $x\in X$ и проверим, что построенные по $g$ перестановки действуют на нём одинаково. 
$$T_g(x)=gx=\psi(g)(x).$$
\endproof

Раньше все группы рассматривались как подгруппы группы перестановок. Покажем, что так всегда можно думать:

\crl[Теорема Кэли] Любая  группа $G$ вкладывается в $S_G$. В частности, если $G$ конечная порядка $|G|=n$, то есть подгруппа  $H \leq S_n$ изоморфная $G$.
\ecrl
\proof Рассмотрим действие $G \curvearrowright G$ сдвигами слева. Это действие задаёт гомоморфизм $G \to S_G$ по правилу $g\to (h \to gh)$. Покажем, что ядро этого гомоморфизма тривиально. Рассмотрим $g\neq e$ и $h=e$. Тогда $e \to g\cdot e=g \neq e$. То есть перестановка, заданная $g$, не тождественная. 
\endproof

Для следующего примера сформулируем простейший геометрический факт.

\fct Изометрия трёхмерного пространства однозначно определяется образами 4-ёх точек не лежащих в одной плоскости.
\efct

Посмотрим, что благодаря нашей теореме можно сказать про группу $G$ всех изометрий, сохраняющих тетраэдр. Для краткости будем называть эту группу группой самосовмещений тетраэдра. 

Заметим, что эта группа действует на вершинах тетраэдра. Это даёт изоморфизм c $S_4$. 


\noindent С понятием действия группы на множестве связаны несколько определений:

\dfn Пусть $G\curvearrowright X$. Подмножество $Y\subseteq X$ называется инвариантным подмножеством относительно этого действия, если для всех $g\in G$ выполнено $g(Y)\subseteq Y$.
\edfn

Если группа действует на множестве $X$ и $Y$ инвариантно, то действие можно ограничить на $Y$. Приведём конструкцию самых маленьких инвариантных множеств


\dfn
Орбита элемента $x$ -- это множество элементов в которые можно попасть из $x$ при помощи действия группы $G$, а именно
$$O_x=G\cdot x:=\{y\in X \,|\, \exists g\in G, \,\, g\cdot x=y\}.$$
\edfn

Орбита является наименьшим возможным инвариантным подмножеством, содержащим точку $x$. Есть ещё одно подмножество, связанное с точкой из $X$. На этот раз в группе $G$.

\rm Отношение <<лежать в одной орбите>> есть отношение эквивалентности. Множество $X$ разбивается в дизъюнктное объединение орбит.
\erm

\exm\\
1) Пусть $G$ это группа поворотов плоскости, оставляющих на месте ноль, а $X$ -- сама плоскость на которой $G$ действует стандартным образом. Тогда орбиты при таком действии есть окружности с центром в нуле и, как исключение, точка $0$.\\
2) Пусть $H$ -- подгруппа $G$. Определим действие $H$ на $G$ по формуле $(h,g) \to gh^{-1}$. Орбиты относительно этого действия  -- это классы смежности относительно $H$.\\
3) Группа $G$ действует на себе сопряжениями $(g,h) \to ghg^{-1}$. Орбиты относительно этого действия есть классы сопряжённости.\\

\dfn
Стабилизатором точки $x$ называется множество элементов группы $G$, оставляющих её на месте, то есть
$$\Stab_x=G_x:= \{g\in G \,|\, g\cdot x=x\}.$$
\edfn

\lm Пусть $G$ действует на множестве $X$. Тогда для любой точки $x\in X$ множество $\Stab_x$ является подгруппой в $G$.
\elm

\rm
Для стабилизатора и орбиты приведены два обозначения. Здесь будут использоваться первые.
\erm








Прежде всего мы установим связь между геометрией  и внутренним строением группы, показав, что  левые смежные классы группы $G$ по стабилизатору точки $x$ соответствуют точкам орбиты $O_x$.

\thrm[О связи орбиты и стабилизатора] Пусть группа $G$ действует на множестве $X$ и задана точка $x \in X$. Тогда для любой точки $y \in O_x$ множество $\{h\in G\,|\, hx=y\}$ является левым смежным классом группы $G$ по стабилизатору $\Stab_x$. Обратно, для любого элемента $h\in g\Stab_x$ верно, что $hx=gx$. В частности, корректно определены и определяют взаимооднозначное соответствие $O_x \leftrightarrow G/\Stab_x$ отображения, заданные как $$y\in O_x \to \{h\in G\,|\, hx=y\}\, \text{ и }\, g\Stab_x \to gx \in O_x.$$
\ethrm
\proof Прежде всего покажем корректность указанных соответствий. Пусть $y\in O_x$ и $g$ такой, что $y=gx$. Покажем, что множество $\{h\in G\,|\, hx=y\}$ элементов группы, переводящих $x \to y$ есть левый смежный класс $g\Stab_x$. 

Действительно, если элемент $h$ лежит в $g\Stab_x$, то $h=gp$, где $p \in\Stab_x$ и, следовательно $hx=gpx=gx=y$. Обратно, если есть элемент $h$, что $hx=y$, то тогда $g^{-1}hx=g^{-1}y=x$. Отсюда $g^{-1}h \in\Stab_x$ и следовательно $h=g(g^{-1}h) \in g\Stab_x$. Это и доказывает равенство $\{h\in G\,|\, hx=y\}=g\Stab_x$.


Перейдём к соответствию делающему из смежного класса элемент орбиты. Пусть класс имеет вид $g\Stab_x$. Тогда сопоставим ему $y=gx \in O_x$. Здесь тоже есть проблема с корректностью. Дело в том, что в качестве элемента $g$ может быть взять какой-то другой элемент смежного класса и получиться, априори, другой элемент орбиты. Выберем другой элемент $h=gp\in g\Stab_x$ и  покажем, что $hx=gx$. Действительно $hx=gpx=gx$ так как $p\in\Stab_x$. Итого, обратное соответствие корректно задано

Теперь должна быть проведена небольшая, но необходимая проверка, что оба соответствия действительно взаимно обратны. 
Пусть мы стартовали с элемента $y\in O_x$, ему соответствует класс $g\Stab_x$, где $y=gx$. В свою очередь этому классу соответствует элемент $gx=y$. Что и требовалось. 

В другую сторону. Пусть мы взяли смежный класс $g\Stab_x$. Ему соответствует элемент $y=gx$. Этому элементу $y$ соответствует класс $\{h\in G\,|\, hx=y\}$. Заметим, что $g$ лежит в этом классе. Из свойства, что если классы пересекаются, то они совпадают, следует, что это и есть $g\Stab_x$.
\endproof

\crl
Пусть $G$ -- конечная группа, действующая на множестве $X$. Если задан элемент $x\in X$, то $$|G|=|O_x||\Stab_x|.$$
\ecrl


Пусть группа $G$ действует на множестве $X$ и вы знаете, что $G$ порождена элементами $g_1,\dots,g_n$. Как найти орбиту элемента $x \in X$ в такой ситуации?

Ответ ожидаемый: надо подействовать на $x$ образующими $g_1\dots,g_n$, то есть найти элементы $y_1=g_1x,\dots, y_n=g_n x$, потом применить образующие к новым получившимся элементам $y_1,\dots, y_n$, и так далее, пока на некотором шаге  новые элементы перестанут появляться. Все элементы $X$, что появились к этому моменту и составляют  $O_x$.

Посмотрим, как это работает на простеньком примере:

\upr Пусть $H$ подгруппа $S_4$, порождённая перестановками $(123)$ и $(13)(24)$. Тогда $H=A_4$.
\eupr
\proof[Решение]
Действительно, порядок $H$ должен делиться на порядок её элемента $(123)$, который равен 3. Осталось показать, что $|H|\di 4$. Для этого посчитаем орбиту элемента 1. Если мы применим перестановку $(123)$ несколько раз, то получим, что в орбите 1 лежат 2 и 3. Но при действии перестановкой $(13)(24)$ из $2$ получается $4$-ка, которая по транзитивности тоже лежит в орбите $1$. Так как больше элементов в множестве $\{1,2,3,4\}$ нет, то получаем, что $O_1=\{1,2,3,4\}$. Так как порядок орбиты делит порядок группы получаем, что число элементов в $H$ делится на 4. Итого, порядок $H$ делится на 12. Но $H$ и так лежит в группе из $12$ элементов -- в $A_4$. Значит $H=A_4$.
\endproof

Так же получаем ещё одно тривиальное следствие:

\crl Пусть $G$ -- конечная группа. Тогда количество элементов в любом классе сопряжённости делит $|G|$. 
\ecrl



\subsection{Теорема Коши}

Разберёмся со старым долгом -- частичным обращением теоремы Лагранжа:

\thrm[Коши] Пусть $G$ конечная группа порядок которой делится на простое число $p$. Тогда в группе $G$ есть элемент порядка $p$.
\ethrm
\proof Рассмотрим декартово произведение $G^p$. На этом декартовом произведении действует группа $\mb Z/p$ отображающая упорядоченную $p$-шку в её циклический сдвиг 
$$k,(g_0,\dots,g_{p-1})\to (g_k,g_{k+1},\dots,g_{p-1},g_0,\dots,g_{k-1}).$$
Заметим, что множество $$Y=\{(g_0,\dots,g_{p-1})\,|\, g_0\dots g_{p-1}=e\}$$
является инвариантным относительно действия $\mb Z/p$. Действительно, если $g_0\dots g_{p-1}=e$, то $g_1\dots g_{p-1}=g_0^{-1}$ и, следовательно $g_1\dots g_{p-1} g_0=e$. Это значит, что сдвиг на 1 переводит $Y$ в себя, а значит так поступают и все остальные сдвиги.

Если $(g_1,\dots,g_p)\in Y$ неподвижная точка относительно этого действия, то $g_1=\dots=g_p=g$, где $g^p=e$. То есть либо $g$ элемент порядка $p$ либо $g=e$. Осталось показать, что есть неподвижная точка не соответствующая $g=e$.

Действительно, заметим, что все орбиты относительно $\mb Z/p$ действия состоят либо из $1$ элемента, либо из $p$ элементов. Всего в $Y$ ровно $|G|^{p-1}\di p$ элементов. Значит число неподвижных точек делится на $p$. В частности, их либо нет, либо их больше $2$. Но уже есть одна неподвижная точка при $g=e$. Значит есть ещё одна. Значит есть  элемент порядка $p$.
\endproof


\section{Алгоритм Шрайера-Симса}

Где может быть использовано вычисление орбиты? Представим себе, что наша подгруппа $H \leq S_n$ задана образующими $H=\lan g_1,\dots,g_k\ran$. Хочется вычислить, например порядок $H$. Мы знаем, что порядок $|H|=|\Stab_1|\,|O_1|$. Как мы отметили, найти орбиту легко. Осталось найти порядок стабилизатора $1$ относительно действия $H$. $\Stab_1$ -- это подгруппа в $S_{n-1}$ так как она никуда не перемещает $1$. Тут бы и сказать, что можно воспользоваться индукционными соображениями, но есть одна загвоздка -- мы не знаем образующие стабилизатора.

Нахождение образующих стабилизатора не совсем простая вещь: мы знаем, что если $H=\lan (12),\dots,(1n)\ran$, то $H=S_n$ и $\Stab_1=S_{n-1}$ (действующая на точках от $2$ до $n$). Но! Ни одна из образующих $H$ не лежит в стабилизаторе! Однако все эти сложности можно преодолеть. Сформулируем соответствующий результат  в общем контексте действия группы на множестве.

\thrm[Лемма Шрайера] Пусть группа $G=\lan S\ran$ действует на множестве $X$.  Пусть дан $x\in X$ и для каждого элемента $y\in O_x$ задан $h_y \in G$, что $h_y x=y$. Также потребуем, чтобы для $y=x$ было $h_x=e$. Тогда 
$$\Stab_x =\lan h_{(sy)}^{-1}s h_y\ran \text{ по  всем $y\in O_x$ и $s\in S$.} $$
\ethrm
\proof Прежде всего отметим, что $h_{(sy)}^{-1}s h_y$ лежат в стабилизаторе $x$. Далее, в указанных предположениях любой элемент $g\in G$ есть произведение образующих $g=s_k^{\eps_k}\dots s_1^{\eps_1}$. Пусть $g\in \Stab_x$, то есть $gx=x$. Предположим, что все $\eps_i =1$ (так можно считать, если группа конечная, потом мы разберёмся с этим случаем). Посмотрим куда переходит элемент $x$ под действием $s_1$ и далее. Нарисуем диаграмму

\begin{center}
\begin{tikzpicture}
\node (a1) at (-2,0) {$x$};
\node (a2) at (1,0) {$x_1$};
\node (a3) at (4,0) {$x_2$};
\node (a4) at (7,0) {$x_{k-1}$};
\node (a5) at (10,0) {$x=x_k$};


\node (b2) at (1,-2) {$x$};
\node (b3) at (4,-2) {$x$};
\node (b4) at (7,-2) {$x$};


\draw[->, cyan ] (a1) -- node[above]{\color{black} $s_1$} (a2);
\draw[->, cyan ] (a2) -- node[above]{\color{black} $s_2$} (a3);
\draw[->, cyan ] (a4) -- node[above]{\color{black} $s_k$} (a5);
\draw[->, cyan ] (a3) -- node[above]{\color{black} $\dots$} (a4);

\draw[->, cyan] (a2) to[bend right] (b2);
\node (c1) at ($0.5*(a2)+0.5*(b2)-(0.7,0)$) {\color{black} $h^{-1}_{x_1}$};

\draw[->, cyan] (a3) to[bend right] (b3);
\node (c1) at ($0.5*(a3)+0.5*(b3)-(0.7,0)$) {\color{black} $h^{-1}_{x_2}$};

\draw[->, cyan] (a4) to[bend right] (b4);
\node (c1) at ($0.5*(a4)+0.5*(b4)-(0.8,0)$) {\color{black} $h^{-1}_{x_{k-1}}$};



\draw[<-, cyan] (a2) to[bend left] (b2);
\node (c1) at ($0.5*(a2)+0.5*(b2)+(0.7,0)$) {\color{black} $h_{x_1}$};

\draw[<-, cyan] (a3) to[bend left] (b3);
\node (c1) at ($0.5*(a3)+0.5*(b3)+(0.7,0)$) {\color{black} $h_{x_2}$};

\draw[<-, cyan] (a4) to[bend left] (b4);
\node (c1) at ($0.5*(a4)+0.5*(b4)+(0.8,0)$) {\color{black} $h_{x_{k-1}}$};

\draw[->,cyan] (a1) to[bend left] (a5);
\node (dd) at ($0.5*(a1)+0.5*(a5)+(0,1)$) {$g$} ;


\draw[->,cyan] (b2) to[bend left] (b3);
\node (dd1) at ($0.5*(b2)+0.5*(b3)$) {$h_{x_2}^{-1}s_2 h_{x_1}$};


\draw[->,cyan] (b3) to[bend left] (b4);

\end{tikzpicture}
\end{center}
Здесь $x_i=s_i x_{i-1}$. Произведение $h_{x_i}^{-1}s_{i-1}h_{x_{i-1}}$ является одной из образующих (если взять $y=x_{i-1}$). Видно, что $$g=s_kh_{x_{k-1}}h_{x_{k-1}}^{-1}\dots h_{x_2} (h_{x_2}^{-1}s_2 h_{x_1})h_{x_1}^{-1}s_1.$$
Осталось заметить, что $h_{x_1}^{-1}s_1=h_{x_1}^{-1}s_1h_x$ и $s_k h_{k-1}=h_x^{-1}s_k h_{x_{k-1}}$ благодаря тому, что $h_x=e$. Значит все элементы из $\Stab_x$ разложились в виде произведения потенциальных образующих. 

Если же где в последовательности встретился $\eps^{i}=-1$, то в качестве указанного тройного произведения возникнет $h^{-1}_{s_i^{-1}y} s_i^{-1} h_y$. Осталось заметить, что этот элемент обратный к образующей $h^{-1}_{s_iz}s_i h_z$, где $z=s_i^{-1}y$.

\endproof



Лемма Шрайера даёт возможность посчитать практически всё про группу $H \leq S_n$, начиная с порядка. Однако напрямую применять её не стоит. Дело в том, что если изначально в группе было $k$ образующих, то лемма выдаст в качестве образующих стабилизатора вплоть до $nk$ элементов. Если ничего не менять, то продолжая пользоваться леммой дальше,  получим, что множество образующих может очень разрастись. 

Покажем, однако, что в группе $G\leq S_n$ всегда порождена небольшим множеством.


\thrm Пусть $G\leq S_n$. Тогда существует существует набор образующих из не более чем $\frac{n(n-1)}{2}$ элементов.
\proof  При $n=2$ это очевидно. Переход. Посмотрим на $\Stab_{1}^G \leq S_{n-1}$. По индукционному предположению у этой подгруппы есть множество из $(n-1)(n-2)/2$ образующих. Достроим его до образующих $g$. Для этого для каждого $y\in O_{1},\, y\neq 1$ рассмотрим элемент $h_y\in G$, что $h_y(1)=y$. Таких $h_y$ не более $n-1$ штук.

Пусть теперь $\sigma \in G$. Если $\sigma \in \Stab_{1}$, то он выражается через образующие стабилизатора. Иначе рассмотрим произведение $h_{\sigma(1)}^{-1}\sigma$. Оно лежит в $\Stab_{1}$. Но тогда $\sigma$ выражается через образующие $\Stab_{1}$ и элементы вида $h_y$. В сумме их не более чем 
$$n-1+\frac{(n-1)(n-2)}{2}=\frac{n(n-1)}{2}.$$
\endproof
\ethrm

В указанной конструкции, мы получили набор образующих, среди которых есть образующие некоторого элемента (1 в данной конструкции), пары элементов и т.д. Однако, напрямую воспользоваться соображениями из теоремы нельзя, так как мы не знаем, как задать стабилизатор. Идея состоит в том, чтобы скрестить оба подходы -- беря постепенно образующие из леммы Шрайера для стабилизатора одной точки мы будем постепенно строить образующие для стабилизаторов некоторого набора точек, что  в итоге даст нам небольшой и удобный набор образующих.


\dfn[База] Пусть группа $G$ действует на множестве $X$. Назовём набор элементов $(b_1,\dots,b_k)$ из $X$ базой, если для любого $ g\in G$, если $gb_i=b_i$ для всех $i$, то $g=e$.
\edfn

Таким образом, действие элемента группы $G$ определяется его действием на базе. Не для любых действий группы на множестве вообще может существовать база: дело в том, что вообще говоря, элемент $g\in G$ не обязан определяться тем, как он действует на $X$, то есть отображением $T_g\in S_X$. Иными словами, гомоморфизм $G\to S_X$ в этом случае не инъективен. 

Всюду далее мы будем предполагать, что база есть. В этом случае указанный гомоморфизм инъективен, и можно считать, что $G$ подгруппа в $S_X$. То есть, если $X$ конечна, то $G$ можно считать подгруппой в $S_n$. 

Следующая конструкция позволяет удобно хранить хранить необходимую информацию об орбите конкретного элемента относительно действия данной группы и получать элементы $h_y$ из леммы Шрайера


\dfn[Дерево Шрайера] Пусть $G$ группа с конечным множеством образующих $S$ действует на множестве $X$. Деревом Шрайера для элемента $x\in X$ относительно множества образующих $S$ называется дерево (с корнем, направление рёбер -- к корню), вершины которого соответствуют элементам орбиты $O_x$ (сам $x$ -- корень), а на рёбрах стоят пометки из элементов $S$ таким образом, что есть ребро из $u$ в $v$ с меткой $s$, если $su=v$.
\edfn

Если дана система образующих $S$ для подгруппы $H$ в $S_n$ и некоторый элемент $x\in \{1,\dots,n\}$, то легко посчитать соответствующее дерево Шрайера для $x$ относительно $S$. Для этого надо, стартуя с элемента $x$, последовательно применять образующие из $S$. На каждом шаге будут появляться элементы орбиты $x$. продолжать это нужно до тех пор, пока новые подстановки не приведут к  полученным  ранее элементам орбиты $x$ и перебраны все ранее полученные элементы орбиты и к ним применены все образующие.

\dfn[Полная цепочка стабилизаторов] Пусть $G$ действует на множестве $X$, дана база $B=(b_1,\dots,b_k)$. Полной цепочкой стабилизаторов в $G$ относительно базы $B$ будем называть цепочку подгрупп 
$$G=G_0\geq G_1 \geq \dots \geq G_k=\{e\},$$
обладающую следующими свойствами:
\enm
\item имеет место равенство $G_{i+1}=\Stab_{b_{i+1}}^{G_i}$, при $0\leq i\leq k-1$.
\item группы $G_i$ заданы при помощи множества образующих $S_i$.
\item Для каждого $i\geq 0$ задано $T_i$ -- дерево Шрайера для $b_{i+1}$ относительно $S_i$.
\item Мы будем дополнительно предполагать невырожденность, то есть что $G_i\neq G_{i+1}$.
\eenm
\edfn

Группы $G_i$ в определении полной цепочки стабилизаторов можно определить как
$$G_{i+1} = \Stab_{(b_1,\dots,b_i)}^G.$$
Следуя доказательству леммы про небольшое число образующих, можно заметить, что данные, записанные в полной цепочке стабилизаторов дают возможность построить небольшую систему образующих для $G$.

\rm Для каждого $i$ возьмём подмножество тех элементов из $S_i$, которые встречаются в $T_i$. Объединим все эти множества. Получится множество образующих $G$. В нём не более чем $n(n-1)/2$ элементов (так как рёбер во всех деревьях $T_i$ ровно столько). 
\erm

Это множество образующих обладает ещё одним замечательным свойством:

\dfn[Сильное порождающее множество] Пусть группа $G$ действует на множестве $X$ и задана база $B=(b_1,\dots,b_k)$. Тогда множество $S$ называется сильным порождающим множеством относительно $B$, если 
$$S\cap G_i \text{ -- даёт образующие } G_i.$$ 
\edfn

Полная цепочка стабилизаторов (или просто сильное порождающее множество) позволяют легко решить задачу принадлежности элемента группы. Действительно -- пусть $G$ подгруппа $S_n$ и задана перестановка $\sigma$. Опишем процедуру проверки принадлежности $\sigma$ группе $G$, если задана полная система стабилизаторов для $G$.  Для индукционных рассуждений заметим, что из полной системы стабилизаторов для $G$ можно получить полную систему для всех $G_i$ просто отбросив начальный кусок. Будем сводить проверку $\sigma \in G_i$ к проверке $\sigma'\in G_{i+1}$ для некоторого элемента $\sigma_{new}$:
\enm
\item Рассмотрим натуральное число $i$ -- номер шага. Положим вначале $i=0$. Будем предполагать, что на $i$ шаге $\sigma$ оставляет на месте элементы $b_1,\dots,b_{i-1}$. Значит $\sigma \in G$ тогда и только тогда, когда $\sigma \in G_i$.
\item Рассмотрим $u=\sigma(b_i)$. Если $u\notin O_{b_i}^{G_i}$, то $\sigma \notin G_i$ и значит не из $G$.
\item Иначе, если $u\in O_{b_i}^{G_i}$, то по дереву Шрайера можно найти элемент $h_u\in G$, что $h_u u=b_i$. Тогда $\sigma \in G_i$ в том и только том случае, когда $\sigma'=h_u\sigma \in G_{i+1}$.
\item При $i=k$ проверка тривиальна.
\eenm

Посмотрим теперь, какие данные нам нужно будет поддерживать в процессе работы алгоритма по нахождению полной цепочки стабилизаторов:

\dfn[Частичная цепочка стабилизаторов]
Пусть $G$ действует на множестве $X$, набор точек $B=(b_1,\dots,b_k)$. Частичной цепочкой стабилизаторов в $G$ относительно набора точек $B$ будем называть цепочку подгрупп 
$$G=G_0\geq G_1 \geq \dots \geq G_k=\{e\},$$
обладающую следующими свойствами:
\enm
\item имеет место включение $G_{i+1}\leq \Stab_{b_{i+1}}^{G_i}$, при $0\leq i\leq k-1$.
\item группы $G_i$ заданы при помощи множества образующих $S_i$.
\item для каждого $i\geq 0$ задано $T_i$ -- дерево Шрайера для $b_{i+1}$ относительно $S_i$.
\eenm
\edfn

Как проверить, что частичная цепочка стабилизаторов -- полная? Для этого при каждом $0\leq i\leq k-1$ надо проверить, что образующие Шрайера для $G_{i+1}$ относительно $S_i$ лежат в $G_i$. Если цепочка полная, то все эти проверки выполнимы. На этом основана работа алгоритма Шрайера-Симса: на каждом этапе мы стараемся проверить, что некоторая образующая Шрайера лежит в соответствующей подгруппе. Мы либо выполняем эту проверку и переходим к следующей, либо добавляем недостающий элемент в базу, либо уменьшаем индекс $G_{i+1}$ в $\Stab_{b_{i+1}}^{G_i}$ для некоторого $i$. 

Итак, пусть $G$ задана при помощи семейства образующих $S$. Будем считать, что $X=\{1,\dots,n\}$. На самом деле нам нужен только порядок на $X$. Будем говорить, что частичная цепочка стабилизаторов корректна после уровня $i$, если $G_{t+1}=\Stab_{b_{t+1}}^{G_t}$ при $t> i$. Посмотрим на алгоритм:

\enm
\item стартовые данные: $G=G_0 \geq G_1=\{e\}$, $b_1=1$, $B=(b_1)$, $G_0=\lan S\ran$, дерево $T_1$ можно вычислить по $S$  и $b_1$. Цепочка корректна после уровня $i=1$. Сгенерируем по дереву $T_1$ образующие Шрайера для $\Stab_{b_1}$. Возьмём первую образующую.
\item Пусть дана частичная цепочка стабилизаторов $G_0\geq \dots\geq G_k$, для  $B=(b_1,\dots,b_k)$ корректная после уровня $i$ и дана некоторая образующая Шрайера $h\in G_i$ для $\Stab_{b_{i+1}}^{G_i}$. Запустим проверку принадлежности $G_{i+1}$ для $h$. 
\item Если $h\in G_{i+1}$, то его можно не включать в систему образующих для $G_{i+1}$ и надо перейти к следующей образующей Шрайера. Если следующей образующей нет, то надо сказать, что цепочка корректна после уровня $i-1$ и взять образующую Шрайера с уровня $i-1$ (или с большего уровня, если выкинет выше).
\item Если $h\notin G_{i+1}$, то алгоритм проверки может выкинуть какой-то номер $t$, что некий элемент $h'$ из стабилизатора $\Stab_{b_1,\dots,b_{t-1}}^G$, полученный по $h$ и уже известным образующим переводит элемент $b_t$ в элемент не из дерева $T_{t-1}$. В этом случае надо пересчитать дерево $T_{t-1}$, добавить $h'$ к образующим $G_{t-1},\dots,G_{i+1}$, сказать, что цепочка корректна после уровня $t-1$ и начать проверку образующей Шрайера для $\Stab_{b_t}^{G_{t-1}}$. 
\item Но бывает так, что $u=h'(b_k)\neq b_k$. Тогда надо взять новый элемент базы $b_{k+1}=u$, добавить $h'$ к образующим $G_k,\dots,G_{i+1}$, пересчитать дерево $T_k$, и взять $G_{k+1}=\{e\}$. Цепочка будет корректна после уровня $k$.
\eenm


Подробнее смотри в \cite{PGA}. Кроме проверки элемента полная цепочка стабилизаторов позволяет решить задачу о нахождении порядка группы $G$:


\crl[Вычисление порядка] Пусть дана полная цепочка стабилизаторов: $G=G_0\geq G_1\geq \dots \geq G_k=\{e\}$. Тогда $|G|=\prod_{i=0}^{k-1} |T_i|$. 
\ecrl




\section{Лемма Бернсайда}

Довольно часто в том или ином контексте возникает задача о подсчёте комбинаторных объектов. Базовый пример здесь число $C_{n}^k$, то есть число $k$ элементных подмножеств. Для того, чтобы задать $k$ элементное множество нужно выписать подряд $k$ различных элементов. Однако несколько таких записей часто задают один и тот же объект. Например, разные упорядоченные наборы $(1,2,3)=(2,1,3)=\dots$ задают одно и тоже множество. Какие записи мы отождествляем? Те, в которых элементы отличаются перестановкой, то есть лежат в орбите действия симметрической группы $S_k$. Всего упорядоченных $k$ элементных наборов $n(n-1)\dots(n-k+1)$. Орбита каждого набора состоит из $k!$ наборов -- при каждой перестановке получается новый набор. Итого число подмножеств есть отношение $C_n^k=\frac{n(n-1)\dots(n-k+1)}{k!}$, что вам хорошо известно. 



Приведём ещё одну похожую постановку задачи. Представим себе разноцветные бусы из $k$ бусин, которые могут быть раскрашены в $n$ возможных цветов. Совершенно ясно, что бусы задаются последовательностью бусин. Но две разные последовательности могут соответствовать одним бусам. Это можно понять следующим образом -- расставим бусины в вершины правильного $k$-угольника. Тогда любое самосовмещение правильного $k$-угольника меняет расположение бусин, но при этом сами бусы не меняются. Итого,  бусы однозначно соответствуют раскраскам вершин правильного $k$-угольника с точностью до самосовмещений этого $k$-угольника, то есть орбитам действия группы $D_k$ на множестве раскрасок вершин этого $k$-угольника в  $n$ цветов.

Эта задача сложнее предыдущей -- в задаче про $k$-элементные подмножества в каждой орбите было одинаковое число элементов и это мгновенно приводило к ответу. 

Здесь же более симметричные раскраски имеют меньшую орбиту. Попробуем проконтролировать это. Прежде всего введём обозначения.

\dfn Пусть группа $G$ действует на множестве $X$. Множество всех орбит относительно этого действия будем обозначать $X/G$.
\edfn 

\dfn Пусть так же задан элемент $g\in G$. Тогда обозначим за $\Fix(g)$ -- множество неподвижных точек элемента $g$ то есть 
$$\Fix(g)=\{x \in X\,|\, gx=x\}.$$
\edfn

Теперь мы можем разобрать основной технологический момент, который упрощает нахождение числа орбит:

\thrm[Лемма Бернсайда]
Пусть конечная группа $G$ действует на конечном множестве $X$. Тогда справедливо равенство
$$|X/G|=\frac{1}{|G|}\sum_{g\in G}|\Fix(g)|.$$
\ethrm
\proof Докажем нужное равенство подсчитав двумя способами число элементов в множестве $$\{(x,g) \,|\, x\in X,\, g\in G \text{ и } gx=x\}.$$
При каждом конкретном $g\in G$, число элементов $x$, образующих с ним пару равно $|\Fix(g)|$. C другой стороны имеем, что для каждого $x\in X$ число элементов в паре с ним равно $|\Stab_x|$. Итого $$\sum_{g\in G}|\Fix(g)|= \sum_{x\in X} |\Stab_x|.$$
Теперь вспомним, что $|\Stab_x|=|G|/|O_x|$. После этого замечания разобьём сумму по отдельным орбитам. В каждой орбите величина $|O_x|$ одинакова. Итого 
$$\sum_{g\in G}|\Fix(g)|= \sum_{x\in X} |\Stab_x|= |G|\sum_{O\in X/G}\,\, \sum_{x\in O} \frac{1}{|O|}=|G|\sum_{O\in X/G} 1=|G||X/G|.$$
Здесь мы заметили, что сумма $|O|$ раз величины $\frac{1}{|O|}$ равна единице. Для завершения доказательства осталось только поделить на $|G|$.
\endproof

Применим лемму Бернсайда для подсчёта числа различных бус. Для простоты ограничимся четырьмя бусинами, но не будем ограничивать число цветов. 

Напомню, что группа $D_n$ состоит из $2n$ элементов. $n$ из них -- это повороты на угол кратный $\frac{2\pi}{n}$. Ещё есть $n$ различных симметрий относительно прямых. В случае чётного $n$ половина из этих прямых проходит  через пары противоположенных вершин, а половина проходит через середины противоположенных рёбер. 

Итого в нашем случае имеем $|D_4|=8$. Переберём все элементы $D_4$. Для начала заметим, что все раскраски неподвижны относительно тождественного преобразования. Итого $|\Fix(\id)|=n^4$. Рассмотрим симметрию, проходящую через середины противоположенных сторон. Пары вершин на каждой из этих сторон должны быть раскрашены одинаково. Итого $n^2$ инвариантных раскрасок. Если же симметрии проходят через противоположенные вершины, то инвариантных раскрасок $n^3$. 

Теперь рассмотрим повороты. Инвариантность относительно поворота на 90  и 270 градусов означают, что все вершины должны быть покрашены одинаково. Итого на каждый по $n$ раскрасок. Поворот на 180 градусов даёт $n^2$ раскрасок. Итого
$$\frac{1}{8}(n^4+2n^2+2n^3+2n+n^2)=\frac{1}{8}(n^4+2n^3+3n^2+2n)$$
различных бус из 4 бусин для $n$ возможных цветов.


\chapter{Линейные операторы}



\section{Определитель}

Есть ли какая-нибудь численная характеристика, которая позволяет сказать, что матрица $A\in M_n(K)$ обратима? Для этого вспомним, что обратимость матрицы $A$ равносильна линейной независимости её столбцов. Если эти столбцы представить себе как вектора в пространстве $\mb R^n$, например, при $n=3$ в обычном трёхмерном пространстве, то видно, что вектора линейно независимы тогда и только тогда, когда объём параллелепипеда на них натянутого отличен от нуля. Конечно, в случае пространства, размерности больше трёх понятие параллелепипеда нужно уточнить:

\dfn Пусть $V$ -- векторное пространство размерности $n$ над $\mb R$, тогда для набора  $v_1,\dots,v_n \in V$ определим параллелепипед
$$D(v_1,\dots,v_n)=\left\{\sum_{i=1}^n \lambda_i v_i\,|\, \text{ где } \lambda_i\in [0,1]\right\}.$$
\edfn


Обозначим для краткости $\Vol(v_1,\dots,v_n)= \Vol (D(v_1,\dots,v_n))$. 

Попробуем понять есть ли возможность как-то аксиоматизировать понятие объёма параллелепипеда на пространстве $V=\mb R^n$, так, чтобы его можно было перенести на произвольное пространство над каким угодно полем. Мне будет удобно набор из $n$  столбцов из $\mb R^n$ объединять в матрицу. Итак, отображение объёма $\Vol M_n(\mb R) \to \mb R$ должно удовлетворять следующим свойствам:
\enm 
\setcounter{enumi}{-1}
\item Объём единичного кубика, единичен. То есть $\Vol(E_n)=1$.
\item При растяжении одного вектора объём меняется пропорционально $\Vol(\dots,\lambda v,\dots)=|\lambda|\Vol(\dots,v,\dots)$.
\item Исходя из принципа Кавальери $\Vol(\dots,v,\dots,u,\dots)=\Vol(\dots,v,\dots,u+\lambda v,\dots)$.
\item Если в наборе есть два одинаковых вектора, то $\Vol(\dots,v, \dots, v,\dots)=0$.
\eenm

Перейдём теперь к ситуации над произвольным полем $K$.
Для общего пространства $V$ условие типа 0) не имеет смысла, так как нет возможности выбрать какой-то канонический базис в $V$. Безусловно, модуль числа в свойстве 1) нет возможности определить над произвольным полем так как  возникают сложности с понятием положительности (особенно в конечных полях). Таким образом, заменой свойства 1) над произвольным полем стоит считать
\enm
\item[1')] $\omega(\dots,\lambda v,\dots)=\lambda \omega(\dots,v,\dots)$.
\eenm
Заметим, что если мы находимся над $\mb R$ и  для отображения $\omega \colon V \times \dots \times V \to \mb R$ выполнено свойство  1'), то для отображения $|\omega|$ выполнены свойства 1). 

Наконец, с алгебраической точки зрения свойство 2) означает независимость относительно элементарных преобразований столбцов матрицы. Это не самое удобное условие. Вместо него мы рассмотрим, как кажется, более сильное свойство
\enm \item[2')] $\omega(\dots,u+v,\dots)=\omega(\dots,u,\dots)+\omega(\dots,v,\dots)$.
\eenm
Действительно из свойства 1'), 2') и 3) следует свойство 2): $$\Vol(\dots,v,\dots,u+\lambda v,\dots)= \Vol(\dots,v,\dots,u,\dots)+\lambda \Vol(\dots,v,\dots,v,\dots)=\Vol(\dots,v,\dots,u,\dots).$$
Свойство 3) имеет смысл всегда и менять его, видимо не следует. Давайте немного покрутимся около этих свойств и посмотрим, что нам даёт их наличие.


\dfn[Общее определение полилинейности] Пусть $U_1,\dots,U_l, V$ -- векторные пространства над полем $K$. Отображение $\omega \colon  U_1\times \dots \times U_l\to V $ называется полилинейным, если
$$\omega(v_1,\dots,v_i+\lambda u_i,\dots, v_l)= \omega(v_1,\dots,v_i,\dots, v_l)+\lambda\omega(v_1,\dots,u_i,\dots, v_l).$$
Множество всех полилинейных отображений будем обозначать как $\Hom_K(U_1,\dots,U_l;V)$. Здесь одновременно зашифрованы свойства типа 1')  и  2').
\edfn


\dfn[Форма]
Полилинейное отображение $\omega \colon  V^l\to K $ называется полилинейной формой степени $l$ на $V$.
\edfn

\rm Вообще, формой принято называть любое отображение из векторного пространства в базовое поле.
\erm


\dfn
Полилинейная форма  $\omega \colon V^l\to K$ на пространстве $V$ над полем $K$ называется:
\enm 
\item антисимметричной или кососимметричной, если $\omega(v_1,\dots,v,\dots,v,\dots, v_l)=0$.
\item симметричной, если $\omega(v_1,\dots,v_i,\dots,v_j,\dots, v_l)=\omega(v_1,\dots,v_j,\dots,v_i,\dots, v_l)$
\eenm
\edfn



Теперь стоит  отметить основные свойства, характерные для полилинейных форм. 

\lm Пусть $V$ -- пространство размерности $n$. Для  полилинейного отображения $\omega \colon V^l \to K $ и любого $e_1,\dots,e_n$ -- базиса $V$  выполнено, что
$$\omega(v_1,\dots,v_l)=\sum_{1\leq i_1,\dots,i_l\leq n}\omega(e_{i_1},\dots,e_{i_l})\prod_{j=1}^l a_{i_j,j}, \text{ где $a_{ij}$ -- это $i$-ая координата вектора $v_j$ в базисе $e$.}$$ 
\elm
\proof По условию $v_j=\sum_{i=1}^n a_{ij}e_i$. Тогда $$\omega(v_1,\dots,v_l)=\sum_{i_1=1}^n a_{i_1,1}\omega(e_{i_1},v_2,\dots,v_l)= \dots = \sum_{1\leq i_1,\dots,i_l\leq n}\omega(e_{i_1},\dots,e_{i_l})\prod_{j=1}^l a_{i_j,j},$$
\endproof

Предыдущую лемму можно доказать и в более общей форме -- для произвольных полилинейных отображений. Но сейчас нам это не нужно. Посмотрим, какие дополнительные ограничения накладывает условие кососимметричности. 


\lm Пусть $V$ -- пространство размерности $n$. Для  полилинейного отображения $\omega \colon V^l \to K $ выполнено: \\
1) Если $\omega$ кососимметрично, то $\omega(\dots,u,\dots,v,\dots)= -\omega(\dots,v,\dots,u,\dots)$.\\
2) В случае, если $\chr K \neq 2$, то из  заключения пункта 1), следует кососимметричность.\\
3) Если $\omega$ кососимметрична, то для любой перестановки $\sigma \in S_l$ выполнено, что $\omega(v_{\sigma(1)},\dots,v_{\sigma(l)})= \sgn(\sigma) \omega(v_1,\dots,v_n)$.\\
4) В предположении кососимметричности $\omega(\dots,v,\dots,u,\dots)=\omega(\dots,v,\dots,u+\lambda v,\dots)$.\\
5) В предположении кососимметричности и условия $l=n$ для набора векторов $v_1,\dots,v_n$ и базиса $e_1,\dots,e_n$ выполнено
$$\omega(v_1,\dots,v_n)=\omega(e_1,\dots,e_n)\sum_{\sigma \in S_n} \sgn(\sigma)\prod_{j=1}^n a_{\sigma(j),j}=\omega(e_1,\dots,e_n)\sum_{\sigma \in S_n} \sgn(\sigma)\prod_{i=1}^n a_{i,\sigma(i)},$$
где $a_{i,j}$ -- это $i$-ая координата $j$-ого вектора в базисе $e_1,\dots, e_n$.
\proof Докажем первое утверждение. Распишем 
\begin{align*} 
0&=\omega(\dots,u+v,\dots,u+v,\dots)=\\&=\omega(\dots,u,\dots,u,\dots)+\omega(\dots,v,\dots,u,\dots)+\omega(\dots,u,\dots,v,\dots)+\omega(\dots,v,\dots,v,\dots)=\\ &=\omega(\dots,v,\dots,u,\dots)+\omega(\dots,u,\dots,v,\dots),
\end{align*}
что и доказывает утверждение. Для доказательства второго утверждения предположим, что $\chr K\neq 2$. Тогда, переставляя одинаковые вектора в выражении $\omega(\dots,v,\dots,v,\dots)$ получаем
$$\omega(\dots,v,\dots,v,\dots)=-\omega(\dots,v,\dots,v,\dots).$$
То есть $2\omega(\dots,v,\dots,v,\dots)=0$. Осталось поделить на 2. покажем третий пункт. По определению знака перестановки
$$\sgn(\sigma)=(-1)^{k}, \text{ где $k$ -- число транспозиций в разложении $\sigma$}.$$
Откуда получаем, что применить $\sigma$ это тоже самое, что применить $k$ транспозиций, то есть изменить знак $k$ раз, что и требовалось. Докажем пункт 4)
$$\omega(\dots,v,\dots,u+\lambda v,\dots)=\omega(\dots,v,\dots,u,\dots)+\lambda\omega(\dots,v,\dots, v,\dots)=\omega(\dots,v,\dots,u,\dots).$$
Для доказательства свойства 5) воспользуемся общим выражением для полилинейной формы из предыдущей леммы
$$\omega(v_1,\dots,v_n)=\sum_{i_1,\dots,i_n} \omega(e_{i_1},\dots,e_{i_n}) \prod_{j=1}^n a_{i_j,j}.$$
Если два индекса совпадают, то $\omega(e_{i_1},\dots,e_{i_n})=0$, а вместе с ним и всё слагаемое. Остаются только наборы с разными $i_{\alpha}$, которые однозначно задают перестановку $\sigma(k)=i_k$. Теперь заметим, что $\omega(e_{\sigma(1)},\dots,e_{\sigma(n)})=\sgn(\sigma)\omega(e_1,\dots,e_n)$, что доказывает первое равенство. Теперь
$$\prod_{j=1}^n a_{\sigma(j),j}=\prod_{i=1}^n a_{i,\sigma^{-1}(i)}.$$
Если вместо $\sigma$ поставить сумму по $\sigma^{-1}$, то с одной стороны сумма не поменяется, а с другой стороны по тождеству выше, можно будет перекинуть $\sigma$ на другой индекс.
\endproof
\elm







\dfn
Пусть $n=\dim V$. Антисимметричная полилинейная форма $\omega \colon V^n \to K $ называется формой объёма на $V$. Если такая форма не равна 0, то будем говорить, что она невырождена.
\edfn



До этого момента мы говорили про объекты которых, возможно, просто не существует. Настала пора предъявить для них конструкцию и доказать её единственность. Для этого достаточно заметить, что все формы объёма, если их расписать в координатах пропорциональны фиксированной функции от координат векторов. Дадим название этой функции:

\dfn  Определителем $\det$ называется отображение $\det \colon M_n(K) \to K$, такое, что $$\det(A)=\sum_{\sigma \in S_n} \sgn(\sigma)\prod_{1\leq i\leq n} a_{i\sigma(i)}.$$
\edfn

Используя понятие определителя можно попытаться определить форму объёма, связанную с фиксированным базисом. Значение получившегося отображения на  самом исходном базисе (с учётом порядка) будет равно единице.

\dfn Пусть $e_1,\dots,e_n$ базис пространства $V$. Определим отображение $\Vol_e \colon V^n \to K$, такое, что
$$\Vol_e(v_1,\dots,v_n)=\det(e(v_1),\dots, e(v_n)),$$
где $e\colon V \to K^n$ -- отображение сопоставления координат.
\edfn

\thrm Верны следующие свойства:\\
1) Определитель является формой объёма на $K^n$, такой, что $\det E=1$.\\
2) Если $V$ -- пространство размерности $n$, то любая форма объёма на $V$ имеет вид $$\omega=\omega(e_1,\dots,e_n)\Vol_e.$$
В частности, если есть два базиса $e$ и $f$, то $\Vol_{f}=\det(C_{f \to e}) \Vol_{e}$.\\
3) Пространство форм объёма одномерно.\\
4) Для любой невырожденной формы объёма $\omega$ верно утверждение: $\omega(v_1,\dots,v_n)=0$ тогда и только тогда, когда $v_1,\dots,v_n$ линейно зависимы.
\proof Определитель является полилинейной функцией столбцов, так как  каждое слагаемое содержит ровно одну координату в первой степени из каждого столбца матрицы $A$. Осталось показать, что если в матрице $A$ пара столбцов одинакова, то её определитель равен нулю.
Пусть равны столбцы $k$ и $l$. Рассмотрим транспозицию $\tau=(kl)$. Тогда все перестановки разбиваются на классы $A_n$ и $A_n\tau$. Выпишем теперь сумму
$$\sum_{\sigma \in S_n} \sgn(\sigma)\prod_{j=1}^n a_{\sigma(j)j} = \sum_{\sigma \in A_n} \prod_{j=1}^n a_{\sigma(j)j}- \sum_{\sigma\tau \in A_n\tau}\,\,\, \prod_{j=1}^n a_{\sigma(\tau(j))j}.$$
Посмотрим на слагаемое в первой и во второй сумме, соответствующие одной и той же перестановке $\sigma \in A_n$. Заметим, что $a_{\sigma(\tau(j))j}=a_{\sigma(j)j}$, если $j\neq k,l$. Но при  $j=k$, благодаря тому что $\tau(k)=l$  и равенству столбцов матрицы $A$, имеем $a_{\sigma(\tau(k))k}=a_{\sigma(l)k}=a_{\sigma(l)l}$. Аналогично $a_{\sigma(\tau(l))l}=a_{\sigma(k)k}$, что означает, что произведения для одной и той же $\sigma$ в правой и левой части суммы одинаковы.\\

Покажем теперь пункт 2. Используя предыдущую лемму, получаем $\omega(v_1,\dots,v_n)=\omega(e_1,\dots,e_n)\Vol_e(v_1,\dots,v_n)$. То есть, форма $\omega$ пропорциональна форме $\Vol_e$ с коэффициентом $\omega(e_1,\dots,e_n)$. Для завершения доказательства осталось вычислить указанный коэффициент пропорциональности между $\Vol_e$ и $\Vol_f$ для базисов $e$ и $f$. Для этого надо вычислить $\Vol_f(e_1,\dots,e_n)$. То есть, надо построить матрицу из координат столбцов $e_i$ в базисе $f$ и посчитать её определитель. Но это матрица перехода $C_{f\to e}$. Итого $\Vol_{f}=\det(C_{f\to e}) \Vol_{e}$. Здесь мы получили, в частности, что любая форма объёма на пространстве $V$  пропорциональна форме $\Vol_e$, что доказывает пункт 3).


Пусть теперь $v_1,\dots,v_n$ набор векторов. Тогда, если $v_1,\dots,v_n$ линейно зависимы, то существует нетривиальная линейная комбинация $\sum \lambda_i v_i=0$. Пусть $\lambda_1\neq 0$, тогда можно считать, что $\lambda_1=1$. Тогда, прибавляя к первому аргументу остальные с соответствующими коэффициентами, получаем набор с нулевым первым вектором. но при таких преобразованиях значение формы объёма не должно меняться. Итого $\omega(v_1,\dots,v_n)=\omega(0,\dots)=0$.
Обратно, если $v_1,\dots,v_n$ независимы, то $\omega=\omega(v_1,\dots,v_n)\Vol_v$. Так как $\omega \neq 0$, то коэффициент пропорциональности $\omega(v_1,\dots,v_n)\neq 0$.
\endproof
\ethrm


Как видно построение форм объёма сводится к построению определителя. Технически, мы всегда будем обращаться с определителями. Поэтому сформулируем важные свойства: 

\lm Для определителей квадратных матриц верны следующие свойства:\\
0) $\det(A)=\det(A^{T})$.\\
1) Определитель не меняется при элементарных преобразованиях первого типа для строк и столбцов. При смене строк местами меняется знак определителя. При домножении строки на $\lambda$, определитель домножается на $\lambda$.\\
2) $\det(AB)=\det(A)\det(B)$.\\
3) $\det \left(\begin{matrix}A & B\\
0 & C \\
\end{matrix}\right)= \det(A)\det(C)$.\\
4) Определитель верхнетреугольных или нижнетреугольных матрицы равен произведению диагональных элементов.\\
5) $\det (A^{-1})=(\det A)^{-1}.$\\
6) $\det \colon \GL(V) \to K^*$
является гомоморфизмом групп.
\proof Свойство 0) мы уже доказали. Из него сразу же следует свойство 1). Докажем 2).\\
Заметим, что форма $B \to \det AB$ линейна по столбцам $B$. Тогда это форма объёма и следовательно она пропорциональна форме $B \to\det B$. Для того, чтобы найти коэффициент пропорциональности, подставим $B=E_n$. Имеем $\det AE_n=\det A= c \det E_n=c$. Откуда для любого $B$ получаем
$$\det AB=\det A \det B.$$\\
Покажем 3). Сначала посчитаем $$\det \pmat E_n& B\\
0& E_m\epmat.$$
Заметим, что с помощью элементарных преобразований 1-го типа из неё можно сделать $E_{n+m}$. Тогда определитель равен 1.
Теперь заметим, что форма $$A \to \det \pmat A & B\\ 0& E_m\epmat $$ форма объёма на $K^n$. Как и раньше подставив $A=E_n$ получаем что коэффициент пропорциональности с $A \to \det A$ равен 1.\\
Теперь отображение $$C \to \det \pmat A& B\\ 0 & C \epmat $$  есть форма объёма, по строчкам $C$. Подставляя $B=E_m$ находим коэффициент пропорциональности $\det A$, что и даёт
$$\det \pmat A&B\\ 0&C \epmat = \det A \det C.$$
 Применяя предыдущее условие несколько раз приходим к пункту 4).\\
Беря равенство $AA^{-1}=E_n $ и считая определитель получаем $\det A \det A^{-1}=E_n$, что завершает доказательство 5). Пункт 6) следует из 2) и того факта, что определитель обратимой матрицы обязан быть обратимым.
\endproof
\elm



С вычислительной точки зрения формула для определителя бесполезна -- в ней $n!$ слагаемых. Но она даёт некоторые важные следствия. Например, то, что определитель есть однородный многочлен степени $n$ с целыми коэффициентами от элементов матрицы (следовательно, понятие определителя можно ввести над любым коммутативным кольцом по этой формуле).

Обычно, для вычисления определителя матрицу приводят элементарными преобразованиями к ступенчатому виду. При таких преобразованиях не сложно проконтролировать, как определитель меняется. В ступенчатом же виде определитель так же легко сосчитать. Посмотрим на конкретные примеры вычисления определителей.



\exm
\enm 
\item Определитель  $\det \left(\begin{smallmatrix} a&b \\ c& d\end{smallmatrix}\right)=ad-bc$.
\item Определитель матрицы $3\times 3$ тоже можно выписать. 
\item Приведём пример, вычислив определитель Вандермонда.
$$\det \pmat 1 & \dots & 1\\
\lambda_1 & \dots & \lambda_n\\
\vdots &&\vdots\\
\lambda_1^{n-1}& \dots & \lambda_n^{n-1} \epmat= \prod_{i>j}(\lambda_i-\lambda_j).$$
Мы не будем слепо следовать методу Гаусса, а используем элементарные преобразования в другом порядке. Сначала вычтем из $n$-ой строки $n-1$-ую с коэффициентом $\lambda_1$. Потом из $n-1$-ой $n-2$-ую и т.д. Получим:
$$\det \pmat 1 & 1& \dots & 1\\
0 & \lambda_2-\lambda_1 & \dots & \lambda_n-\lambda_1\\
\vdots& \vdots &&\vdots\\
0& \lambda_2^{n-2}(\lambda_2-\lambda_1)& \dots & \lambda_n^{n-2}(\lambda_n-\lambda_1) \epmat= \prod_{i>1}(\lambda_i-\lambda_1)\det \pmat 1 & \dots & 1\\
\lambda_2 & \dots & \lambda_n\\
\vdots &&\vdots\\
\lambda_2^{n-2}& \dots & \lambda_n^{n-2} \epmat.$$
\eenm

Мы стартовали с понятия объёма параллелепипеда в $\mb R^n$. Отображение $A \to |\det A|$ удовлетворяет всем предполагаемым свойствам объёма. Может ли тем не менее, определить дать другое определение объёма параллелепипеда, которое даёт отличные от определителя результаты? \\
Оказывается, что нет. И для этого не нужно требовать все свойства объёма. Достаточно знать, что модуль определителя и объём одинаково изменяются при элементарных преобразованиях. Точнее:

\utv Пусть дано отображение $\Volume \colon M_n(\mb R) \to \mb R $, обладающее некоторыми свойствами объёма параллелепипеда: \\
1) $\Volume(E_n)=1$\\
2) $\Volume(\dots,u+\lambda v,\dots,v,\dots)=\Volume(\dots,u,\dots,v,\dots)$\\
3) $\Volume(\dots,\lambda v,\dots)=|\lambda|\Volume(\dots,v,\dots)$\\
Тогда $\Volume(A)=|\det A|$.
\proof Покажем, что эти свойства однозначно позволяют вычислить $\Volume(A)$. Так как отображение $A\to |\det A|$ тоже им удовлетворяет, то это гарантирует их совпадение.  Действительно, если матрицы $A$ вырождена, то элементарными преобразованиями первого типа можно сделать так, чтобы один столбец $A$ стал нулевым. По третьему свойству $$\Volume(v_1,\dots,0,\dots,v_n)=\Volume(v_1,\dots,0\cdot 0,\dots,v_n)=0\cdot \Volume(v_1,\dots,0,\dots,v_n)=0.$$
Если же $A$ невырождена, то элементарными преобразованиями первого типа её можно привести к диагональному виду не меняя определителя. Теперь вынося по свойству 3) диагональные элементы приводим матрицу $A$ к единичному виду. Осталось воспользоваться свойством 1).
\endproof
\eutv


\subsection{Ориентация}

Настало время отдать долги за школьный курс геометрии и поговорить о важном понятии -- понятии ориентации пространства.  Вспомним, что понятие определителя выросло из аксиоматизации свойств объёма и немного его переросло. А именно, для набора элементов из пространства $\mb R^n$ можно посчитать не только объём параллелепипеда на них натянутого (заведомо неотрицательное число), но ещё и некоторый знак. О смысле этого знака и пойдёт сейчас речь.



\dfn Будем говорить, что два базиса пространства $V$ над $\mb R$ одинаково ориентированы, если матрица перехода между ними имеет положительный определитель.
\edfn

\rm Отношение одинаковой ориентированности есть отношение эквивалентности.
\erm

\dfn Выбор одного из классов эквивалентности базисов вещественного векторного пространства $V$ называется заданием ориентации.
\edfn

\exm
\enm
\item Рассмотрим базисы  плоскости $(0,1),(1,0)$ и $(1,0),(0,1)$.  Они имеют разную ориентацию. Ориентация на $\mb R^n$, заданная стандартным базисом называется стандартной. 
\item Вообще, если один базис плоскости получен из другого при помощи поворота плоскости, то они будут одинаково ориентированы.
\item Аналогично, смена порядка двух базисных векторов приводит к смене ориентации. Растяжение одного вектора на положительную константу  не меняет ориентации, а домножение на отрицательное число -- меняет ориентацию.
\eenm


\utv Пусть есть два базиса $e_1,\dots,e_n$ и $f_1,\dots,f_n$ в пространстве $V$ над $\mb R$. Если они имеют разную ориентацию, то их нельзя продеформировать один в другой (внутри пространства базисов).
\proof Определимся с тем, что доказываем. Прежде всего перейдём к координатам. Можно считать, то $V=\mb R^n$ и множество всех базисов -- это множество обратимых матриц. Надо показать, что не бывает непрерывного отображения $f\colon [0,1] \to \GL_n(\mb R)$, так что $f(0)$ и $f(1)$ по разному ориентированы. 

Действительно, посмотрим на $\det (f(t))$. Пусть при $t=0$ выполнено $\det f(0)>0$,  а при $t=1$ выполнено $\det f(1)<0$. Заметим, что $\det f(t)$ непрерывная функция $[0,1] \to \mb R$. Тогда между $0$ и $1$ у неё есть корень. Но если, $\det f(t)=0$, то $f(t)$ вырождена, чего не может быть, так как по условию $f(t)$ -- обратимая матрица для любого $t$.
\endproof
\eutv

\upr Покажите, наоборот, что любые два одинаково ориентированных базиса можно продеформировать друг в друга.
\eupr

\subsection{Линейные операторы и ориентация}

Особо ценным классом линейных отображений являются отображения пространства в само себя. Я буду называть такие отображения линейными операторами на $V$. Сразу отмечу, что в литературе под словом оператор часто подразумевают линейное отображение между произвольными двумя пространствами. В этой ситуации есть специальное слово, говорящее о том, что речь идёт об отображениях из пространства в себя  -- эндоморфизм пространства. Мы, тем не менее, будем использовать узкое значение слова оператор, имеющее тот же смысл, что и понятие эндоморфизм.

\dfn Пусть $V$ -- пространство. Тогда линейное отображение $L\colon V \to V$ называется (линейным) оператором  на пространстве $V$. Пусть $e_1,\dots, e_n$ базис $V$. Тогда матрицей оператора $L$ в базисе $e$ называется матрица $[L]_e^e$ (базисы с разных концов взяты одинаковыми).
\edfn

Это определение удобно, потому что при фиксированном базисе композиции операторов $L_1\circ L_2$ соответствует произведение их матриц $A_1A_2$ в базисе $e$. В частности, если $A$ матрица $L$ в базисе $e$, то оператору $L^n$ соответствует матрица $A^n$. Если бы базисы были выбраны не согласовано, это было бы неверно. 

\dfn Пусть $L\colon V \to V$ -- линейный оператор. Тогда определим $\det L=\det A$, где $A$ -- матрица оператора в каком-то базисе.
\edfn

Априори неясно, не зависит ли данное определение от выбора базиса. Однако сразу можно заметить, что линейный оператор $L$ обратим тогда и только тогда, когда $\det L \neq 0$. Это даёт уверенность в том, что понятие определителя имеет безкоординатный смысл и, значит, инвариантно, относительно замены базиса. Давайте это покажем:

\utv Определитель корректно определён.
\proof Корректность получается из соображения, что при смене базиса $e\to f$ матрица $A$ заменяется на $C_{f\to e}A C_{f \to e}^{-1}$. Считая определитель получаем
$$\det C_{f\to e}A C_{f \to e}^{-1}= \det C_{f\to e} \det A \det C_{f\to e}^{-1}= \det A.$$
\endproof
\eutv



\dfn Пусть $V$ -- векторное пространство над $\mb R$. Будем говорить, что линейный оператор $L\colon V \to V$ сохраняет ориентацию, если $\det L>0$ и не сохраняет, если $\det L<0$.
\edfn

\lm Сохраняющее ориентацию отображение переводит одинаково ориентированные базисы в одинаково ориентированные.
\proof Пусть $e$ -- базис. Если матрица $L$  в базисе $e$ это $A$, то $A$ -- это то же самое, что матрица перехода от $e$ к $Le$, что и доказывает утверждение. Условие сохранения ориентации говорит, что $\det A > 0$, а значит $e$ и $Le$ одинаково ориентированы.
\endproof
\elm

\exm\\
1) Симметрия относительно прямой в $\mb R^2$ меняет ориентацию.\\
2) Поворот сохраняет ориентацию.\\
3) Центральная симметрия в $\mb R^n$ (домножение на $-1$) меняет или нет ориентацию в зависимости  от чётности $n$.\\





Проинтерпретируем наши результаты с случае $\mb R^n$. Возьмём базис $v_1,\dots,v_n$ и пусть $L$ -- задан матрицей $A$. Пусть $B$ -- это матрица составленная из $v_1,\dots,v_n$. Мы знаем, что $\det AB= \det A \det B$. Это значит, что $\det L= \det A$ есть коэффициент изменения между объёмом параллелепипеда, натянутого на $v_i$ и объёма параллелепипеда, натянутого на $Lv_i$. То есть определитель оператора есть коэффициент изменения объёма любого параллелепипеда под действием $L$. Это причина того, что при многомерной замене координат в интеграле вылезает определитель Якобиана -- то есть линейного приближения к исходному отображению.

Стоит отметить, что это отношение не зависит и от выбора невырожденной формы объёма так как любые две формы объёма пропорциональны.


\dfn Определим группу операторов $\SL(V)=\{ L\colon V \to V \,|\, \det L=1\}$. Если $V$ -- вещественное векторное пространство, то это операторы, которые сохраняют понятие объёма и выбор ориентации пространства. $\SL_n(K)$ называется группа матриц с определителем единица.
\edfn







\subsection{Явные формулы в линейной алгебре}

Ещё одно применение определителя -- это возможность выписать явное решение для многих задач линейной алгебры. Посмотрим, как это проявляется. Для начала взглянем на сам определитель. Заметим, что так как определитель линеен по столбцам, то должно иметь место равенство $\det (\dots,v,\dots)= \sum c_i v_i$,  где $v_i$ -- координат столбца $v$, а $c_i$ -- некоторые на зависящие от столбца $v$ коэффициенты. Мы сейчас найдём эти коэффициенты $c_i$. Для того, чтобы сформулировать ответ нам понадобятся несколько определений.

\dfn Пусть $A \in M_{m\times n}(K)$. Пусть $I\subseteq \{1,\dots,m\}$, а $J\subseteq \{1,\dots n\}$. Подматрицей $A_{I,J}$ будем называть матрицу, составленную из элементов $A$, стоящих в строках с номерами из $I$ и в столбцах с номерами из $J$. Минором порядка $k$ матрицы $A$ называется определитель некоторой квадратной подматрицы $M_{I,J}=\det A_{I,J}$, где $|I|=|J|=k$. Если $A\in M_n(K)$, то алгебраическим дополнением элемента $a_{ij}$ называется $A^{ij}=(-1)^{i+j} M_{\ovl{i},\ovl{j}}$.
\edfn



\lm При разложении по $j$-ому столбцу имеет место формула  $$\det(A)=\sum_{i=1}^n a_{ij} A^{ij}.$$
\proof Прежде всего установим эту формулу в простейшем случае $$A=\pmat 1& *\\
0& A_{\ovl{1},\ovl{1}}\epmat. $$
Это следствие вычисления определителя блочной матрицы. Итак, пусть фиксирован столбец $j$. Как же теперь найти коэффициенты при $a_{ij}$ в разложении? Для этого надо в качестве $j$-го столбца поставить стандартный базисный столбец $e_i$ и посчитать определитель. Сделаем это. Напишем матрицу
$$B= \bordermatrix{
 & &       &  j &      & \cr
 & a_{11} && 0 &  & a_{n1}    \cr
 & \vdots& & \vdots& & \vdots\cr
 i &a_{i1}&\dots & 1 &\dots & a_{in} \cr
 & \vdots& & \vdots& & \vdots\cr
 & a_{1n} &  & 0 &      &a_{nn} } $$
и переставим по циклу столбцы, чтобы $j$-ый столбец стал первым, и $i$-ая строчка стала первой. Такое преобразование изменит знак определителя на $(-1)^{i-1+j-1}=(-1)^{i+j}$. А в правом нижнем блоке будет стоять нужная подматрица $A$.
\endproof
\elm
Это свойство называется разложением  определителя матрицы  по столбцу. Аналогичная формула верна и для разложения по строке.


В прошлом разделе мы научились по некоторой формуле определять обратима матрица или нет. Это полезно в различных задачах с параметром, где метод Гаусса напрямую не применим для вычисления определителя. Можно поставить новый вопрос: а можно ли написать формулу для решения системы линейных уравнений при некоторых ограничениях? Ответ на  этот вопрос дают формулы Крамера.

\thrm[Формула Крамера]
Пусть дана система линейных уравнений $Ax=b$ с квадратной матрицей $A$ над полем $K$. Если матрица $A$ --- обратима, то единственное решение этой системы имеет вид $$x_i=\frac{\Delta_i}{\Delta}, \text{ где } \Delta=\det A, \text{ а } \Delta_i \text{ --- определитель матрицы, полученной из  $A$ заменой $i$-го столбца на столбец $b$}.$$
\proof Пусть $x$ такой, что $Ax=b$. За $v_1,\dots,v_n$ обозначим столбцы $A$. Тогда $b=x_1v_1+\dots+x_nv_n$.  Теперь посчитаем определитель $\Delta_i$. Это определитель матрицы $(v_1,\dots,v_{i-1},b,v_{i+1},\dots,v_n)$. Подставим $b=x_1v_1+\dots+x_nv_n$ и раскроем скобки в определителе, пользуясь его полилинейностью. Останется ровно одно слагаемое с $x_iv_i$. Итого
$$\Delta_i=\det (v_1,\dots,v_{i-1},b,v_{i+1},\dots,v_n)= x_i \det(v_1,\dots,v_{i-1},v_i,v_{i+1},\dots,v_n)=x_i\det A=x_i \Delta.$$
\endproof
\ethrm

Итак мы научились явно решать систему линейных уравнений. Заметим, в свою очередь, что столбец $u_j$ в матрице $A^{-1}$ есть решение уравнения $Au_j=e_j$. Тогда по формуле Крамера
$$(A^{-1})_{ij}=\frac{\det(v_1,\dots,v_{i-1}, e_j,v_{i+1},\dots,v_n)}{\det A}=\frac{A^{ji}}{\det A}.$$
Это приводит нас к определению:

\dfn
Присоединённой матрицей к матрице $A$ называется матрица $(\Adj A)_{ij}= A^{ji}$ где $A^{ij}$ -- алгебраическое дополнение элемента $a_{ij}$.
\edfn

\thrm
Пусть $A\in M_n(K)$. Тогда имеет место соотношение
$$\Adj A \cdot A= A\cdot \Adj A= \det(A)\cdot E.$$
\proof[Первое доказательство] Для определённости будем доказывать $A\cdot \Adj A= \det(A)\cdot E$. Посчитаем диагональные элементы. Они имеют вид $\sum_{k=1}^n a_{ik} A^{ik}=\det A$ по формуле разложения по $i$-ой строке для матрицы $A$. 

Посмотрим теперь на внедиагональный элемент в произведении. Он представим суммой $\sum_{k=1}^n a_{ik}A^{jk}$ при $j\neq i$. Рассмотрим матрицу $A'$, в которой все строки, кроме $j$-ой такие же как в матрице $A$, а на место $j$-ой строки поставлена копия $i$-ой. Понятно, что $\det A'= \sum_{k=1}^n a_{ik}A^{jk}$ по формуле разложения по строке. С другой стороны матрица $A'$ вырождена. Поэтому это $0$.

\proof[Второе доказательство] Заметим, что если матрица $A$ обратима, то это уже у нас в кармане. Что же делать для необратимых $A$?
Оказывается, что вместо того, чтобы пытаться как-то свести к тождеству для всех матриц над тем же полем $K$ проще доказать это тождество над любым кольцом. Точнее, заметим, что это тождество равносильно равенству нулю некоторых целочисленных многочленов от коэффициентов матрицы. Это тождество имеет смысл над любыми кольцами. Тогда, если мы доказали это тождество для кольца $R$ и матрицы $B$, и есть гомоморфизм $\ffi \colon R \to S$, то это тождество верно для матрицы $\ffi(B)$ над $S$.

Рассмотрим кольцо $\mb Z[a_{11}, \dots, a_{nn}]$ многочленов от $n^2$ независимых переменных. Над этим кольцом есть матрица $A$, что $A_{ij}=a_{ij}$. Эта матрица называется общей матрицей (размера $n\times n$). Потому что для любой матрицы $B$ размера $n\times n$ над кольцом $R$ есть гомоморфизм $\ffi \colon \mb Z[a_{11},\dots,a_{nn}]\to R $, что $\ffi(A)=B$. Иными словами в эту матрицу можно подставить любые значения.

Таким образом, достаточно доказать наше тождество для одной матрицы $A$. Для этого вложим кольцо
$$\mb Z[a_{11},\dots,a_{nn}] \to Q(\mb Z[a_{11},\dots,a_{nn}]).$$ Тогда достаточно доказать тождество для матрицы $A$ в $Q(\mb Z[a_{11},\dots,a_{nn}])$. Для этого покажем, что матрица $A$ невырождена. Заметим, что её определитель -- это ненулевой многочлен $$\sum_{\sigma \in S_n} \sgn(\sigma) \prod_{i=1}^n a_{i\sigma(i)} \in Q(\mb Z[a_{11},\dots,a_{nn}]),$$ который мы обычно и называем определителем. Раз определитель $A$ не ноль, то $A$ -- невырожденная матрица над полем $Q(\mb Z[a_{11},\dots,a_{nn}])$ и, следовательно для неё тождество выполнено. Что и требовалось.
\endproof
\ethrm

В этом доказательстве мы пользовались возможностью перейти к полю частных и тем, что общая матрица обратима хотя и не над кольцом $\mb Z[a_{11},\dots,a_{nn}]$, но в его поле частных. Если посмотреть, когда ещё можно провернуть похожее рассуждение, то можно сформулировать принцип: если есть некоторое полиномиальное тождество, верное для всех значений переменных, кроме, возможно, тех, которые задаются нетривиальным полиномиальным уравнением $p(x_1,\dots,x_n)=0$, то это тождество выполнено без исключений. Под словом, нетривиальный здесь подразумевается следующее: есть такой набор координат, для которых $p(x_1,\dots,x_n)\neq 0$. 

\upr Покажите, что над любым кольцом $\det(AB)=\det A \det B$.
\eupr


\section{Алгебры}

{\bf {\color{red} Внимание!!!}} Начиная с этого момента под словом кольцо я буду понимать ассоциативное кольцо с единицей. Коммутативно кольцо или нет теперь придётся уточнять.

Настала пора познакомиться с самой навороченной структуры, которая будет у нас в курсе. Начнём издалека. Если есть два пространства $V_1$ и $V_2$ размерностей $n$ и $m$, то имеет место изоморфизм векторных пространств $\Hom(V_1,V_2)\cong M_{m\times n}(K)$. Ничего лучше не сказать, потому что никаких других структур на этих пространствах в общем случае нет. Однако если $V=V_1=V_2$, то пространство $\Hom(V,V)=\End(V)$, то есть множество линейных операторов на $V$ является ещё и кольцом относительно сложения и композиции.  Структура векторного пространства и кольца связаны (кроме дистрибутивности, см. прошлый семестр) ещё одним соотношением:
$$\forall \lambda \in K \text{ и } L_1, L_2 \in \End(V) \text{ верно, что } (\lambda L_1)\circ L_2= \lambda (L_1 \circ L_2)=L_1 \circ(\lambda L_2).$$


\dfn(Алгебра над полем) Пусть $K$ -- поле. Кольцо $S$ вместе с отображением $K \times S \to S$ называется алгеброй, если \\
1) $\forall r \in K$, $\forall u,v \in S$ $r(uv)=(ru)v=u(rv)$.\\
2) $S$ является  векторным пространством над $K$ относительно указанных операций.
\edfn


\rm По нашему соглашению получается, что алгебры всегда ассоциативные кольца с единицей. Стоит понимать, что это всего лишь соглашение в рамках нашего курса. Общепринятое определение не подразумевает этих свойств. Есть важные примеры алгебр, которые не имеют единицы или неассоциативны. Это алгебры функций с компактным носителем в $\mb R^n$ (нет единицы) и так называемые алгебры Ли (нет ассоциативности и единицы). В нашем курсе они не будут систематически появляться.

Можно определить понятие алгебры над произвольным коммутативным кольцом. Дать аналогичное определение над некоммутативным кольцом затруднительно. Причина в свойстве 1).
\erm

Многие из колец, которые появлялись в курсе были алгебрами над каким-то полем:

\exm
\enm
\setcounter{enumi}{-1}
\item Поле $K$ есть алгебра над собой
\item Если $L$ -- расширение поля $K$, то $L$ является алгеброй над $K$.
\item Например, $\mb C$ -- это алгебра над $\mb R$
\item Кольцо эндоморфизмов $\End_K(V)$ векторного пространства $V$ над полем $K$ является алгеброй над $K$. В частности, кольцо матриц над полем $M_n(K)$ есть алгебра над $K$. Это некоммутативная алгебра над $K$ при $n\geq 2$. 
\item То же можно сказать про кольцо верхнетреугольных(нижнетреугольных) матриц $UT_n(K)$.
\item Кольцо многочленов $K[x_1,\dots,x_n]$ есть алгебра над $K$.
\item Любой фактор кольца многочленов $K[x_1,\dots,x_n]/I$ есть алгебра над $K$.
\item Разберём для разнообразия ещё одну конструкцию, которая приводит к интересным некоммутативным алгебрам. Представим себе векторное пространство $V$ с базисом $e_1,\dots,e_n$. Допустим, хочется завести на $V$ структуру алгебры. То есть необходимо научиться умножать два элемента из $V$. Произвольный элемент из $V$ выглядит как сумма $\lambda_1 e_1+\dots+\lambda_n e_n$. Посмотрим, что должно происходить при перемножении двух таких элементов:
$$(\sum_{i=1}^n \lambda_i e_i)\cdot (\sum_{j=1}^n \mu_j e_j)=\sum_{i,j} \lambda_i\mu_j (e_i\cdot e_j).$$
Таким образом видно, что произведение на самом деле достаточно определить только на базисных элементах, а дальше продолжить на все элементы пользуясь указанной выше формулой. Это гарантированно даст структуру кольца. Однако, нам стоит разобраться, когда такая операция даст ассоциативную кольцо. Оказывается, что для этого достаточно ассоциативности умножения на базисных элементах $(e_i\cdot e_j)\cdot e_k=e_i\cdot (e_j\cdot e_k)$. Действительно
$$\left((\sum_{i=1}^n \lambda_i e_i)\cdot (\sum_{j=1}^n \mu_j e_j)\right)\cdot(\sum_{k=1}^n \nu_k e_k)= \sum_{i,j,k} \lambda_i\mu_j \nu_k (e_i\cdot e_j)\cdot e_k=\sum_{i,j,k} \lambda_i\mu_j \nu_k e_i\cdot (e_j \cdot e_k)=(\sum_{i=1}^n \lambda_i e_i)\cdot \left((\sum_{j=1}^n \mu_j e_j)\cdot(\sum_{k=1}^n \nu_k e_k)\right)$$
Теперь осталось привести конкретный пример: пусть $G$ группа из $n$ элементов (можно конечно и не конечную брать -- к чему это приведёт подумайте сами). Групповой алгеброй $K[G]$ над полем $K$ назовём следующую алгебру: возьмём пространство столбцов размера $n$, занумеруем элементы стандартного базиса элементами группы $G$ (любым способом). Соответствующий $g\in G$ базисный вектор будем обозначать $e_g$. Унаследуем умножение на базисных векторах с элементов группы:
$$e_g e_h=e_{gh}.$$
Это определяет структуру алгебры на $K^n$, которая и называется групповой алгеброй. $K[G]$ некоммутативна тогда и только тогда, когда $G$ некоммутативна. Это центральный объект теории представлений групп -- важной области, использующейся в физике.
\eenm





\dfn Отображение $f \colon S_1 \to S_2$, где $S_1$ и $S_2$ являются $K$-алгебрами, называется гомоморфизмом $K$-алгебр, если $f$ -- гомоморфизм колец и линейное отображение.
\edfn



\rm Алгебра эндоморфизмов $\End_K (V)$  изоморфна,  алгебре матриц $M_{\dim V}(K)$.
\erm

В теории групп любая конечная группа могла быть реализована как подгруппа в $S_n$. Оказывается, что мы давно знаем алгебру, которая играет аналогичную $S_n$ роль.



\thrm[Теорема типа Кэли] Любая алгебра $A$, конечномерная над полем $K$, вкладывается в $\End_K(A)$ (то есть фактически в кольцо матриц).
\proof Пусть $x\in A$. Тогда рассмотрим отображение $L_x \colon A \to A$, заданное по правилу $L_x(z)=xz$. Тогда $L_x\circ L_y=L_{xy}$. Так же $L_x+L_y=L_{x+y}$, а $L_1=\id_A$. Заметим, что, если $x\neq 0$, то $L_x$ не нулевое, потому что $L_x(1)=x\neq 0$. Таким образом отображение $A \to \End_K(A)$ заданное правилом $x\to L_x$ является инъективным гомоморфизмом алгебр над $K$, что и требовалось.
\endproof
\ethrm

\rm[Дополнительно] В частности, $\mb C$ вкладывается в алгебру матриц $M_2(\mb R)$ по правилу $a+bi \to \left( \begin{smallmatrix} a &-b\\ b & a \end{smallmatrix}\right) $. Для любой алгебры указанное вложение позволяет ввести понятие нормы: $norm (y)=\det L_y$ -- численная характеристика для любого элемента алгебры за бесплатно. 
\erm



Нам извести, что, если  два коммутативных кольца $R$ и $S$  связаны гомоморфизмом $f\colon R \to S$, то есть единственный гомоморфизм $R[x] \to S$ переводящий $x$ в заданный элемент $s \in S$, а на константах, совпадающий с $f$. Он имеет вид $p(x) \to f(p)(s)$. Я буду апеллировать к аналогичной конструкции в несколько другой ситуации.

\rm
Пусть $K$ -- поле, $A$ -- алгебра над $K$. Заметим, что для элемента $y \in A$ и многочлена $p(x)=a_0+\dots+a_n x^n\in K[x]$ можно определить элемент $p(y)=a_0+\dots+a_n y^n \in A$. Соответствие $p(x) \to p(y)\in A$ определяет единственный гомоморфизм $K$-алгебр $\ffi \colon K[x]\to A$ такой, что $\ffi(x)= y$.
\erm



\rm[Дополнительно] Однако не всё так просто, если переменных больше. Пусть $a,b$ два элемента алгебры $A$, которые не коммутируют между собой. Тогда не существует гомоморфизма $K[t_1,t_2]$ переводящего $t_1\to a$, и $t_2 \to b$.
\erm

\upr Является ли условие коммутирования необходимым условием для существования гомоморфизма? Каков ответ для $K[x_1,\dots,x_n]$?
\eupr




\utv Для любого элемента $y$ конечномерной алгебры $A$ существует $p(x)\in K[x]$, $p(x)\neq 0$ такой, что $p(y)=0$.
\proof Рассмотрим набор элементов $1,y,\dots, y^{\dim A}$. Они линейно зависимы. Следовательно, их нетривиальная линейная комбинация равна нулю, что и даёт искомое уравнение.
\endproof
\eutv


\dfn Ядро гомоморфизма $K[x] \to A$, переводящего $x \to y$ является идеалом  $Ann_y\leq K[x]$. Его элементы называют аннуляторами для элемента $y$ из $A$. Если этот идеал не 0 (есть нетривиальный многочлен, аннулирующий $y$), то образующую этого идеала (со старшим коэффициентом 1) называют минимальным многочленом для элемента $y\in A$ и обозначают $\mu_y(x)$. По другому, это многочлен минимальной степени со старшим коэффициентом 1, аннулирующий $y$.
\edfn

Мы только что показали, что в конечномерной алгебре у любого элемента есть минимальный многочлен. Получим из этого простое следствие:

\thrm Любой элемент $y$ конечномерной алгебры $A$ над полем $K$ либо обратимым, либо делитель нуля (с любой стороны).
\proof Рассмотрим минимальный многочлен $\mu_y(x)= x^n+ \dots+a_0$. Пусть $a_0$ свободный член $p(y)$. Если $a_0=0$, то $$y(y^{n-1}+
\dots + a_1)=(y^{n-1}+
\dots + a_1)y=0.$$ Благодаря минимальности оба выражения отличны от 0, что показывает, что $y$ делитель нуля с любой стороны. Если же $a_0\neq 0$, то $$y^{-1}=\frac{1}{-a_0}(y^{n-1}+
\dots + a_1).$$ В частности, обратный элемент в конечномерной алгебре всегда есть многочлен от исходного. Одновременно оба условия выполнены быть не могут так как, если $yb=0$ и $y^{-1}y=1$, то $b=y^{-1}yb=0$.
\endproof
\ethrm

\rm В частности, обратная матрица всегда многочлен от исходной.
\erm

\upr Для бесконечномерных алгебр это неверно.
\eupr

\upr Покажите, что если $L$ расширение поля $K$, то минимальный многочлен любого ненулевого элемента $y\in L$ неприводим.
\eupr





\section{Линейные операторы}

Посмотрим более пристально на алгебру матриц, или, в бескоординатной форме -- алгебру операторов. Конструкции, которые относятся к структуре кольца на пространстве матриц уже встречались нам. Это прежде всего возведение матрицы в степень -- с его помощью мы считали количество путей в графе, распределение людей в городах. Вот ещё один пример:

Рассмотрим последовательность $x_{n+2}=x_{n+1}+x_{n}$, $x_0=a,x_1=b$. Как посчитать $x_{1000}=?$. Для того, чтобы воспользоваться рекуррентой, надо сделать 1000 операций. Можно ли меньше? С одной стороны вы знаете ответ -- надо найти характеристический многочлен и посчитать его корни, а потом свести всё к вычислению геометрической прогрессии.  Однако в этом случае для получения точного ответа придётся возиться с  иррациональными корнями. Попробуем сделать по другому. Заменим наше соотношение системой
$$ \begin{cases} x_{n+1}=x_n+y_n \\
y_{n+1}=x_{n}
\end{cases}.$$
Перепишем её в следующем виде
$$ \pmat x_{n+1}\\ y_{n+1}\epmat = A \pmat  x_{n}\\ y_n \epmat, \text{ где } A=\pmat 1& 1\\ 1& 0 \epmat.$$
Тогда  $x_{1000}$ это первая координата столбца $A^{1000} \pmat b\\ a\epmat $. Итого достаточно просто возвести матрицу в 1000 степень. Заметим, что это частный случай общей задачи: имеется последовательность $x_n$ из $K^m$ удовлетворяющая соотношению $x_{n+1}=Ax_n$. Требуется найти её поведение в зависимости от $n$. Например -- дан  набор состояний $s_1,\dots,s_k$ и даны вероятности перехода между состояниями $a_{ij}$ за один шаг. Вопрос: что произойдёт с системой после $n$ шагов?
Или, сводя к произведению матриц: как ведёт себя степень матрицы в зависимости от $n$? Оказывается, что этот вопрос удобнее решать не на языке матриц, а на бескоординатном языке операторов, по той причине, что в этой ситуации выбор подходящих координат остаётся за нами. 


Какие величины, связанные с оператором $L \colon V \to V$ не зависят от системы координат. Все свойства оператора $L$ можно определить по его матрице в каком-нибудь базисе. Заметим, что если есть два базиса $e$ и $f$, то матрицы $A=[L]_e^e$ и $A'=[L]_f^f$ связаны соотношением $A'=CAC^{-1}$ для некоторой обратимой матрицы $C$. Наоборот, если $A=[L]_e^e$  и $A'=CAC^{-1}$, то $A'$  тоже матрица $L$, но в другом базисе.

\upr Докажите это.
\eupr

Получается, что свойства оператора $L$, не зависящие от выбора координат, это такие свойства его матрицы $A$, которые не меняются когда мы $A$ заменяем на $A'=CAC^{-1}$. Это приводит нас к стандартному определению:  



\dfn Две матрицы $A, B \in M_n(K)$ подобны если существует матрица $C \in \GL_n(K)$, что $A=CBC^{-1}$. Матрицы одного оператора в разных базисах подобны.
\edfn

Изучение операторов -- это тоже самое, что изучение матриц с точностью до подобия. Нам будет удобно прыгать между двумя этими языками. Вернёмся к операторам. Попробуем придумать какие-то свойства не зависящие от выбора базиса.



\dfn Пусть $V$ -- пространство с оператором $L$. Пусть $U\leq V$. Тогда $U$ называется инвариантным подпространством, если $L(U) \leq U$.
\edfn

\rm Это условие позволяет сузить оператор $L$ с $V$ на $U$. Наличие или отсутствие инвариантных подпространств какой-нибудь размерности не зависит от выбора системы координат.
\erm

\lm Пусть $U\leq V$ -- подпространство, а $L \colon V \to V$ -- линейный оператор. Тогда $U$ инвариантно относительно $L$ тогда и только тогда, когда в базисе $e_1,\dots,e_k,e_{k+1},\dots,e_n$, где $e_1,\dots,e_k$ базис $U$ матрица оператора имеет блочно диагональный вид
$$\pmat A&B\\
0&C \epmat$$
\proof Образ $L(e_i)$ при $i \leq k$ лежит в $U$ и раскладывается по базису $U$, то есть по первым $k$ векторам. Значит коэффициенты при последних $n-k$ векторах нулевые. Но это и есть коэффициенты в левом нижнем блоке матрицы $A$.
\endproof
\elm


\rm У нас снова всплыли блочные матрицы и на этот раз нам необходимо обсудить как перемножаются матрицы в таком виде. Общая формулировка выглядит так. Если есть две матрицы
$$A=\pmat A_{11} & A_{12}\\
A_{21}& A_{22}
\epmat \text{ и } B=\pmat B_{11} & B_{12}\\
B_{21}& B_{22}
\epmat $$
которые перемножаемы (размеры $A_{ik}$ и $B_{kj}$ согласованы), то тогда
$$AB= \pmat A_{11}B_{11}+ A_{12}B_{21} & A_{11}B_{12}+ A_{12}B_{22}\\
A_{21}B_{11}+ A_{22}B_{21}& A_{21}B_{12}+ A_{22}B_{22}\epmat. $$
То есть матрицы перемножаются по блокам.
\erm

Посмотрим на простейший случай, когда инвариантное пространство одномерно. Пусть оно порождено вектором $v$. Тогда условие инвариантности перепишется как $L(v) = \lambda v$ для некоторого $\lambda \in K$.


\dfn Пусть $V$ -- пространство с оператором $L$. Тогда вектор $0\neq v\in V$ называется собственным вектором с собственным числом $\lambda$ относительно оператора $L$, если $Lv=\lambda v$.
\edfn





\exm \\
1) Рассмотрим пространство  последовательностей и оператор сдвига $S(x)_n= x_{n+1}$. Тогда собственный вектор -- это геометрическая прогрессия.\\
2) Рассмотрим тот же контекст. Тогда несложно увидеть, что подпространство последовательностей, удовлетворяющих линейному рекуррентному соотношению с постоянными коэффициентами является инвариантным относительно оператора сдвига.\\
3) В задаче про поисковую систему нам нужен был вектор $w$, такой что $P_G w=w$. Это собственный вектор $P_G$ с собственным числом $1$.\\
4) Подпространство многочленов степени меньшей или равной $n$ инвариантно относительно оператора дифференцирования.\\
5) Если $p(x)$ многочлен, то $\Ker p(L)$ инвариантно относительно $L$.\\
6) Пусть $v \in V$. Тогда  $V'=\lan v, Lv, L^2v,\dots \ran$ является инвариантным пространством, порождённым $v$. Такое пространство называется циклическим.\\
7) Рассмотрим алгебру $K[x]/p(x)q(x)$. Тогда подпространство многочленов делящихся на $p(x)$ является инвариантным относительно оператора $f(x) \to x f(x)$ домножения на $x$.\\




Как найти собственные векторы и соответствующие им собственные числа? Оказывается, что проще найти сначала собственные числа.

\dfn Определим характеристический многочлен оператора $L$ как $\chi_L(t)=\det(A-tE_n)$, где $A$ -- матрица $L$ в некотором базисе.
\edfn

\lm Характеристический многочлен корректно определён.
\elm
\proof Пусть $A$ матрица оператора $L$ в базисе $e$, $A'$ -- в базисе $f$, а $C$ матрица перехода откуда-то куда-то. Тогда $A'=CAC^{-1}$. Рассмотрим $\chi_A(t)$, как элемент $K(t)$. Тогда $C$ -- тоже матрица над $K(t)$ и
$$\det(A'-tE)=\det(CAC^{-1}-tCC^{-1})=\det(C(A-tE)C^{-1})=\det(C)\det(A-tE)\det C^{-1}=\det(A-tE).$$
Раз эти выражения равны как элементы $K(t)$, то и как элементы $K[t]$.
\endproof

\utv Элемент $\lambda \in K$ является собственным числом оператора $L$ тогда и только тогда, когда $\lambda$ корень $\chi_L(t)$.
\proof $\lambda$ собственное число тогда и только тогда, когда есть ненулевой $v$, что $Lv=\lambda v$ тогда и только тогда, когда $(L-\lambda \id)v=0$ тогда и только тогда, когда матрица этого оператора вырождена тогда и только тогда, когда $\det (A-\lambda E) =0$.
\endproof
\eutv



\section{Диагонализация переписать с учётом ВШЭ}

В прошлый раз мы остановились на том, что собственные числа оператора $L$ -- это корни характеристического многочлена $\chi_L(t)$. В этот раз наша цель -- показать, что  для почти всех операторов, характеристический многочлен есть единственная величина, которая не зависит от выбора базиса. То есть, почти всё определяется характеристическим многочленом. В конце параграфа мы покажем, какие бывают исключения.


Первый вопрос, который мы зададим про характеристический многочлен, следующий: насколько общим $\chi_L(t)$ бывает? Верно ли, что любой многочлен $f(x)\in K[x]$ ($\deg f \geq 1$) может быть реализован с точностью до обратимой константы как характеристический многочлен некоторой матрицы над $K$?

\dfn Пусть $f(x)\in K[x]$ многочлен $\deg f \geq 1$. Тогда сопровождающей матрицей к $f(x)=x^n+a_{n-1}x^{n-1}+\dots+a_0$ называется 
$$ \pmat  &&&&-a_0\\
1&&&&-a_1\\
&1&&&-a_2\\
&&\ddots&&\vdots\\
&&&1&-a_{n-1}\epmat.$$
\edfn

\utv Характеристический многочлен сопровождающей матрицы равен $(-1)^n f(t)$.
\eutv
\proof Рассмотрим матрицу $A-tE$, где $A$ -- сопровождающая матрица:
$$A-tE= \pmat -t&&&&-a_0\\
1&-t&&&-a_1\\
&\ddots&\ddots&&\vdots\\
&&1&-t&-a_{n-2}\\
&&&1&-t-a_{n-1}\epmat.$$
Прибавим последнюю строку к $n-1$-ой с коэффициентом $t$. Имеем:
$$\det A-tE = \det \pmat -t&&&&-a_0\\
1&-t&&&-a_1\\
&\ddots&\ddots&&\vdots\\
&&1&0&-t^2-ta_{n-1}-a_{n-2}\\
&&&1&-t-a_{n-1}\epmat$$
Поступая так и далее мы придём к определителю
$$\det \pmat 0&&&&-f(t)\\
1&0&&&*\\
&\ddots&\ddots&&\vdots\\
&&1&0&*\\
&&&1&*\epmat=(-1)^{n-1}(-f(t))=(-1)^nf(t).$$
\endproof

\upr Найдите в примерах оператор, матрицей которого является данная матрица или её транспонирование.
\eupr



Коэффициенты многочлена $\chi_L(t)$ являются инвариантами оператора $L$. Давайте посмотрим на них чуть внимательнее.
Прежде всего в глаза бросается, что свободный член это $\chi_L(0)=\det L$.
 

\dfn Пусть $A$ -- матрица размера $n$, тогда $$\Tr A=\sum_{i=1}^n a_{ii}.$$ 
След оператора $L$ -- это след его матрицы. Это определение не зависит от выбора базиса.
\edfn

\rm След матрицы $A$ это $(-1)^{n-1}a_{n-1}$, где $\chi_A(t)= \sum a_{i}t^i$. 
\erm

След оператора обладает массой интересных свойств:

\lm След обладает следующими свойствами: \\
1) Пусть $A$ -- квадратная матрица. Тогда $\tr CAC^{-1}= \tr A$ для обратимой матрицы $C$.\\
2) $\tr AB = \tr BA$ для $A\in M_{n\times m}(K)$ и $B\in M_{m\times n}(K)$.\\
3) След равен сумме собственных чисел с учётом кратностей, как корней характеристического многочлена (над любым полем, где характеристический многочлен раскладывается на линейные множители).\\
4) $\tr A= \tr A^{\top}$.\\
5) $\tr( A + \lambda B) = \tr A + \lambda \tr B$.
\elm
\proof
1) следует из того, что характеристический многочлен не зависит от выбора базиса и, следовательно, его коэффициенты. 2) Если расписать, то равенство эквивалентно
$$\sum_{i,k} a_{ik}b_{ki}=\sum_{j,l} b_{jl}a_{lj}.$$
Которое верно, особенно если взять $k=j$ и $i=l$. 3) Многочлен $\chi(t)=\prod (\lambda_i-t)$. Осталось раскрыть скобки. 4),5) ясно. \endproof


\rm Аналогично следу, определитель так же выражается через собственные числа. А именно, он равен произведению собственных чисел с учётом их кратностей, как корней характеристического многочлена.
\erm


\rm Для матриц $2\times 2$ знание определителя и следа равносильно знанию характеристического многочлена. Точнее, имеет место формула $\chi_A(t)=t^2-(\Tr A)\, t + \det A$. Для матриц большего размера есть формула для коэффициентов характеристического многочлена через миноры исходной матрицы.
\erm


Поставим следующий вопрос: насколько простой можно выбрать матрицу оператора $L$? Давайте заметим, что как ни крути, матрица линейного отображения должна хранить информацию о характеристическом многочлене, в частности -- о его корнях. Итого, не менее чем $k=\dim V$ параметров должно остаться. Наше чувство прекрасного или же чувство лени подсказывает нам, что самая удобная форма матрицы  -- это диагональная матрица:
$$\pmat \lambda_1 & 0 &\dots & 0\\
0 & \lambda_2 & &0 \\
\vdots && \ddots &\vdots \\
0 & \dots & 0 & \lambda_k \epmat$$

В такой системе координат матрица $L^n$ будет иметь вид:
$$\pmat \lambda_1^n & 0& \dots & 0\\
0 & \lambda_2^n & &0 \\
\vdots && \ddots &\vdots \\
0 & \dots & 0 & \lambda_k^n \epmat.$$
Теперь начнём подробный разбор:


\dfn Оператор называется диагонализуемым,  если в некотором базисе $V$ его матрица диагональна. Матрица $A\in M_n(K)$ называется диагонализуемой, если соответствующий оператор $x\to Ax$ диагонализуем. Иными словами, должна существовать обратимая $C\in M_n(K)$, что $CAC^{-1}$ -- диагональна. 
\edfn



\lm Матрица оператора $L$ в базисе $v_1,\dots,v_n$ диагональна тогда и только тогда, когда все $v_i$ -- собственные вектора $L$. В этом случае на диагонали матрицы стоят собственные числа оператора $L$.
\proof Если $Lv_i=\lambda_iv_i$, то в $i$ столбце $\lambda_i$ стоит на диагонали, а остальное -- $0$.
\endproof
\elm



\lm Пусть $v_1,\dots,v_n$ собственные вектора $L$ c собственными числами $\lambda_1,\dots,\lambda_n$. Пусть числа $\lambda_i$ попарно различны. Тогда $v_1,\dots,v_n$ линейно независимы.
\proof Пусть есть нетривиальная линейная комбинация $$\sum c_i v_i=0,$$
состоящая из минимально возможного числа векторов (пусть они нумеруются от 1 до $k$). Это означает, что все $c_i\neq 0$. Тогда, применив отображение $L$ получаем
$$0=L\left(\sum c_i v_i\right)= \sum c_i \lambda_i v_i$$
Умножая на $\lambda_1$ и вычитая получаем $$0=\sum_{i=2}^k c_i(\lambda_1-\lambda_i)v_i$$
Если в последней сумме есть хоть одно ненулевое слагаемое, то приходим к противоречию. но это бывает только если $\lambda_i=\lambda_1$ для всех $i$, что невозможно из условия леммы.
\endproof
\elm

\dfn[Алгебраическая и геометрическая кратность] Пусть $L$ оператор на пространстве $V$. Алгебраической кратностью собственного числа $\lambda$ называется его кратность как корня характеристического многочлена $\chi_L(t)$. Геометрической кратностью $\lambda$ называется размерность $\Ker L-\lambda \id$.
\edfn

\lm Пусть $L$ -- линейный оператор на пространстве $V$, а $\lambda$ -- его собственное число. Тогда алгебраическая кратность $\lambda$ не меньше его геометрической кратности.
\elm
\proof Пусть $k$ -- это геометрическая кратность $\lambda$. Тогда есть $k$ линейно независимых собственных векторов $e_1,\dots,e_k$ с собственным числом $\lambda$. Их можно дополнить до базиса. Полученная матрицы оператора $L$ будет иметь блочный вид, в первом блоке которого будет стоять матрица $\lambda E_{k}$. Следовательно характеристический многочлен делится на $(t-\lambda)^k$. Таким образом, алгебраическая кратность не меньше $k$.
\endproof

\rm Несложно найти матрицу. с разными алгебраической и геометрической кратностями для собственных чисел:
$$\pmat \lambda & 1 \\ 0& \lambda \epmat.$$
Собственное число $\lambda$ имеет алгебраическую кратность $2$, но геометрическую кратность $1$.
\erm



\thrm[Критерий диагонализуемости] Пусть $K$ -- поле. Пусть все корни $\chi_L(t)$ лежат в $K$.  Тогда оператор $L$ диагонализуем в том и только том случае, когда для всякого собственного числа $\lambda$ его алгебраическая кратность равна его геометрической кратности.
\ethrm
\proof Пусть $L$ диагонализуем. Рассмотрим его базис из собственных векторов и диагональную матрицу $L$ в этом базисе. Заметим, что $\dim \Ker L- \lambda \id $ совпадает с количеством  $\lambda$ на диагонали у этой матрицы, что, с другой стороны, равно алгебраической кратности собственного числа $\lambda$.

Обратно. Сумма алгебраических кратностей $k_i$ собственных чисел равна $$\sum k_i=\deg \chi_L(t)=n=\dim V.$$
Вспомним, что по условию $k_i=\dim \Ker L - \lambda_i \id$. По условию для каждого собственного числа $\lambda_i$, можно выбрать $k_i$ линейно независимых собственных векторов $v_{i1},\dots,v_{ik_i}$.  Если объединить эти наборы, то будет ровно $n$ штук векторов. Покажем их линейную независимость.
Рассмотрим их нетривиальную линейную комбинацию $$0=\sum_{\substack{1\leq i \leq s \\ 1\leq j\leq k_i}} c_{ij} v_{ij}.$$
Разобьём эту сумму сгруппировав собственные векторы с одним и тем же собственным числом. Сумма собственных векторов с одним и тем же собственным числом есть либо собственный вектор с этим же числом, либо 0. Если какое-то слагаемое $\sum_{1\leq j \leq k_i}c_{ij} v_{ij}=0$, то по независимости векторов $v_{ij}$ при фиксированном $i$ получаем, что $c_{ij}=0$. Такие слагаемые можно выкинуть из суммы. Если слагаемых не осталось, значит все коэффициенты с самого начала были нулевыми. 

Пусть в сумме по $i$ есть ненулевые слагаемые -- то есть, слагаемые вида $\sum_{1\leq j \leq k_i}c_{ij} v_{ij}$.  Это собственные векторы для различных собственных чисел $\lambda_i$. Значит их сумма не может быть равна нулю по предыдущей лемме.

Так как число векторов $v_{ij}$ равно размерности пространства $V$, то  из линейной независимости следует, что они образуют базис $V$.
\endproof



\crl Пусть $K$ -- алгебраически замкнутое поле. Если характеристический многочлен не имеет кратных корней, то оператор $L$ диагонализуем.
\ecrl
\proof В этом случае алгебраическая кратность совпадает с геометрической, потому что обе они равны~1.
\endproof






\crl \label{gprog}
Пусть дана последовательность $x_n\in \mb C$ удовлетворяющая линейному рекуррентному соотношению 
$$x_{n+k}+a_{k-1}x_{n+k-1}+\dots+a_0x_n=0,$$
где $a_i \in \mb C$. Рассмотрим многочлен $f(t)=t^k+a_{k-1}t^{k-1}+\dots+a_0$. Пусть у $f(t)$ нет кратных корней. Тогда $x_n=c_1 \lambda_1^n+\dots+c_k\lambda_k^n$, где $\lambda_i$-- корни $f(t)$.
\ecrl

\proof  Пусть $V$ -- пространство последовательностей, заданных линейным рекуррентным соотношением $x_{n+k}+a_{k-1}x_{n+k-1}+\dots+a_0x_n=0$. Оно имеет размерность $k$: в качестве базиса можно взять последовательности, которые начинаются с вот таких вот упорядоченных $k$-шек:
$$(0,\dots,1,\dots,0).$$
Посмотрим как на них действует оператор сдвига: последовательность с началом $(0,\dots,1,\dots,0)$, где $1$ стоит на позиции $1\leq i\leq k$ переходит в последовательность с началом $(0,\dots,1,\dots, -a_{i-1})$, где $1$ стоит на позиции $i-1$, либо $1$ вообще нет, если $i=1$.
Таким образом, матрица оператора сдвига имеет вид: 
$$ \pmat 
&1&& \\
&&\ddots&\\
&&&1 \\
-a_0 & -a_1& \dots & -a_{k-1}
\epmat $$
Но это просто транспонированная матрица к присоединённой для многочлена $f(t)$. Её характеристический многочлен равен $(-1)^nf(t)$. Многочлен $f(t)$ не имеет кратных корней.  По критерию диагонализуемости у оператора сдвига есть базис из собственных векторов, с собственными числами $\lambda_i$ -- корнями $f(t)$. Но собственные вектора для оператора сдвига -- это геометрические прогрессии. Значит в $V$ есть базис из геометрических прогрессий и частные этих прогрессий -- это $\lambda_i$.
\endproof

Даже если все корни характеристического многочлена матрицы  лежат в данном поле, это не значит, что матрица диагонализуется: вот простейший пример 
$$\pmat 0&1\\ 0&0 \epmat.$$

Теперь мы так же можем отчасти ответить на самый первый вопрос: если две матрицы (над $\mb C$) имеют одинаковый характеристический многочлен, и у него нет кратных корней, то они подобны: действительно, они обе подобны диагональной матрице, у которой на диагонали стоят собственные числа их характеристического многочлена.

С другой стороны, если кратные собственные числа есть, то ситуация усложняется. Вот две матрицы, имеющие одинаковый характеристический многочлен, но не подобные:
$$\pmat 0&1\\ 0&0 \epmat\,\,\,\,\, \pmat 0&0\\ 0&0 \epmat.$$
Первая недиагонализуема (по критерию), а вторая уже диагональна.


\section{Подготовка}

Прежде чем дать общий ответ на вопрос о том, что может происходить в случае, если оператор не диагонализуется обсудим несколько лемм и конструкций:



\lm Пусть $L$ -- оператор на пространстве $V$, а  многочлен $g(t)=p(t)q(t)$ аннулирует $L$ (то есть $g(L)=0$). Причём $(p(t),q(t))=1$. Тогда подпространство $V$ раскладывается в прямую сумму инвариантных подпространств
$$V = \Ker p(L)\oplus \Ker q(L).$$
\elm
\proof Рассмотрим линейное разложение $1=a(t)p(t)+b(t)q(t)$. Тогда любой вектор $v$  представим в виде
$$v=a(L)p(L)v+ b(L)q(L)v.$$
Тогда $q(L)a(L)p(L)v=0=p(L)b(L)q(L)v$. Следовательно, $V= \Ker p(L)+\Ker q(L)$. Покажем, что ядра пересекаются по нулю. Пусть $v\in \Ker p(L) \cap \Ker q(L)$. Тогда $v=a(L)p(L)v+ b(L)q(L)v=0$.
\endproof

\upr Как мы уже отмечали, если пространство разложилось в сумму двух $V=U_1\oplus U_2$, то есть оператор проекции, который вектору $v=u_1+u_2$ сопоставляет $u_1$ -- его $U_1$ компоненту. Это отображение называется проекцией на $U_1$ вдоль $U_2$.
Где в доказательстве всплыл оператор проектирования на $\Ker p(L)$ вдоль $\Ker q(L)$?
\eupr


Начнём с того, почему разложение пространства в прямую сумму инвариантных так удобно:

\utv Пусть $L$ -- оператор на $V$, пространство $V=U_1\oplus U_2$, где $U_1,U_2$ инвариантны. Если $e_1,\dots,e_k$ и $f_1,\dots,f_l$ -- базисы $U_1$ и $U_2$, то матрица $L$ в базисе $e_1,\dots, f_l$ имеет вид
$$\pmat A_1 & 0 \\ 0 & A_2 \epmat.$$
Угадайте, что это за матрицы $A_1$ и $A_2$.
\eutv




Прекрасно, когда всё пространство раскладывается в прямую сумму двух инвариантных подпространств. Но так тоже не всегда бывает. Вот более общая конструкция. Надеюсь, за ней вы увидите ранее знакомый алгебраический принцип:

\dfn Пусть $U$ -- подпространство в $V$. Определим на факторе $V/U$ структуру векторного пространства следующим $\lambda \ovl{v}=\ovl{\lambda v})$.
\edfn

\upr Проверьте, что это действительно векторное пространство.
\eupr

\upr Как найти базис $V/U$ зная базис $U$ и базис $V$?
\eupr

\upr Рассмотрим пространство $V=\mb R^4$ и в нём подпространства $U=\lan (1,1,1,1)^\top, (2,2,1,1)^\top \ran$. Найдите базис факторпространства.
\eupr


\dfn Пусть $V$ -- пространство с оператором $L$, а $U$ -- инвариантное подпространство. Тогда определим оператор $\ovl{L}$ на $V/U$ следующим образом:
$$\ovl{L}(\ovl{v})=\ovl{L(v)}.$$
\edfn

\rm Разумеется, вычислять многочлен от оператора на факторпространстве можно на представителях. Формально, если $p(x)$ -- многочлен, а $v\in V$, то $p(\ovl{L})\ovl{v}=\ovl{p(L)v}$.
\erm

\rm Мы уже знаем, что инвариантное подпространство приводит к тому, что в подходящем базисе матрица линейного оператора становится блочно-верхнетреугольной и верхний блок -- это матрица сужения оператора. Дадим интерпретацию нижнего правого блока:  Пусть $e_1,\dots,e_n$ базис $V$ и $\lan e_1,\dots,e_k\ran$ -- инвариантное пространство относительно $L$. Если  матрица $L$ в этом базисе имеет вид $$\pmat A& B \\ 0 & C\epmat,$$
то $C$ -- это матрица $\ovl{L}$ в базисе
$\ovl{e_{k+1}},\dots,\ovl{e_n}$.
Следовательно, $$\chi_L(t)=\chi_{L|_{V'}}(t)\cdot \chi_{\ovl{L}}(t).$$
\erm

Итак, если мы нашли инвариантное подпространство $U$ внутри $V$ относительно $L$, то исследование $L$ в некотором смысле сводится к исследованию $L|_U$ на  $U$ и $\ovl{L}$ на $V/U$. Покажем пример, как это работает.


\thrm[Теорема Гамильтона-Кэли] Пусть $L$ -- оператор на $V$. Пусть многочлен $\chi_L(L)$ раскладывается в $K$ на линейные множители. Тогда $\chi_L(L)=0$.
\ethrm
\proof Докажем по индукции. Случай $\dim V=1$ ясен. Шаг. Так как $K$ алгебраически замкнуто, то у характеристического многочлена есть корень $\lambda_1$ и собственный вектор $e_1$. Рассмотрим фактор $V/\lan e_1\ran$. Для него теорема выполнена. Заметим, что $$\chi_L(t)= -(t-\lambda_1)\chi_{\ovl{L}}(t).$$
Пусть $v \in V$. Тогда $\chi_{\ovl{L}}(L)v = ce_1$ так как в факторе этот элемент равен 0. Но тогда
$$\chi_L(L)v= -(L-\lambda_1 E)\chi_{\ovl{L}}(L)v=-(L-\lambda_1 E) ce_1=0$$
\endproof

\upr Избавьтесь от предположения о том, что многочлен раскладывается на линейные множители (используя какой-нибудь  алгебраический трюк).
\eupr


\rm Теорему Гамильтона-Кэли очень легко доказать, когда $L$ -- диагонализуем.
\erm


\section{Жорданова форма}

В этом разделе я буду предполагать, что  поле $K$ алгебраически замкнуто. Ровно те же результаты можно получить, предполагая, что поле $K$ содержит все корни характеристического многочлена рассматриваемого оператора. В этом разделе мы получим полную классификацию операторов над алгебраически замкнутым полем $K$, то есть для каждого оператора мы построим некоторую матрицу -- его каноническую <<модель>> и научимся строить базис, в котором матрица оператора -- это его <<модельная>> матрица.

\dfn
Матрица $k\times k$
$$J_k(\lambda) = \begin{pmatrix}
\lambda& 1&& \\
& \lambda &1& \\
&&\ddots &\ddots& \\
&  && \lambda & 1\\
&  &&& \lambda
\end{pmatrix}
$$
называется жордановой клеткой размера $k$ с собственным числом $\lambda$.
\edfn

\rm Заметим, что для того, чтобы в базисе $e_1,\dots,e_n$ матрица оператора $L$ была жордановой клеткой необходимо и достаточно, чтобы $(L-\lambda E)e_i=e_{i-1}$ для $i\geq 2$ и $(L-\lambda E)e_1=0$. В частности, оператор $L-\lambda E$ должен быть нильпотентным.
\erm







\thrm Пусть $L\colon V \to V$ --- оператор на конечномерном пространстве над алгебраически замкнутым полем $K$.
Тогда существует базис $e_1,\dots, e_n$ в котором матрица $L$ имеет вид
$$A=\begin{pmatrix}
J_{k_1}(\lambda_1) &&&\\
& J_{k_2}(\lambda_2) &&\\
&& \ddots& \\
&&& J_{k_s}(\lambda_s)

\end{pmatrix}.
$$

Более того, такая матрица единственна с точностью до перестановки блоков. Эта матрица называется матрицей оператора в форме Жордана. Базис, в котором матрица оператора имеет такой вид называется жордановым базисом.
\ethrm
\proof

Сначала докажем единственность. Прежде всего, если нам дана матрица в жордановой форме, то мы легко можем вычислить её характеристический многочлен. Он равен $\prod (t-\lambda_i)$ где $\lambda_i$ -- все числа на диагонали, откуда сразу становится ясно, что $\lambda_i$ -- собственные числа $L$. Более того, алгебраическая кратность
$k$ собственного числа $\lambda$ равна сумме размеров клеток с этим собственным числом.

Пусть сами размеры клеток для собственного числа $\lambda$ заданы как набор чисел $h_i$. Имеем
$\sum h_i =k.$
Итак, набору клеток соответствует разбиение числа $k$ в сумму некоторого числа слагаемых. Удобно упорядочить эти слагаемые по возрастанию $h_i\geq h_{i+1}$.

Можно ли как-то лучше визуализировать себе структуру жордановой формы?
Каждому разбиению числа на слагаемые однозначно соответствует картинка:


\begin{figure}[hhh]
\begin{center}
\begin{tikzpicture}[xscale=0.7, yscale=0.7]
\draw (1,0) -- (1,3) -- (0,3) -- (0,0) -- (4,0) -- (4,1) -- (0,1);
\draw (0,2) -- (3,2) -- (3,0);
\draw (2,0) -- (2,2);


\end{tikzpicture}
\end{center}
\caption{$8=3+2+2+1$, соответствует одной клетке размера 3, двум клеткам размера 2, одной клетке размера 1}
\end{figure}

А именно, сопоставим каждому слагаемому $h_i$ столбик высоты $h_i$. Такие (обычно, правда, перевёрнутые) картинки называются диаграммами Юнга.

Как же восстановить эту картинку зная оператор $L$ и собственное число $\lambda$?

Рассмотрим оператор $N=L-\lambda E$. Понятно, что сколько клеток с $\lambda$ было у $L$ в указанном базисе, столько же клеток с с.ч. 0 будет и у $N$ и они будут такого же размера.
Заметим, что жордановы клетки с собственным числом 0 являются нильпотентными матрицами.



Вспомним, что каждой клетке соответствует кусок базиса из векторов вида  $v_i, N v_i, \dots,N^{h_i} v_i$. Заметим, что эти вектора образуют базис пространства $\Ker N^k$. Действительно, $N^k$ обнуляется на векторах $v_i$ и их образах. С другой стороны $N^k$ обратим на дополнительном слагаемом.

Пририсуем эти вектора к нашей картинке следующим образом -- поместим $v_i$ наверху соответствующего клетке  столбика, $N v_i$ на ступень ниже и т.д. Таким образом заполним все ячейки диаграммы. При действии оператора $N$ на диаграмму происходит следующее -- все вектора съезжают на единицу вниз, кроме самых нижних, которые переходят в $0$.
\begin{figure}[hhh]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1]
\draw (1,0) -- (1,3) -- (0,3) -- (0,0) -- (4,0) -- (4,1) -- (0,1);
\draw (0,2) -- (3,2) -- (3,0);
\draw (2,0) -- (2,2);
\draw[->] ( -0.5, 2.5) -- (-0.5, 0.5);
\node at (0.5, 2.5) {$v_1$};
\node at (0.5, 1.5) {$Nv_1$};
\node at (0.5, 0.5) {$N^2 v_1$};
\node at (1.5, 1.5) {$v_2$};
\node at (1.5, 0.5) {$Nv_2$};
\node at (2.5, 1.5) {$v_3$};
\node at (2.5, 0.5) {$Nv_3$};
\node at (3.5, 0.5) {$v_4$};
\node at (-1, 1.5) {$N$};
\end{tikzpicture}
\end{center}
\caption{$8=3+2+2+1$, расставляем базисные вектора}
\end{figure}
Итого, количество ячеек в диаграмме Юнга для собственного числа $\lambda$ оператора $L$ на высоте не более $s$ равно $\dim \Ker(L - \lambda E)^s $.
Это  позволяет однозначно восстановить разбиение числа и, следовательно, конфигурацию клеток, если мы знаем числа $\dim \Ker(L - \lambda E)^s $.

Точнее, число ячеек в строке уровня $s$ равно $\dim \Ker(L - \lambda E)^s -\dim \Ker(L - \lambda E)^{s-1} $.







\proof[Существование]


Для начала надо разбить всё пространство на куски с одним и тем же собственным числом. По теореме Гамильтона-Кэли оператор $L$ аннулируется многочленом $\chi_L(t)$. Разложив последний на простые множители $\chi_L(t)=\prod (x-\lambda_i)^{\alpha_i}$ получим разложение $V=\bigoplus V_i$ на примарные инвариантные подпространства $V_i=\Ker (L-\lambda_i E)^{\alpha_i}$. Ограничимся на пространства $V_i$. Эти пространства называются корневыми. Заметим, что оператор $N=L-\lambda_i E$ нильпотентен. Осталось применить следующую  лемму к операторам $N|_{V_i}$.

\lm[Основная] Для любого нильпотентного оператора $N$ на пространстве $V$ существует базис $e_1,\dots,e_n$ в котором матрица $N$ имеет вид
$$A=\begin{pmatrix}
J_{k_1}(0) &&&\\
& J_{k_2}(0) &&\\
&& \ddots& \\
&&& J_{k_s}(0)
\end{pmatrix}.
$$
\elm
\proof
Докажем по индукции. Если $V=\Ker N$, то матрица просто 0 и всё доказано. Иначе рассмотрим пространство $\Ker N$. Тогда для $V/\Ker N$ теорема верна. Рассмотрим требуемый базис $V/\Ker N$ $\ovl{\ovl{e_{ij}}}$, где первый индекс обозначает номер клетки $1\leq i \leq s$, а второй -- номер вектора(сверху вниз) в  диаграмме Юнга для $V/\Ker N$ -- $0\leq j\leq h_i-1$.
Рассмотрим вектора $e_{i0}$, которые лежат в классе $\ovl{\ovl{e_{i0}}}$ и определим $$e_{ij}=N^{h_i-j}(e_{ih_i}),$$ где  $j\in \ovl{0,h_i}$. Имеет место равенство $$\ovl{e_{ij}}=\ovl{\ovl{e_{ij}}}.$$
Рассмотрим набор векторов $e_{i,h_i}$ -- это вектора из $\Ker N$.
Покажем, что они линейно независимы и, в частности, не 0. Пусть $\sum_{i=1}^s c_ie_{i,h_i}=0$. Это значит $N(\sum_{i=1}^s c_i e_{i,h_i-1} )=0$. Тогда $\sum_{i=1}^s c_i e_{i,h_i-1} $ лежит в ядре $N$. Но в этом случае $\sum c_i \ovl{e_{i,h_i-1}}=0$ в $V/\Ker N$, чего не может быть, так как они там линейно независимы.

Теперь дополним набор $e_{1,h_1},\dots,e_{s,h_s}$ до базиса $\Ker N$  элементами $e_{s+1,0},\dots,e_{k,0}$. Я утверждаю, что  дополненный набор 
$$e_{ij}, \text{ где } 1\leq i\leq k, \,\, 0\leq j\leq h_i (\text{ при } j\leq s) \text{ и } j=0 \text{ иначе }$$
является нужным базисом. Для этого необходимо показать, что он базис. Рассмотрим отображение $V \to V/\Ker N$. Тогда часть этого набора образует базис образа, а оставшаяся часть -- базис ядра. Тогда размерность пространства, порождённого этими векторами равна $\dim \Ker N + \dim V/\Ker N = \dim V$. Откуда получаем, что это базис.

\endproof

\endproof








Допустим мы нашли характеристический многочлен, то есть все его собственные числа. Далее нашли размерности $\dim \Ker(L-\lambda E)^s$. Как теперь найти сам жорданов базис? Для этого нам необходимо заполнить верхушки всех столбцов, остальное заполнится автоматически.



Как расставить векторы в верхней строке диаграммы? Векторы $v_{i_1}, \dots, v_{i_s}$  в верхней строке определяются тем, что их образы при $(L-\lambda E)^{k-1}$ линейно независимы (в частности, не лежат в ядре). Или, (что эквивалентно) система $v_{i_1}, \dots, v_{i_s}$ вместе с базисом (любым) $\Ker (L-\lambda E)^{k-1}$ образуют линейно независимую систему. Напомню, что их число равно
$$s=\dim \Ker (L-\lambda E)^k - \dim \Ker (L-\lambda E)^{k-1}.$$



Что делать с теми клетками, чьи столбики в диаграмме Юнга начинаются не на самом верху? Пусть мы уже заполнили все строки на высоте больше $i$. Заполним остаток строки на высоте $i$.  Очевидно, что оставшиеся векторы лежат в ядре $(L-\lambda E)^{i}$ и при этом их образы при $(L-\lambda E)^{i-1}$ линейно независимы. Однако вектора из уже заполненных клеток на уровне $i$ тоже подходят под это описание. Можно однако заметить, что образы системы <<старые вектора на уровне $i$>>, <<новые вектора на уровне $i$>> при $(L-\lambda E)^{i-1}$ линейно независимы все вместе. Это даёт необходимые условия на оставшиеся вектора в строке $i$.

\lm Выполнено равенство:
$$J_k(\lambda)^n= \pmat \lambda^n & n\lambda^{n-1} & \dots & C_n^{k-1}\lambda^{n-k+1}\\
 &  \lambda^n & &\vdots \\
 &            & \ddots & n\lambda^{n-1}\\
 &&&  \lambda^n \epmat,$$
\elm
\proof Жорданова клетка представима в виде суммы $J_k(\lambda)= \lambda E + N$, где $N$ -- нильпотентная матрица, причём $N^k=0$. На самом деле степени $N$ выглядят следующим образом:
$$N^l= \pmat  & &0& 1& \dots &0 \\
   & && \ddots &\ddots& \vdots\\
 &&&&\ddots& 1\\
 &&&&& 0 \\
 &&&&&  \epmat $$
Теперь
$$(\lambda E+N)^n= \sum_{i=0}^{k-1} C_n^i\lambda^{n-i}N^i,$$
что и даёт требуемое.
\endproof

\crl Пусть $A$ -- матрица из $M_n(K)$. Тогда существует такая обратимая матрица $C$, что $A^n=CJ^nC^{-1}$, где $J$ -- жорданова форма $A$. Причём, матрица $J^n$ составлена из блоков как в предыдущей лемме.
\ecrl
\proof Достаточно взять матрицу $C$ равной матрице перехода из стандартного базиса в жорданов базис для оператора, заданного $A$.
\endproof

\crl Для всякой матрицы $A$ коэффиент её степени $A^n$ есть сумма последовательностей вида $C_n^s\lambda^{n-s}$ с независящими от $n$ коэффициентами. Здесь $\lambda$ -- произвольное собственное число $A$, а $s$ --  меньше, чем максимальный размер жордановой клетки с собственным числом $\lambda$.
\ecrl



\crl Пусть дана последовательность $x_n\in \mb C$ удовлетворяющая линейному рекуррентному соотношению 
$$x_{n+k}+a_{k-1}x_{n+k-1}+\dots+a_0x_n=0,$$
где $a_i \in \mb C$. Рассмотрим многочлен $f(t)=t^k+a_{k-1}t^{k-1}+\dots+a_0$. Тогда $x_n$ есть линейная комбинация последовательностей вида $n^s\lambda^n$, где $\lambda$ -- корень $f(t)$, а $s$ -- строго меньше, чем кратность $\lambda$, как корня $f(t)$.
\ecrl
\proof Как мы помним, матрица оператора сдвига на $V$ -- пространстве последовательностей с заданным линейным рекуррентным соотношением -- имеет характеристический многочлен равный $\pm f(t)$. Пусть $A$ -- матрица оператора сдвига (в стандартном для этого пространства базисе см. следствие \ref{gprog}). Тогда все коэффициенты $A^n$ есть суммы $C_n^s\lambda^{n-s}$.

Если $v$ -- начальный вектор для последовательности, то верхний элемент столбца $A^nv$ равен  $x_n$. Значит последовательность $x_n$ есть сумма последовательностей вида $C_n^s\lambda^{n-s}$ с какими-то коэффициентами. $C_n^s$ -- это многочлен степени $s$ от $n$. Разложив все такие слагаемые в сумму мономов получаем требуемое.
\endproof

\rm[Дополнительно] Последовательностей $C_n^s\lambda^{n-s}$ ровно $k$ штук. Любой элемента $k$-мерного пространства лежит в пространстве, порождённом $C_n^s\lambda^{n-s}$. Значит последовательности $C_n^s\lambda^{n-s}$ лежат в $V$ и являются базисом этого пространства.
\erm

\upr Покажите, что $C_n^s\lambda^{n-s}$ -- это жорданов базис оператора сдвига.
\eupr


\thrm
Пусть $L$ -- оператор на векторном пространстве $V$ над полем характеристики $0$. Тогда матрица оператора $p(L)$ в жордановом базисе $L$ составлена из блоков вида
$$ \pmat p(\lambda) & p'(\lambda) & \dots & \frac{p^{(k-1)}(\lambda)}{(k-1)!}\\
 &  p(\lambda) & &\\
 &            & \ddots & \\
 &&&  p(\lambda) \epmat,$$
где $\lambda_i$ собственные числа, а число и размер блоков c $\lambda_i$ равны числу и размеру блоков в жордановой форме $L$.
\proof Благодаря линейности по многочлену, достаточно проверить равенство только для возведения $L$ в степень. 
\endproof
\ethrm

\crl Пусть $A$ матрица, тогда $p(A)=C p(J) C^{-1}$, где $p(J)$ составлена из блоков как выше, а $C$ составлена из жорданового базиса для $A$.
\proof Если $J$ -- жорданова форма для матрицы $A$, то $A=CJC^{-1}$, где матрица $C$ составлена из собственных векторов $A$.
\endproof
\ecrl

Для дальнейшего нам понадобится понятие аналитической функции. Это понятие из математического анализа.


\dfn Пусть $D\subseteq K$ -- открытый диск с центром в точке $z_0$ радиуса $r>0$ в $K$, где $K$ -- это либо $\mb C$, либо $\mb R$ (то есть круг или интервал). Будем говорить, что функция $f\colon D \to K$ аналитична, если существует последовательность $a_n
\in K$, что $f(z)=a_0+a_1(z-z_0)+\dots+ a_n(z-z_0)^n+\dots$ для любого $ z\in D$. 
\edfn

\exm\\
1) $e^z$ на всём $\mb C$ или на всём $\mb R$\\
2) Да и вообще, любая элементарная функция на любом открытом диске в области определения.

\fct Если $f(z)$ аналитична в $D$, то ряд $a_1+2a_2(z-z_0)+\dots+na_n(z-z_0)^{n-1}+\dots$ сходится в $D$ и равен $f'(z)$. То есть производная существует и аналична в том же диске.
\efct

\dfn Пусть $A$ квадратная матрица над $K=\mb C$ или $\mb R$.  Пусть $f(z)$ -- аналитическая функция в диске $D$, а все собственные числа $A$ так же лежат в $D$. Тогда определим 
$$f(A)=a_0+a_1(A-z_0E)+\dots+a_n(A-z_0E)^n+\dots,$$
относительно покоэффициентной сходимости на $M_n(K)$.
\edfn






\rm  В этом случае матрицу $f(A)$ корректно определена и её можно посчитать следующим образом
$$f(A)= C f(J) C^{-1}.$$
\erm

\rm Особенную популярность функция от матриц получает при решении системы линейных однородных дифференциальных уравнений 
$$x'(t)=Ax(t).$$
Решение даётся в виде $e^{At}C_0$, где $C_0$ -- вектор начальных данных. Это корректно определённое выражение так как ряд для экспоненты сходится везде.
\erm

Есть возможность избежать вычисления самой жордановой формы, обойдясь вычислением характеристического многочлена для того, чтобы посчитать функцию от матрицы. Разберёмся со случаем многочлена от матрицы. 

Допустим мы хотим посчитать $p(A)$, где $p(x)$ -- многочлен c коэффициентами $K$. Рассмотрим $\chi_A(t)$. Допустим нам удалось найти остаток $$p(t)=q(t)\chi_A(t)+r(t).$$
Подставим $A$ и воспользуемся теоремой Гамильтона-Кэли. Тогда
$$p(A)=r(A).$$
Например, для вычисления $A^n$ необходимо знать $x^n \mod \chi_A(t)$. Такой подход может дать сокращение в матричных умножениях, когда $n$ велико. Например, если вы хотите вычислить $n$-ый член последовательности, удовлетворяющей линейному рекуррентному соотношению можно посчитать $r(x)\equiv x^n \mod f(x)$, а потом посчитать начальный член последовательности  $r(L)v$, где $L$ -- оператор сдвига. Вычислить степени оператора сдвига легко -- сдвинуть последовательность несколько раз.


\upr Предложите способ вычисления характеристического многочлена. 
\eupr


\utv Пусть $A$ -- вещественная (или комплексная) матрица с собственным числом $\lambda_1=1$ кратности 1, а все остальные собственные числа $A$ по модулю строго меньше 1. Если вектор $v= \sum c_i e_i$, где $e_i$ жорданов базис, то $$\lim_{n \to \infty}A^nv= c_1 e_1.$$
\eutv

Где мы видели такие матрицы?


\section{Дополнительно: неотрицательные матрицы и теория Перрона-Фробениуса}
\dfn

В каких задачах нам может пригодиться наблюдение про предельный переход? Вспомним старые примеры: для каждого графа $G$ можно построить  несколько  различных матриц, которые кодируют его структуру. Прежде всего это три квадратные матрицы  размера $n\times n$, где $n$ -- это число вершин $G$. 
Первая -- матрица смежности  $A(G)$, которая полностью определяет граф $G$
$$a_{ij}=\begin{cases} 1, \text{ если вершины $i$ и $j$ соединены ребром}\\
0, \text{ иначе }
\end{cases}.$$

Так же нам уже встречалась матрица случайного блуждания  $P(G)$

$$P(G)_{ij}=\begin{cases}
\frac{1}{d_j}, \text{ если есть ребро $j\to i$}\\
1, \text{ если $i=j$ и из вершины $j$ не исходит рёбер} \\
0, \text{ иначе }
\end{cases}.$$
\edfn

Выражение  $P_G^n v$ вычисляет распределение после $n$ шагов случайного блуждания, если начальное распределение было равно $v$. След матрицы $A(G)^n$ считает количество циклов длины $n$ в графе $G$. Кроме того, спектр графа не зависит от порядка вершин графа и значит по нему можно указать, что два графа не изоморфны.

Не остановимся на этом. Это, не единственные примеры, где нужно знать собственные числа матрицы. Вот ещё один: модель Лесли для распределения по возрастам в популяции.

Зададимся следующим вопросом как можно промоделировать эволюцию распределения людей по возрастам? Прежде всего необходимо завести разбиение людей на группы $F_i$ -- одного возраста. $F_i$ можно выбрать, например, группой людей с одинаковым годом рождения. Или $F_i$ может быть группой людей, родившихся в определённое десятилетие. Для каждой  группы мы можем посчитать два параметра: $f_i$ -- ожидаемое количество потомства от члена группы $F_i$ за выбранный временной промежуток; $s_i$ -- процент от общего числа индивидов группы $F_i$ которые выжили за фиксированный промежуток времени и перешли в группу $F_{i+1}$. Пусть $n_1,\dots,n_k$ -- количества индивидов в группах $F_1,\dots,F_k$. Тогда для тех же самых количеств но в следующий промежуток времени имеет место место соотношение:
$$\pmat n_1 \\ \vdots \\ n_k \epmat_{new}=
\pmat f_1 & \dots &f_{k-1} & f_k \\ 
s_1 && &\\
& \ddots & \\
& &s_{k-1} & \epmat \pmat n_1 \\ \vdots \\ n_k \epmat$$
Мы представляем себе, что популяция в целом может расти и убывать. Так же логично предположить, что при стабильных условиях (то есть тогда, когда коэффициенты $f_i$ и $s_i$ не зависят от времени) наблюдается некоторое равновесие. Точнее распределение населения по возрастам должно стабилизироваться. Это означает, что определённый процент популяции составляют старики, определённый процент -- дети и т.д.
Что всё это означает на матричном языке? Прежде всего ясно, что речь идёт о предельном поведении  последовательности векторов $A^n v_0$, где $A$ -- это матрица Лесли, а $v_0$ -- начальное состояние. 
Сделаем несколько предположений про матрицу $A$. Первое предположение -- у матрицы $A$ есть положительное вещественное число $\lambda$, которое больше по модулю всех остальных  собственных чисел (над $\mb C$).  Так же, будем считать, что это собственное число $\lambda$ не кратное (речь об алгебраической кратности) и собственный вектор для этого числа можно выбрать с положительными компонентами.

В этой ситуации оказывается, что $\lambda$ -- это есть скорость роста, а соответствующий ему собственный вектор $e_1$ отвечает за распределение популяции. Точнее, пусть $e_1,\dots,e_k$ -- жорданов базис для $A$, $e_1$ -- тот самый собственный вектор для $\lambda$. Пусть вектор $v_0$ имеет столбец координат $x=(x_1,\dots,x_k)^\top$ относительно базиса $e$. Последнее предположение состоит в том, что $x_1$ -- коэффициент при $e_1$ не равен нулю. Если мы хотим узнать координаты $A^n v_0$ в жордановом базисе, то нужно найти произведение 
$$J^n x= \pmat 
\lambda^n &&&\\
& \lambda_2^n & O(n\lambda_2^n)&\\
&& \ddots \\
&&& \lambda_k^n 
\epmat
\pmat x_1 \\ \vdots \\ x_k\epmat$$
По нашему предположению все остальные $\lambda_i$ по модулю меньше $\lambda$. Тогда видно, что первая координата есть $\lambda^n x_1$, а остальные есть $o(\lambda^n)$. Это означает, что $A^n v_0= x_1 \lambda^n e_1 + o(\lambda^n)$. Значит, за один шаг размер популяции в пределе меняется в $\lambda$ раз, а предельное соотношение координат для $A^n v_0$ такое же, как у вектора $e_1$. Такое отношение существует, потому что компоненты $e_1$ все не равны нулю.


Итого для понимания того, как устроена последовательность $A^nv$, необходимо представлять себе как устроены собственные числа матрицы $A$ и её собственные вектора. В связи с этим возникает несколько вопросов:\\
1) Нас интересует максимальное по модулю собственное число. Хочется, чтобы это число было вещественным и положительным. Когда это выполнено?\\
2) Какова алгебраическая кратность максимального по модулю вещественного собственного числа (если оно есть)?\\
3) Есть ли другие собственные числа, равные по модулю максимальному?\\
4) В указанных задачах хотелось бы, чтобы собственный вектор $v$ для максимального собственного числа был положительным? Всегда ли можно так сделать?

Понятно, что в общем случае ответ на эти четыре вопроса <<нет>> даже в рамках поставленных задач.

\exm
\enm
\item Рассмотрим граф $1 \rightarrow 2$. У матрицы $P(G)$ собственный вектор для собственного числа $1$ имеет нулевую координату.
\item Рассмотрим граф 
\begin{center}
\begin{tikzpicture}
\node (A) at (0,0) {3};
\node (B) at (1,0.5) {1};
\node (C) at (1,-0.5) {2};
\path[->,font=\scriptsize,>=angle 60]
(A) edge (B)
(A) edge (C);
\end{tikzpicture}
\end{center}
У матрицы $P(G)$ есть два собственных вектора $(1,0,0)$ и $(0,1,0)$ с собственным числом 1.
\item Рассмотрим граф $C_n$ -- ориентированный цикл длины $n$. Его спектр -- это корни степени $n$ из единицы.
\eenm



Сейчас мы докажем, что при некоторых предположениях на матрицу $A$ для неё ответы на все четыре вопроса имеют желаемый ответ. Эти предположения не будут выполнены для матриц $A(G)$ и  $P(G)$ непосредственно, однако мы тем не менее сможем извлечь пользу. Что же общего между указанными в наших примерах матрицами?

\dfn Назовём матрицу $A$ положительной (не путать с положительно определённой матрицей квадратичной формы), если все её элементы $A_{ij}$ строго положительны. Будем писать в этом случае $A>0$.
\edfn

\dfn Назовём матрицу  $A$ неотрицательной, если $A_{ij}\geq 0$. Обозначение $A \geq 0$.
\edfn



\dfn В дальнейшем нам будут удобны следующие обозначения: если $A\in M_n(\mb C)$, то $|A|$ -- это матрица составленная из $|a_{ij}|$. Про вещественные матрицы $A$ и $B$ будем говорить, что $A>B$ или $A\geq B$, если $A-B>0$ или $A-B \geq 0$ соответственно.
\edfn



\thrm[Перрон, 1907] Если матрица $A$ положительна, то наибольшее по модулю собственное число $A$ единственное и является вещественным и положительным. Это собственное число не является кратным корнем характеристического многочлена. Собственный вектор для этого собственного числа положителен.
\ethrm
\proof Пусть $\lambda$ -- наибольшее по модулю собственное число и $Ax=\lambda x$. Можно считать, что $|\lambda|=1$ разделив всю матрицу на $|\lambda|$. Тогда покажем, что $A|x|=|x|$.

Прежде всего мы имеем цепочку неравенств $|x|=|Ax|\leq |A||x|=A|x|$, где все неравенства подразумеваются покомпонентными. Обозначим за $z=A|x|$. Это вектор состоящий из положительных координат и рассмотрим вектор $y=z-|x|$. Вектор $y$ неотрицателен. При этом если $y=0$, то мы доказали то, что хотели. Предположим, что есть координата $y_i>0$. Тогда $Ay$ -- положительный вектор, то есть существует такое $\eps>0$, что $Ay>\eps z$. Распишем это неравенство: $Az - z= Az-A|x|> \eps z$ или же $\frac{A}{1+\eps}z>z$. Ввиду положительности правой и левой части мы без сомнений можем применить оператор $\frac{A^n}{(1+\eps)^n}$ к правой и левой части и получить верное неравенство. Итого имеем цепочку 
$$\frac{A^n}{(1+\eps)^n}z>\frac{A^{n-1}}{(1+\eps)^{n-1}}z> \dots > z.$$
Но оператор $\frac{A}{1+\eps}$ имеет собственные числа по модулю меньшие 1 и поэтому, предел левого выражения равен 0. Противоречие!

Итак, в частности, единица собственное число. Покажем теперь, что нет отличных от единицы собственных чисел с модулем $1$. Пусть $\lambda\in \mb C$ собственное число $A$ с $|\lambda|=1$ и $x \in \mb C^n$ -- соответствующий собственный вектор. Тогда $A|x|=|x|=|Ax|$. Заметим, что все координаты $x$ отличны от нуля. Рассмотрим $i$-ую координату. Имеем $\sum A_{ij}|x_j|=x_i=|\sum A_{ij}x_j|$. Посмотрим на это равенство как на равенство между нормой суммы и суммой норм векторов в $\mb C =\mb R^2$. Хорошо известно, что сумма норм больше или равна нормы суммы и равенство достигается тогда и только тогда, когда вектора сонаправлены. Итого все $x_i$ должны быть сонаправлены, но это означает, что $x=e^{i\ffi} |x|$ и следовательно $\lambda=1$. 

Покажем, что единица не кратный корень. Допустим противное. Есть два случая -- либо есть две клетки с собственным числом $1$, либо клетка ровно одна, но при этом размера по крайней мере $2$. Если есть две клетки, то есть два линейно независимых собственных вектора  $x_1$ и $x_2$. Тогда подберём $c$, так что $x_1-cx_2$ имеет нулевую координату. Получаем противоречие, так как $|x_1-cx_2|$ неотрицательный вектор для 1, но при этом с нулевой координатой. Осталось разобрать случай с клеткой размера $k$. В этом случае все коэффициенты матрицы $A^n$ имеют вид $cn^{k-1}+o(n^{k-1})$. При этом $c$ не всегда $0$. Значит некоторые коэффициенты $A^n$ растут при $n\to \infty$. Но тогда некоторые коэффициенты $A^nx_1=x_1$ тоже растут, что очевидно не так.
\endproof

Бывает полезно ещё одно утверждение, которое позволяет установить максимальность собственного числа для заданной неотрицательной матрицы. Оказывается, что для этого достаточно положительности собственного вектора. Нам удобно будет сформулировать и уточнить это соображение не для матрицы $A$, а для матрицы $A^\top$.

\utv Пусть $A\geq 0$, и у матрицы $A^{\top}$ есть положительный собственный вектор для собственного числа $\lambda$. Тогда $\lambda$ -- наибольшее по модулю собственное число $A$. Если у матрицы $A$ есть собственный вектор $y\geq 0$, то $y$ собственный вектор для числа $\lambda$. 
\eutv
\proof Рассмотрим матрицу $A^{\top}$. Пусть $x$ -- положительный  собственный вектор, соответствующий собственному числу $\lambda$. Пусть $\mu$ -- собственное число для собственного вектора $v$. Тогда 
$$\lambda x^{\top}|v|= x^{\top}A|v|\geq x^{\top}|\mu| |v|=|\mu| x^{\top}|v|.$$
Так как $x^{\top}|v| >0$, то $\lambda\geq |\mu|$. Если же, $y=v=|v|$, то имеет место равенство. Так как для неотрицательного вектора $y$ собственное число $\mu$ тоже неотрицательно
\endproof

\dfn Неотрицательная матрица $A\in M_n(\mb R)$ называется стохастической (по столбцам),  если сумма всех коэффициентов в каждом её столбце равна $1$ (иногда дефолтным считается условие на строки). 
\edfn

\rm Такие матрицы встречаются в теории вероятностей, когда речь идёт о марковских цепях -- процессах с дискретным временем в которых последующие события зависят только от текущего положения, а не от того, как вы в него попали.  
\erm

\crl У стохастической матрицы $A$ единица является максимальным по модулю собственным числом.
\ecrl
\proof Вектор $(1,\dots,1)$ является положительным собственным вектором для $A^{\top}$ с собственным числом $1$. Значит у матрицы $A$ есть собственное число $1$ и оно максимальное. 
\endproof


Вообще говоря матрица $P(G)$ имеет довольно много нулевых компонент. И, строго говоря, заключение теоремы Перрона не всегда верно для $P(G)$, как следует из примеров. Как же наша теория может помочь? Для этого мы схитрим и немного поменяем задачу. А именно, рассмотрим матрицу $$P_{\alpha}(G)=(1-\alpha) P(G) + \alpha\tfrac{1}{n}J_n,$$
где $J_n$ -- матрица из одних единиц, а $\alpha \in (0,1)$. Тогда матрицы $P_{\alpha}(G)$ являются положительными. С точки зрения блуждающего пользователя это означает, что у него есть два режима -- первый, в котором он находится с вероятностью $1-\alpha$ -- это режим брожения по ссылкам, а второй режим -- переход на случайную страницу. Для матрицы $P_{\alpha}(G)$ выполнены условия теоремы и поэтому она имеет единственное не кратное максимальное собственное число, равное $1$. Соответствующий собственный вектор можно выбрать положительным.


То, что у $P_{\alpha}(G)$ все собственные числа по модулю меньше единицы означает, что $P_{\alpha}(G)^kv \to cx$, при $k \to \infty$, где $x$ -- положительный вектор с собственным числом равным 1. Это позволяет приближённо найти $x$, что даёт желаемое распределение весов. Практически, для этого можно взять $k\sim \log n$. Количество точных знаков линейно зависит от $k$. Это позволяет заметно сэкономить на вычислениях по сравнению с теоретическим нахождением собственных векторов. Изучая предел $P_{\alpha}(G)$ при $\alpha \to 0$ можно получить информацию и про исходную матрицу.


Можно ли тем не менее что-то сказать в случае неотрицательных матриц? Ответ на этот вопрос дал Фробениус.

\dfn Пусть $A$ -- неотрицательная вещественная матрица размера $n$. Свяжем с матрицей $A$ ориентированный граф $G$ (возможно с петлями). Вершины $G$ есть числа от $1$ до $n$, а ребро между $j\to i$ есть только в том случае, когда коэффициент $A_{ij}\neq 0$. 
\edfn

\dfn Неотрицательная матрица $A$ называется неприводимой, если связанный с ней граф сильно связен.
\edfn

\rm Это равносильно тому, что нельзя так перенумеровать координаты, чтобы в новых координатах матрица $A$ имела блочно-верхнетреугольный вид 
$$\pmat B & C \\ 0 & D \epmat .$$
\erm

\dfn Неотрицательная матрица $A$ называется эргодической или перемешивающей, если существует такая степень $k$, что $A^k>0$. Ещё такие матрицы называют примитивными.
\edfn

\rm Элемент с индексами $i,j$ матрицы $A^k$ не равен нулю только если в графе $G(A)$ есть путь из $j\to i$ длины ровно $k$. В частности, это означает, что эргодическая матрица неприводима. Так  же, понятно, что если матрица $A^k>0$, то это выполнено и для больших степеней.
\erm

На самом деле оба эти понятия ещё более тесно связаны между собой. 

\lm Пусть $A$ -- неотрицательная неприводимая матрица размера $n$. Тогда для любого $\eps>0$  матрица $A+\eps E$ эргодическая.
\elm
\proof Заметим, что  в графе $G(A)$ есть пути $j\to i$ между любыми двумя вершинами. Их длину можно ограничить числом $n-1$. Граф $G(A+\eps E)$ отличается от $G(A)$ только тем, что около каждой его вершины есть петля. Но наличие петель в каждой вершины позволяет из пути длины $k\leq n-1$ сделать путь длины ровно $n-1$. Значит $(A+\eps E)^{n-1}>0$, что и требовалось.
\endproof



\thrm[Фробениус, 1912] Пусть $A$ -- эргодическая матрица. Тогда у $A$ есть единственное максимальное по модулю собственное число  $\lambda$ и оно вещественно и положительно. Кроме того, число $\lambda$ не является кратным для $A$. Числу  $\lambda$ соответствует положительный собственный вектор.
\ethrm
\proof Пусть $\mu\in \mb R_{>0}$ -- наибольшее собственное число для $A^k$, а $v$ -- соответствующий ему собственный вектор. Тогда 
$$\mu Av= A\cdot A^k v= A^k\cdot A v.$$
Откуда получаем, что $Av$ либо $0$, либо собственный вектор $A^k$ с тем же собственным числом $\mu$. Нулём вектор $Av$ быть не может. Тогда нулём был бы и $A^kv=\mu v$. Значит, это собственный вектор для $A^k$ c тем же $\mu$. Значит он пропорционален $v$, то есть $Av=\lambda v$. Так как $v$ вещественный положительный, матрица $A$ неотрицательна, то $\lambda>0$. То есть $v$ -- положительный собственный вектор для собственного числа $\lambda>0$. 

Заметим, что, $\lambda^k=\mu$. Кроме того, если $\lambda_1,\dots,\lambda_n$ -- все собственные числа матрицы $A$, выписанные с учётом кратности, то $\lambda_1^k,\dots,\lambda_n^k$ -- все собственные числа для $A^k$ с учётом кратности. 

В частности, отсюда следует, что у матрицы $A$ не может быть никаких собственных чисел по модулю равных  $\lambda$ и при этом отличных от $\lambda$. Посмотрев на кратности так же можно понять, что $\lambda$ -- не кратное для $A$ число. 
\endproof

\crl Пусть $A$ -- неприводимая матрица. Тогда у $A$ есть вещественное собственное число $\lambda>0$, которое больше или равно всех остальных собственных чисел по модулю. Это собственное   число не кратно. Соответствующий собственный вектор можно выбрать положительным. 
\ecrl
\proof Матрица $A+\eps E$ эргодическая и значит у неё есть положительный собственный вектор $v$, соответствующий собственному числу $\lambda_{\eps}>0$. Тогда $v$ -- собственный вектор матрицы $A$ с собственным числом $\lambda_{\eps}-\eps$. Так как $v$ положительный, а матрица $A$ неотрицательна, то $\lambda_{\eps}-\eps>0$.

Покажем, что $\lambda_{\eps}-\eps$ не зависит от $\eps$. Действительно, если бы у матрицы $A$ было другое вещественное собственное число, большее $\lambda_{\eps} - \eps$ (могло взяться от другого $\eps'$), то это дало бы собственное число матрицы $A+\eps E$, большее $\lambda_{\eps}$, чего не может быть. Обозначим за $\lambda=\lambda_{\eps}-\eps$.   Посмотрим на другое собственное число $\mu$ матрицы $A$. Тогда $\mu+\eps$ -- собственное число матрицы $A+\eps E$. Мы знаем, что $\lambda >|\mu+\eps|$. Это выполнено для всех $\eps$. Значит $\lambda \geq |\mu|$. Завершая доказательство отметим, что если $\lambda$ -- было бы кратным собственным числом для $A$, то $\lambda_{\eps}$ было бы кратным для $A+\eps E$, чего не может быть.
\endproof

\rm Если величины $f_i$ при $1<i\leq k$ и $s_i$ при $1\leq i<k$ в матрице Лесли положительны, то матрица Лесли эргодическая. Если, скажем, все $s_i>0$ и $f_k>0$, то матрица Лесли, по крайней мере, неприводима.

Если граф $G$ сильно связен, то матрица $P_G$ неприводима. А что на языке графов означает эргодичность матрицы $P_G$?
\erm


\upr Покажите, что если $A$ неотрицательная матрица, то у $A$ есть вещественное собственное число $\lambda\geq 0$, которое больше или равно всех остальных собственных чисел по модулю. Для него есть неотрицательный собственный вектор. Как видно из примеров, большего требовать не получится.
\eupr


\section{Дополнительно: модель Леонтьева}
Модель затраты-выпуск Леонтьева. Предположим, что у нас есть несколько типов товаров. Для выпуска каждого товара нужно вложить сколько-то товаров из представленных типов. Например, для производства угля нужна сталь (для стальных инструментов), а для производства стали нужен уголь и сама сталь (в виде инструментов для добычи железной руды). 

Пусть числа $a_{ij}$ характеризуют, сколько нужно взять единиц $j$-го товара для производства товара $i$. Обозначим за $A$ матрицу, составленную из $a_{ij}$. Если $x$ -- это совокупный выпуск всех товаров за фиксированный промежуток времени. Назовём $x$ вектором производств. Пусть $y$ -- это вектор итогового выпуска товаров для внешнего рынка (то, что не нужно для внутреннего производства). Тогда $y$ и $x$ будут связаны уравнением
$$x=y+Ax.$$
Из этого соотношения легко найти $x=(E-A)^{-1}y$. Таким образом, если мы хотим произвести для внешнего рынка $y$ товаров, то для этого совокупно надо произвести $x$ товаров. 

Модель называется продуктивной, если по любому запросу $y\geq 0$ можно предъявить подходящий вектор  производств $x$. При этом вектор $x$ должен быть неотрицательным. 

Продуктивность модели равносильна неотрицательности матрицы $(E-A)^{-1}$. Как можно переформулировать это условие? Заметим, что $\frac{1}{1-x}$ -- аналитическая функция в диске $|z|<1$. Поэтому, если $\lambda$ -- максимальное собственное число $A$ меньше $1$, то $$(E-A)^{-1}=E+A+A^2+\dots +A^n+\dots$$
является неотрицательной матрицей. Если $\lambda=1$, то $E-A$ просто не обратима. Если $\lambda>1$, то $E-A$ может быть необратимой, а может иметь обратную. Покажем, что тем не менее это не приводит к продуктивной модели. Заметим, что у $A$ есть неотрицательный собственный вектор с собственным числом $\lambda$. Но тогда $v$ -- собственный вектор и для $(E-A)^{-1}$ c собственным числом $\frac{1}{1-\lambda}<0$. Но это значит, что матрица $(E-A)^{-1}$ точно не положительная, так как домножив её на $v$ мы получили вектор с отрицательными компонентами.

\section{Дополнительно: метод итераций и оценки на собственные числа}

Как найти приближённо или оценить максимальное по модулю собственное число матрицы. На самом деле, мы уже видели ответ: необходимо возвести матрицу в степень. Точнее: пусть $A$ -- вещественная матрица, которая имеет единственное максимальное по модулю собственное число $\lambda$ (не кратное). Пусть $e_1,\dots,e_n$ -- жорданов базис для $A$, а $e_1$ -- соответствует собственному числу $\lambda$. Рассмотрим случайный вектор $v\in \mb R^n$. С вероятностью $1$ его $e_1$ компонента не $0$. Тогда $A^nv \sim \lambda^n v$. Пусть $x_1(n)$ -- первая компонента $A^nv$. Тогда, если первая компонента у вектора $e_1$ не ноль, то $$\frac{x_1(n)}{x_1(n-1)} \to \lambda.$$
Для оценки скорости сходимости хотелось бы иметь представление об отношении  $\frac{\lambda}{|\lambda_2|}$.

\begin{comment}

Похожая технология применяется, когда необходимо приближённо найти решение системы $x=Ax+b$, где $A$ -- квадратная матрица. Точнее, пусть собственные числа $A$ по модулю меньше $1$. Возьмём случайный вектор $x_0$ и построим последовательность $$x_i=Ax_{i-1}+b.$$
Покажем, что при указанных условия, эта последовательность стремится к решению системы. Прежде всего заметим, что матрица $E-A$ обратима, и, следовательно, система имеет единственное решение. Пусть это решение --- $x'$. Обозначим $h_i=x_i-x'$. Для $h_i$ получается соотношение:
$$x'+h_i=Ax'+Ah_{i-1}+b.$$
Откуда получаем, что $h_i=Ah_{i-1}$. Но тогда $h_i$ стремятся к нулю. Значит $x_i\to x'$.

\end{comment}

Можно ли как-то из теоретических соображений оценить собственные числа матрицы? Оказывается, что такой способ есть. Пусть $A$ -- вещественная или комплексная квадратная матрица. Рассмотрим строки $A$. Зададим элементы $r_i=\sum_{j\neq i} |a_{ij}|$. Рассмотрим множество замкнутых кругов радиуса $r_i$ с центром в $a_{ii}$ на комплексной плоскости. Эти круги называются кругами Гершгорина. 

\utv Все собственные числа матрицы $A$ лежат в объединении кругов Гершгорина. 
\eutv
\proof Пусть $\lambda$ -- собственное число $A$, а $v$ -- соответствующий собственный вектор. Тогда $Av=\lambda v$. Посмотрим на максимальный по модулю элемент $v$ -- $v_i$ и соответствующую ему строчку. Покажем, что $\lambda$ лежит в круге Гершгорина с номером $i$. Имеем $\sum a_{ij}v_j =\lambda v_i$  и, значит, $\sum_{j\neq i} a_{ij}v_j= (\lambda - a_{ii})v_i $. Имеем неравенство $$|\lambda -a_{ii}||v_i| \leq |v_i|\sum_{j\neq i} |a_{ij}|$$
Сокращая на $v_i$ получаем требуемое. 
\endproof

\rm В частности, это рассуждение даёт критерий невырожденности матрицы -- матрица $A$ невырождена, если сумма модулей её внедиагональных элементов каждой строки меньше модуля диагонального элемента. Такие матрицы называются матрицами с диагональным преобладанием
\erm

\rm
Переходя от матрицы к транспонированной можно получить ещё одну оценку, которая получается из рассмотрения столбцов $A$.
\erm


\chapter{Полилинейная алгебра}

\section{Билинейные формы}

Одним из основных примеров нормы на векторном пространстве над $\mb R$ является корень из суммы квадратов координат $\|x\|=\sqrt{x_1^2+\dots +x_n^2}$. Однако вместе с такой нормой <<в комплекте>> идёт отображение от двух переменных $(x,y) \to \sum x_iy_i$, которое называется скалярным произведением. Оно обладает свойством билинейности, то есть линейности по каждой координате. Наша задача разработать единый подход к таким объектам над произвольным полем и разобраться с самыми важными примерами.

\dfn Пусть $V$ -- векторное пространство над $K$. Отображение $h\colon V\times V \to K$ называется билинейной формой, если\\
1) $\forall \lambda \in K$ $\forall u,v,w \in V$ верно, что $h(u+\lambda v, w) = h(u,w)+\lambda h(v,w)$,\\
2) и по второй координате: $h( w, u+\lambda v) = h(w,u)+\lambda h(w,v)$.
\edfn

\exm\\
1) Рассмотрим пространство $K^n$ и определим на нём билинейную форму $h(u,v)=\sum_{i=1}^n x_iy_i$.\\
2) Пусть $A\in M_{n}(K)$. Тогда на $K^n$  можно задать билинейную форму $(x,y)\to x^{\top}Ay$.\\
3) Рассмотрим пространство многочленов $\mb R[x]$ или пространство непрерывных функций на промежутке $[a,b]$, а так же какую-нибудь непрерывную функцию $w(x)$ и зададим билинейную форму $h(f,g)=\int_a^b f(x)g(x)w(x)dx$. Часто  в качестве промежутка выступает вся вещественная прямая, а в качестве веса выбирается $w(x)=\frac{1}{\sqrt{\pi}}e^{-x^2/2}$. Обычно, такой вес возникает, когда речь идёт о нормально распределённых случайных величинах.\\
4) Рассмотрим пространство один раз непрерывно дифференцируемых функций на отрезке $C^1[a,b]$ и введём на нём билинейную форму по правилу $h(f,g)=\int_a^b f'(x)g(x)dx$.\\
5) Рассмотрим пространство матриц $M_n(K)$ и введём на нём билинейную форму $h(A,B)= \Tr(AB)$.\\
6) Рассмотрим конечномерную алгебру $A$ над полем $K$. Тогда любой элемент $a\in A$ задаёт линейное отображение $L_a \colon A \to A$, переводящее $x\to ax$. У этого линейного отображения есть след. Для простоты обозначим его как $\tr a$.
Тогда отображение $A\times A \to K$ заданное по правилу $\tr_{A/K}(u,v)=\tr(uv)$ является билинейной формой. Замечу, что конструкция из предыдущего пункта не является частным случаем этой, а отличается на константу.\\

Как обычно, поведение объекта линейной алгебры определяется его взаимодействием с каким-либо базисом.

\dfn Пусть $e_1, \dots, e_n$ базис $V$, а $h$ -- билинейная форма на $V$. Тогда матрица $A$ составленная из элементов $h(e_i,e_j)$ называется матрицей билинейной формы.
\edfn

\lm Пусть $V$ -- пространство с выбранным базисом $e_1,\dots,e_n$. Тогда имеет место взаимооднозначное соответствие между билинейными формами $h$ на $V$ и матрицами  $A\in M_n(K)$. В частности, если вектор $v$ имеет столбец координат $x$, а вектор $u$ -- столбец $y$, то значение $h(u,v)$ можно найти по формуле $y^{\top}Ax$.
\elm
\proof Пусть $u=\sum x_i e_i$  и $v=\sum y_ie_i$. Тогда $h(u,v)=\sum x_i h(e_i,v)=\sum x_iy_j h(e_i,e_j)=x^{\top}Ay$.
\endproof

\lm Пусть $V$ -- пространство с билинейной формой $h$ и базисом $e_1,\dots,e_n$. Пусть матрица $h$ в этом базисе -- это $A$. Если выбрать другой базис $f$ с матрицей перехода $C$, то в новом базисе матрица $A$ будет иметь вид 
$A'=C^{\top}AC.$
\elm
\proof Распишем: $x^{\top}Ay=(Cx')^\top A Cy'= {x'}^{\top}C^{\top}AC y'= {x'}^\top A' y'$. Тогда матрицы $C^{\top}AC$ и $A'$ равны.
\endproof



\dfn Ранг билинейной формы -- это ранг её матрицы. 
\edfn

\dfn Будем говорить, что элемент $u$ ортогонален (слева)  элементу $v$, если $h(u,v)=0$, и записывать это как $u\bot v$. 
\edfn

\dfn Билинейная форма $h$ называется невырожденной, если $\forall v \neq 0$ существует $u \in V$, что $h(u,v)\neq 0$.
\edfn

\utv Билинейная форма невырождена тогда и только тогда, когда её матрица в некотором базисе невырождена.
\eutv
\proof Пусть $A$ -- матрица билинейной формы $h$ в некотором выбранном базисе. Пусть $A$ -- невырождена. Проверим условие для формы $h$ в координатах. Если столбец $x\neq 0$, то столбец $Ax$ ненулевой. Значит, есть столбец $y$ (с одной единицей на подходящей позиции), что $y^\top Ax\neq 0$. Но это и есть свойство невырожденности для $h$ в координатах. Обратно, если $h$ невырождена, что для любого столбца $x\neq 0$ можно подобрать столбец $y$, что $y^{\top}Ax\neq 0$. Значит $Ax\neq 0$, для любого $x\neq 0$. Значит $\Ker A=0$ и $A$ -- невырождена.
\endproof

\dfn[Ортогональное дополнение справа] Пусть $h$ -- билинейная форма на $V$. Если $U$ -- подпространство $V$, то определим множество $$U^{\bot}=\{v\in V\,|\, \forall u \in U \, \, u\bot v\}.$$ 
Это множество называется правым ортогональным дополнение к $U$ (внутри $v$ относительно $h$). Аналогично можно говорить про ортогональное дополнение слева. Оно обозначается как ${}^\bot U$
\edfn

\rm Если $e_1,\dots,e_k$ базис $U$, то условие $v\in U^{\bot}$ равносильно $e_i\bot v$ по всем $i$.  
\erm

Что можно ожидать от ортогонального дополнения относительно билинейной формы? Можем ли мы что-то сказать про его размерность?

\utv Пусть $U$ -- подпространство $V$, $h$ -- билинейная форма на $V$. Тогда $\dim U^{\bot}\geq \dim V - \dim U$.
Если форма $h$ невырождена, то имеет место равенство $\dim U^{\bot}= \dim V- \dim U$ и верно, что ${{}}^\bot(U^{\bot})=U$.
\proof Если $e_1,\dots,e_k$ базис $U$, то принадлежность $v$ ортогональному дополнению задаётся  $k$ уравнениями $h(e_i,v)=0$. Отсюда немедленно вытекает неравенство. Равенство выполнено, если матрица для указанной системы (в каком-нибудь базисе) имеет ранг ровно $k$. Давайте проверим, что для невырожденной формы это имеет место. Дополним набор векторов $e_1,\dots,e_k$ по базиса $V$ и распишем систему уравнений на координаты $x$ вектора $v$ в этом базисе. Она имеет вид:
$$\pmat E_k & 0 \epmat \cdot A x=0$$
Форма невырождена, поэтому матрица $A$ обратима. Ранг правой матрицы равен $k$. При домножении на обратимую матрицу ранг не меняется. Итого ранг равен $k$. 

Для того, чтобы доказать последнее утверждение заметим, что $U\leq {U^{\bot}}^{\bot}$. Но размерность ${}^{\bot}(U^{\bot})=n-(n-k)=k=\dim U$. Откуда получаем равенство.
\endproof 
\eutv

Можно поставить вопрос: при каком условии на подпространство $U\leq V$ пространство $V$ раскладывается в виде $U\oplus U^{\bot}$?


\utv Пусть $U\leq V$  и $h$ --  билинейная форма на $V$ тогда $V=U\oplus U^{\bot}$ тогда и только тогда, когда $h|_{U}$ невырождена.
\proof Проверим, что $U \cap U^{\bot}= \{0\}$. Пусть это не так и есть такой $0\neq x \in U$, что $x \in U^{\bot}$. Но тогда $\forall y \in U$ $h(y,x)=0$ так как $x\in U^{\bot}$, что противоречит невырожденности ограничения. Значит вместе они порождают подпространство размерности по крайней мере $\dim U+\dim U^{\bot}$. Но, как мы знаем $\dim U +\dim U^{\bot} \geq \dim V$. Значит имеет место равенство и, следовательно, пространство раскладывается в прямую сумму. 

Обратно, если $V=U \oplus U^{\bot}$, то $x\in \Ker h|_{U}$ лежит одновременно в $U$ и в $U^{\bot}$, что противоречит определению прямой суммы.
\endproof
\eutv

\rm Даже в случае невырожденных форм условие, что $h|_U$ невырождена выполнено далеко не всегда. Рассмотрим билинейную форму на $\mb R^2$, заданную  матрицей 
$$ \pmat 0& 1\\ 1& 0\epmat.$$
Её ограничения на первую ось $\lan e_1\ran$ имеет нулевую матрицу. Несложно вычислить и ортогональное дополнение к $\lan e_1 \ran $ -- это само это пространство. Разложения в прямую сумму точно нет.
\erm


\dfn В случае, если пространство $V$ разложилось в виде прямой суммы подпространств $V=U\oplus U'$, таких, что $ U'\leq U^{\bot}$, то будем говорить, что имеет место разложение в ортогональную сумму подпространств  $V=U\oplus^{\bot} U'$.
\edfn

\rm[Дополнительно] Если форма $h$ невырождена, то для данного подпространства $U$ может найтись не более одного пространства $U'$, что $V=U\oplus^{\bot} U'$ -- ортогональная прямая сумма. А именно, $U'=U^{\bot}$.
\proof Если сумма ортогональная, то $U' \leq U^{\bot}$. Осталось заметить, что их размерности должны быть равны.
\endproof
\erm

 


















\section{Симметричные билинейные формы и квадратичные формы}
Наибольший интерес среди билинейных форм вызывают формы со специальными свойствами. В этом разделе речь пойдёт про симметрические формы, хотя мы вскользь обсудим и кососимметричные формы. 



\dfn Билинейная форма $h$ называется симметричной, если $h(u,v)=h(v,u)$. Форма $h$ называется кососимметричной, если $h(u,v)=-h(v,u)$(это неправильное определение в случае $\chr k=2$).
\edfn

Для симметричных и кососимметрических билинейных форм условие, что $x\bot y$ и $y\bot x$ совпадают. Это позволяет говорить про ортогональное дополнение, не упоминая, с какой стороны мы его берём. Кроме того изучение билинейной формы можно свести (хотя и не самым удобным образом) к изучению симметричных и антисимметричных форм благодаря замечанию:

\rm Любая билинейная форма $h$  над полем, характеристика которого отлична от $2$ может быть единственным образом представлена в виде суммы $h^+$ и $h^-$, где $h^+$ -- симметрическая форма, а $h^-$ -- кососимметрическая. ($h^+(u,v)=\frac{h(u,v)+h(v,u)}{2}$, $h^-(u,v)=\frac{h(u,v)-h(v,u)}{2}$)
\erm

\lm  Билинейная форма $h$ симметрична тогда и только тогда, когда её матрица в некотором базисе симметрична, то есть $A^{\top}=A$ и кососимметрична, если $A^{\top}=-A$.
\elm
\proof Разберём случай, когда $h$ симметрична. Пусть $e_1,\dots,e_n$ -- некоторый базис. Тогда $$A_{ij}=h(e_i,e_j)=h(e_j,e_i)=A_{ji},$$
Что и означает симметричность. Обратно, пусть $x=[u]_e$, а $y=[v]_e$. Тогда $$h(u,v)=x^{\top}Ay=(x^\top A y)^{\top}=y^{\top}A^{\top} x=y^\top Ax=h(v,u).$$
\endproof

\exm\\
1) Если матрица $A$ -- симметричная, то билинейная форма $x^{\top}Ay$ на $K^n$ -- симметричная. В частности, обычное скалярное произведение -- симметричная билинейная форма.\\
2) Отображение $(f,g) \to \int_a^b f(x)g(x)\omega(x)dx$ является симметричной билинейной формой на $\mb R[x]$.\\
3) Если посмотреть на пространство $V$, состоящее из функций $f\in C^1([a,b])$, что $f(a)=f(b)=0$, то форма $f,g \to \int_a^b f'g dx$ является кососимметричной. Причина -- интегрирование по частям.\\


У нас больше не будет идти речи про билинейные формы общего вида, а только про симметричные формы. Предложим теперь альтернативный взгляд на симметричные билинейные формы. {\color{red} Внимание!} С технической точки зрения удобно предполагать, что характеристика поля, над которым мы работаем отлична от $2$. Сохраним это предположение до конца раздела про билинейные формы.

\dfn Квадратичная форма -- это отображение $q\colon V \to K$, такое, что в некоторой линейной системе координат это отображение есть однородный многочлен степени 2, то есть имеет вид $\sum_{i\leq j} b_{ij} x_i x_j$. Матрицей квадратичной формы в указанной системе координат называется матрица $$a_{ij}=\begin{cases} b_{ii}, \text{ если $i=j$},\\
\frac{b_{ij}}{2}, \text{ если $i\neq j$}.
\end{cases}.$$
Если вектор $v$ имеет столбец координат $x$, то $q(v)=x^{\top}Ax$.
\edfn

Матрица $A$ квадратичной формы -- это единственная симметричная матрица, что $q(v)=x^{\top} A x$. Действительно, единственный способ решить уравнения $b_{ij}=a_{ij}+a_{ji}$ и $a_{ij}=a_{ji}$  есть $a_{ij}=a_{ji}=\frac{b_{ij}}{2}$.
Таким образом, при выборе базиса возникает взаимооднозначное соответствие 
$$\text{ Симметричные билинейные формы } \leftrightarrow \text{ Симметричные матрицы } \leftrightarrow \text{ Квадратичные формы } $$
Покажем, что соответствие симметричных билинейных и квадратичных форм не зависит от выбора системы координат. Для этого достаточно предъявить бескоординатные формулы для этого соответствия.


\utv Пусть $h$ -- билинейная симметричная форма на $V$. Тогда $q(v)=h(v,v)$ -- это квадратичная форма. При этом, в любой системе координат матрица $q$ есть $A$ -- матрица $h$.
\proof $q(v)=h(v,v)= x^{\top}Ax$. Матрица $A$ -- симметричная и, следовательно, она и есть матрица для квадратичной формы $q$.
\eutv




\rm
Пусть $q$ -- квадратичная форма. Тогда форма $h(u,v)=\frac{q(u+v)-q(u)-q(v)}{2}$ -- симметричная билинейная. Эта конструкция обратна к конструкции из предыдущего факта. В этом случае, форма $h$ называется поляризацией квадратичной формы $q$.
\erm 


\dfn Квадратичная форма невырождена, если соответствующая ей симметричная билинейная форма невырождена.
\edfn

\subsection{Ортогонализация и метод Лагранжа}

\dfn Пусть $h$ -- симметричная билинейная форма на $V$, тогда система векторов $e_1,\dots,e_k$ называется ортогональной, если $h(e_i,e_j)=0$, при $i\neq j$. Если указанная система векторов является базисом, то такой базис называют ортогональным.
\edfn

Например, стандартный базис в $\mb R^n$ ортогонален относительно билинейной формы $x,y \to x^\top y$.

\rm Матрица симметричной билинейной формы в ортогональном базисе имеет диагональный вид, а выражение для квадратичной формы есть сумма квадратов координат вектора с коэффициентами $\sum \lambda_i x_i^2$.
\erm

\dfn Будем говорить, что симметрические билинейные (или квадратичные) формы эквивалентны, если в некоторых базисах они имеют одинаковые матрицы.
\edfn

Вопросы: всегда ли можно найти ортогональный базис и насколько форма матрицы зависит от выбора ортогонального базиса? На первый вопрос ответ положительный.

\thrm Пусть $V$ -- пространство с симметричной билинейной формой $h$. Тогда в $V$ существует ортогональный относительно $h$ базис. 
\ethrm
\proof
Если пространство $V$ одномерно или $h=0$, то подойдёт просто любой базис. Пусть $h$ -- не ноль. Тогда существует вектор $e_1$, что $h(e_1,e_1)=q(e_1)\neq 0$, потому что форма $q$ не нулевая. Теперь $h|_{\lan e_1\ran}$ невырождена и следовательно $V=\lan e_1 \ran \oplus \lan e_1 \ran^{\bot}$. По индукционному предположению на пространстве $\lan e_1 \ran^{\bot}$ есть ортогональный базис $e_2,\dots,e_n$. Тогда подходящий базис -- это $e_1,\dots,e_n$. 
\endproof







Обсудим алгоритм который стоит за этим доказательством. Для это удобнее будет работать с формой $q$ и представлять её в виде однородного многочлена второй степени. После такого отождествления описанный алгоритм можно условно назвать выделением полного квадрата. Пусть форма $q(x)$ в координатах имеет вид
$$q(x)= a_{11}x_1^2+ 2a_{12}x_1x_2 + \dots + 2a_{1n}x_1x_n  + q'(x_2, \dots, x_n).$$

\noindent{\bf Первый случай} Предположим, что $a_{11}\neq 0$. Тогда представим $q(x)$ в виде, выделив полный квадрат 
$$q(x)= a_{11}\left(x_1+\frac{a_{12}}{a_{11}}x_2 + \dots +\frac{a_{1n}}{a_{11}}x_n\right)^2 - \frac{a_{12}^2}{a_{11}}x_2^2 - 2\frac{a_{12}a_{13}}{a_{11}}x_2x_3 - \cdots - \frac{a_{1n}^2}{a_{11}}x_n^2 + q'(x_2,\dots,x_n).$$

Новые переменные выглядят следующим образом:
\begin{align*}
y_1&=x_1+\frac{a_{12}}{a_{11}}x_2 + \dots +\frac{a_{1n}}{a_{11}}x_n,\\
 y_2&=x_2, \\
&\vdots\\
 y_n&=x_n.
\end{align*}
Или в подходящую сторону
$$ \pmat x_1 \\ x_2 \\ \vdots \\ x_n \epmat = \pmat 1 & -\frac{a_{12}}{a_{11}} & \dots & -\frac{a_{1n}}{a_{11}} \\
& 1 && \\
& & \ddots & \\
&&& 1
\epmat \pmat y_1 \\ y_2 \\ \vdots \\ y_n \epmat.
$$

Видно, что кроме формы $q'$ возникает ещё поправка, которая содержит слагаемые $\lambda x_ix_j$ $i,j\geq 2$. Таким образом мы обнулили $a_{1j}$ и сделали первый вектор новой системы координат ортогональным остальным, как и в доказательстве. Заметим так же, что указанное преобразование над матрицей эквивалентно одновременному применению одинаковых элементарных преобразований строк и столбцов.\\




\noindent{\bf Второй случай.} $a_{11}=0$. Если $a_{ii}\neq 0$, то меняем первую и $i$-ую координаты местами  и продолжаем как раньше. \\


\noindent{\bf Третий случай.} Все $a_{ii}=0$. Если есть такое $i$, что $a_{1i}\neq 0$, то можно перенумеровать координаты, чтобы $i$ стало равно $2$. Пусть $a_{12}\neq 0$. Тогда сделаем замену $x_1=y_1+y_2$, $x_2=y_1-y_2$, $y_i=x_i$, $i\geq 3$. Получим $2a_{12}$ при $y_1^2$ и $-2a_{12}$ при $y_2^2$. Теперь находимся в ситуации первого случая. Если так получилось, что $a_{12}=0$, но $a_{1i}\neq 0$, то опять же, можно перенумеровать координаты и применить указанную конструкцию.\\


\noindent{\bf Четвёртый случай.} Все $a_{ii}=0$ и все $a_{1i}=0$. Тогда форма не зависит от первой переменной и можно смело переходить к следующей переменной.\\ 



\subsection{Канонический вид квадратичной формы и критерий Сильвестра}

\dfn Пусть $A$ -- матрица. Числа $d_i=\det A_i$, где $A_i$ -- подматрица $A$ составленная из элементов первых $i$ строк и столбцов  называются главными минорами матрицы $A$. Будем считать $d_0=1$.
\edfn


Оказывается, что если посмотреть на алгоритм приведения к диагональному виду, то для итоговой формы есть выражение через числа $d_i$.
\thrm[Теорема Якоби]
Пусть $V$ -- векторное пространство, $q$ -- квадратичная форма, $A$ -- её матрица в некотором базисе $e_1,\dots,e_n$. Пусть главные миноры $d_i$ не равны 0. 
Тогда матрица $A$ -- невырожденная и может быть приведена к диагональному виду с числами $\frac{d_{i}}{d_{i-1}}$ на диагонали.
\proof Заметим, что если мы всё время пользуемся первым случаем из алгоритма, то домножение слева на матрицу перехода $C^{\top}$ будет эквивалентно прибавлению строки ко всем остальным строкам матрицы $A$ c коэффициентами $-\frac{a_{1i}}{a_{11}}$ (аналогично при домножении на $C$ справа происходит преобразование столбцов). Заметим, что при таком преобразовании главные миноры матрицы вообще не меняются (внутри каждого минора происходят преобразования первого типа, которые не меняют определитель). Покажем, что только первый случай реализуется. 

Пусть мы доказали это для шага $i$. На шаге $i+1$ первые $i$ столбцов и строк содержат только ненулевые диагональные элементы $a_{11},\dots, a_{ii}$. Тогда $d_{i+1}=a_{11}\dots a_{i+1 i+1}$. Так как $d_{i+1}$ не  поменялось и, следовательно не равно нулю, то и $a_{i+1 i+1} \neq 0$. Следовательно реализуется первый случай. 

Теперь посмотрим, что происходит, после приведения матрицы к диагональному виду. Заметим, что для всех $i$ $d_i=a_{11}\dots a_{ii}$. Тогда $a_{ii}=\frac{d_i}{d_{i-1}}$, что и требовалось.
\endproof
\ethrm



Есть ситуации, где мы можем полностью описать канонический вид, к которому можно привести квадратичную форму. Проще всего дать описание над алгебраически замкнутым полем, скажем $\mb C$. Над $\mb C$ любая квадратичная форма определяется своим рангом, так как приводится к виду $$q(x)=x_1^2+\dots+x_r^2.$$


Рассмотрим поле вещественных чисел $\mb R$. К какому виду можно привести форму над $\mb R$? 

\utv Пусть $q$ -- квадратичная форма на вещественном векторном пространстве $V$. Тогда существует линейная система координат в которой форма имеет вид $$q(x)= x_1^2+\dots + x_k^2 - x_{k+1}^2-\dots-x_{k+l}^2.$$
Такой вид квадратичной формы будем называть каноническим. 
\eutv
\proof Мы уже знаем, что можно найти такие координаты, что $$q(x)= \lambda_1 x_1^2+ \dots + \lambda_k x_k^2+ \lambda_{k+1} x^{k+1} + \dots + \lambda_{k+l} x_{l+k}^2.$$
Здесь все нулевые слагаемые соответствуют последним переменным и выброшены. Нумерация  координат выбрана так, что первые $k$ коэффициентов положительные, а следующие $l$ отрицательные. Тогда выберем новые координаты $y_i=\sqrt{|\lambda_i|} x_i$ при $i\leq k+l$ и $y_i=x_i$ иначе. Это и есть нужная система координат.
\endproof

\dfn Сигнатурой формы над $\mb R$ называется пара чисел $(k, l)$ -- число плюсов и число минусов в каноническом виде. Заметим, что сумма $l+k= \rk q$.
\edfn

\dfn Квадратичная форма называется положительно определённой, если $\forall v\neq 0$ $q(v)>0$. Симметричная билинейная форма называется положительно определённой, если соответствующая форма $q(v)=h(v,v)$ положительно определена. Симметричная матрица называется положительно определённой, если соответствующая форма положительно определена. Аналогично вводится понятие отрицательно определённой формы.
\edfn



\thrm Сигнатура формы $q$ не зависит от способа приведения формы к каноническому виду. Точнее -- число $k$ равно размерности наибольшего подпространства, ограничение формы  на которое положительно определено.
\proof Рассмотрим базис $e_1,\dots,e_k,e_{k+1},\dots,e_{l+k}, \dots, e_n$, что матрица формы $q$ диагональна, и первые $k$ её диагональных компонент положительны, следующие $l$ отрицательны, а остальные 0. 
Пусть $U$ подпространство $\dim U \geq k+1$, что $q|_{U}>0$. Тогда исходя из подсчёта размерности $U\cap \lan e_{k+1},\dots,e_n\ran \neq \{0\}$. Но это приводит к противоречию, так как $q(v)$ для $0\neq v \in U\cap \lan e_{k+1},\dots,e_n\ran $ выполнено, что $q(v)>0$ и $q(v)\leq 0$ одновременно.
\endproof
\ethrm

\crl Пусть $q$ -- форма на вещественном пространстве  $V$ размерности $n$. Тогда канонический вид $q$ однозначно определяется $n$ и её  сигнатурой. 
\ecrl

Можно ли как-то ещё найти сигнатуру не приводя форму к диагональному виду, а воспользовавшись другими знаниями? Ответ: да, можно. А именно:

\crl[Критерий Сильвестра]
Пусть $V$ -- векторное пространство над $\mb R$, $q$ -- квадратичная форма, $A$ -- её матрица в некотором базисе $e_1,\dots,e_n$. Пусть главные миноры  $d_i$ матрицы $A$ все не равны $0$. Тогда число перемен знака в последовательности $1=d_0,d_1,\dots,d_n$ равно числу отрицательных квадратов в каноническом виде.
\proof По теореме Якоби существует система координат в которой форма имеет диагональную матрицу с числами $\lambda_i=\frac{d_i}{d_{i-1}}$ на диагонали. Тогда последовательность $d_i$ меняет знак тогда и только тогда, когда $\lambda_i<0$.
\endproof
\ecrl

Поговорим теперь про частный случай положительно определённых форм:


\lm Положительно определённая билинейная(квадратичная) форма всегда невырождена.
\proof $h(x,x)>0$ и поэтому не равно 0.
\endproof
\elm

\thrm
Пусть дана форма $q$ на вещественном пространстве $V$ и её матрица $A$ в некотором базисе. Следующие условия эквивалентны:\\
1) Форма $q$ положительно определена.\\
2) Главные миноры матрицы $A$ положительны.\\
3) Матрица $A$ представима в виде $A=C^{\top}C$ для некоторой невырожденной верхнетреугольной матрицы $C$.\\
4) Матрица $A$ представима в виде $A=C^{\top}C$ для некоторой невырожденной матрицы $C$.
\proof
1) в 2). Если $q>0$, то $q|_{\lan e_1,\dots,e_k\ran} >0$. Но определитель матрицы этой формы и есть главный минор порядка $k$. Значит все $d_k \neq 0$ и можно применить критерий Сильвестра. Так как форма положительно определена, то  в последовательности $1=d_0,d_1,\dots,d_n$ нет перемен знака. Отсюда $d_i>0$.

2) в 3). Следует из доказательства теоремы Якоби -- всегда реализуется первый случай и, значит, матрицы перехода верхнетреугольные и невырожденные.

3) в 4). Просто забудем, что $C$ -- верхнетреугольная.

4) в 1)  Пусть $A=C^{\top}C$, тогда если $x^{\top}Ax=(Cx)^{\top}Cx\geq 0$. Более того, это выражение равно нулю только если $Cx=0$. Но такое возможно, только если $x=0$ из-за невырожденности $C$.
\endproof
\ethrm

Представление положительно определённой матрицы $A$ в виде $C^{\top}C$ для верхнетреугольной $C$ называется разложением Холецкого. Критерий Сильвестра пригоден в случае, если имеется задача с параметром. Иначе, вычисление определителя практически эквивалентно приведению к диагональному виду. Покажем, как можно применять понятие положительной определённости. Точнее, того, что положительная определённость гарантирует невырожденность.




\zd Рассмотрим множество $\left\{ 1,\dots, n\right\}$. Сколько может быть различных подмножеств $C_1,\dots,C_m$, таких, что $|C_i \cap C_j|=t\geq 1$ одинаково для всех $i \neq j$?
\ezd

Мы покажем, что есть ограничение $m\leq n$. Прежде всего, это надо сделать в случае, если $|C_i|=t$ для некоторого $i$. В этом случае $C_i \subseteq C_j$ для всех $j$. Тогда $C_j'=C_j\setminus C_i$ лежат в множестве $\{1,\dots,n\}\setminus C_i$, которое состоит из $n-t$ элементов. Множества $C_j'$ не пусты, но имеют пустое пересечение друг с другом. Тогда их меньше чем $n-t$ штук, откуда -- каждое множество должно содержать уникальный элемент. Итого
$$m-1\leq n-t \leq n-1 \text{ или, по-другому, } m \leq n.$$

Теперь покажем, что в ситуации $d_i=|C_i|>t$ выполнено то же неравенство. Для этого составим матрицу инцидентности
$$B_{ij}= \begin{cases} 1, \text{ если } i\in C_j. \\
0, \text{ иначе}
\end{cases}. $$
Теперь матрица $B^{\top}B$ есть квадратная симметричная матрица размера $m$. Я утверждаю, что $B^{\top}B$ положительно определена. Для этого найдём её явно.
$$B^{\top}B= \pmat
d_1 & t &t\\ 
t & \ddots & t \\
t & t & d_m
\epmat = \pmat
d_1-t &  &0\\ 
 & \ddots &  \\
0 &  & d_m-t
\epmat + \pmat
t & \dots & t\\ 
\vdots & \ddots & \vdots \\
t & \dots & t
\epmat.$$
Тогда при $x\neq 0$ имеем 
$$x^{\top} B^{\top}B x = \sum (d_i-t) x_i^2 + t(\sum x_i)^2>0.$$
Значит матрица $B^\top B$ имеет ранг $m$. Но с другой стороны 
$$m=\rk B^{\top}B \leq \rk B \leq n.$$

Дальнейшие оценки и результаты существования для различных конфигураций множеств можно найти по ключевым словам Block Design (аналог в русском языке --  комбинаторные схемы)




\section{Евклидовы и унитарные пространства}

Напомню, что основной нашей мотивацией для изучения билинейных форм было, то, что вместе с понятием расстояния часто идёт вместе некоторая билинейная форма.


\dfn Векторное пространство $V$ над $\mb R$ вместе с заданной на нём положительно определённой симметричной билинейной формой $\lan\cdot \, , \cdot \ran$ называется евклидовым пространством. Форма $\lan\cdot \, , \cdot \ran$ называется скалярным произведением. 
\edfn

\dfn Определим  норму на евклидовом пространстве как $\|v\|=\sqrt{\lan v , v\ran }$. Норма задаёт расстояние по правилу $\rho(u,v)=\|u-v\|$. 
\edfn

\lm[Неравенство Коши-Буняковского] В евклидовом пространстве выполнено неравенство
$$ \lan u,v\ran \leq \|u\|\|v\|.$$
\proof  Квадратный трёхчлен $\lan u,u\ran +2\lambda\lan u,v\ran +\lambda^2\lan v,v\ran=\lan u+\lambda v, u+\lambda v\ran \geq 0$ всегда положителен. Значит у него нет корней, то есть дискриминант отрицателен. То есть $$4\lan u,v\ran^2 \leq 4 ||u||^2||v||^2.$$
\endproof
\elm

\lm Введённая норма действительно является нормой.
\proof Необходимо показать неравенство $||u+v||\leq ||u||+||v||$. Оно эквивалентно $$||u+v||^2 \leq ||u||^2+||v||^2+2||u||||v||$$
Расписывая левую часть получаем эквивалентное
$$ ||u||^2+||v||^2+2\lan u,v\ran \leq ||u||^2+||v||^2+2||u||||v||.$$
Сокращая справа и слева приходим к уже известному неравенству.
\endproof
\elm

 
\lm Пусть $V$ -- евклидово пространство. Тогда для всякого подпространства $U$ имеет место ортогональное разложение $V=U\oplus U^{\bot}$. Если есть такое разложение, то оператор проекции на $U$ называется ортогональной проекцией.
\elm
\proof Положительно определённая форма невырождена. Ограничение положительно определённой формы на любое подпространство положительно определено.
\endproof



Мы с вами помним, что в задачах алгебры  удобнее бывает работать над алгебраически замкнутым полем. Однако, если мы дословно перенесём все определения с $\mb R$ на $\mb C$, то нас постигнет неудача. Прежде всего в плане положительной определённости. А именно, любая комплексная квадратичная форма не будет принимать вещественные значения. Это делает невозможным аналогичное вещественному случаю определение расстояния. 

Это приводит нас к тому, что язык билинейных форм не совсем адекватен в комплексной ситуации. С другой стороны у нас есть пример удачного понятия расстояния на $\mb C$, которое задаётся формулой $\sqrt{\ovl{z}z}$. Такой выражение получается не из билинейной формы $xy$ подстановкой $x=y$, а из менее ожидаемого $\ovl{x}y$. Заметим, что и вообще на пространстве $\mb C^n$ можно ввести операцию $
(x,y) \to \sum_{i=1}^n \ovl{x_i}y_i$, которая даст по стандартной схеме норму $\sqrt{\sum_{i=1}^n |x_i|^2}$. Попробуем разобраться в общей ситуации.

\dfn Пусть $V$ -- комплексное пространство.  Отображение $h\colon V \times V \to \mb C$ называется полуторалинейным, если \\
1) $h(x,y+\lambda z)=h(x,y)+\lambda h(x,z)$. \\
2) $h(x+\lambda y,z)=h(x,z)+\ovl{\lambda} h(y,z)$.
\edfn

\exm\\
1) Основным примером полуторалинейной формы на $\mb C^n$ будет форма $(x,y)\to \sum \ovl{x_i}y_i$\\
2) В более общем виде, если взять матрицу $A\in M_n(\mb C)$  то выражение $\ovl{x}^{\top}Ay$ задаёт полуторалинейную форму на $\mb C^n$.\\
3) Рассмотрим пространство комплекснозначных непрерывных функций на отрезке $C([a,b])$. Определим полуторалинейную форму по следующему правилу:
$$h(f,g)=\int_a^b \ovl{f(x)}g(x)w(x)dx,$$
где $w(x)$ -- непрерывная на $[a,b]$ комплекснозначная функция, которая обычно называется весом.\\



\dfn Матрицей полуторалинейной формы $h$
в базисе $e$ называется матрица $a_{ij}=h(e_i,e_j)$. 
\edfn

\lm Если $x$ и $y$ координаты векторов $u$ и $v$, то $$h(u,v)=\ovl{x}^{\top}Ay$$
и обратно, если $$h(u,v)=\ovl{x}^{\top}Ay,$$
то $A$ -- это матрица $h$.
\elm



Заметим, что понятия симметричности  от таких форм не приходится ожидать. Действительно, если $\lambda h(u,v)=h(u,\lambda v) = h(\lambda v,u)=\ovl{\lambda}h(v,u)=\ovl{\lambda}h(u,v)$ для всех $\lambda$. Тогда, конечно, $h(u,v)=0$. Однако приведённые примеры подсказывают нам необходимое свойство.

\dfn Полуторалинейная форма $h$ называется эрмитовой, если $h(u,v)=\ovl{h(v,u)}$ и косоэрмитовой, если $h(u,v)=-\ovl{h(v,u)}$
\edfn

\lm Полуторалинейная форма эрмитова тогда и только тогда, когда её матрица $A$ удовлетворяет соотношению $\ovl{A^{\top}}=A$ и косоэрмитова, если $\ovl{A^{\top}}=-A$.
\elm

Если коэффициенты матрицы вещественны, то условие эрмитовости -- это условие симметричности. Вообще, эрмитовость формы $h$ означает, что все значения $h(v,v) \in \mb R$. Это позволяет дать опеределение аналог положительной определённости. 


\dfn Эрмитова форма называется положительно определённой, если для всех $v\in V\setminus\{0\}$ выполняется $h(v,v)>0$.
\edfn

\lm Матрица положительно определённой эрмитовой формы невырождена.
\proof От противного, если существует вектор $x\in \Ker A\setminus\{0\}$, то $0<\ovl{x}^{\top}Ax = 0$, противоречие.
\endproof
\elm

Вообще, для эрмитовых форм есть аналог теоремы об ортогонализации, теоремы Якоби, сигнатуры и критерия Сильвестра. То есть при желании можно при помощи координат проверить положительную определённость эрмитовой формы. Перейдём к основному определению, связанному с положительной определённостью в комплексном случае.


\dfn(Унитарное пространство) Пространство $V$ над $\mb C$ вместе с положительно определённой эрмитовой формой $\lan \cdot, \cdot \ran$ называется унитарным пространством. Форма $\lan \cdot, \cdot \ran$ называется скалярным произведением.
\edfn

Попробуем доказать аналог неравенства Коши-Буняковского для унитарного пространства $V$ и вывести из него, что отображение $\|\cdot\| \colon V \to \mb R$, заданное по правилу $v\to \sqrt{\lan v,v\ran}$, так же как и в вещественном случае задаёт  норму на $V$.

\lm Пусть $u,v \in V$ -- два вектора в унитарном пространстве. Тогда имеет место неравенство:
$$|\lan u,v\ran| \leq \|u\| \|v\|.$$
\proof Как и раньше  $\lan u+\lambda v, u+\lambda v\ran \geq 0$ для всех $\lambda \in \mb R$. Раскрывая скобки получаем однако $$\lan u,u\ran +2\lambda \Re\lan u,v\ran +\lambda^2\lan v,v\ran\geq 0.$$  Итого имеем  $$4(\Re\lan u,v\ran)^2 \leq 4 ||u||^2||v||^2.$$
Пусть $\lan u ,v \ran = r e^{i\ffi}$. Тогда $\Re \lan  e^{i\ffi}u , v \ran= e^{-i\ffi} r e^{i\ffi}=r=|\lan u,v\ran|$, а $||e^{i\ffi}u||=||u||$. Применение предыдущего неравенства  к паре $e^{i\ffi} u, v$ доказывает общее неравенство.
\endproof
\elm

\crl Отображение $\|\cdot\| \colon V \to \mb R$, заданное по правилу $v\to \sqrt{\lan v,v\ran}$ задаёт  норму на $V$.
\ecrl


Так же полезно будет проинтерпретировать геометрическое понятие угла между двумя векторами, чтобы отдать дань школьному определению скалярного произведения.

\dfn Пусть $x,y\neq 0$ два вектора в $V$. Если $V$ -- евклидово, то углом между ними называется такое число $0\leq\ffi\leq \pi$, что 
$$\cos\ffi = \frac{\lan x,y\ran}{\|x\| \|y\|}.$$
В случае унитарного пространства $V$ угол $\ffi$ может принимать значения $0\leq \ffi \leq \frac{\pi}{2}$ и определяется соотношением
$$\cos\ffi = \frac{|\lan x,y\ran|}{\|x\| \|y\|}.$$
\edfn

Оба определения корректны благодаря неравенству Коши-Буняковского. По каждому из этих определений угол между векторами равен $\frac{\pi}{2}$ тогда и только тогда, когда $\lan x,y 
\ran=0$, то есть когда векторы ортогональны. 

В дальнейшем мы будем обсуждать  евклидовы и унитарные пространства. Свойства евклидовых и унитарных пространств похожи, поэтому  мы будем стараться доказывать общие утверждения в обоих случаях.



\section{Ортогонализация Грама-Шмидта}

Если мы рассматриваем вектор $x$ на плоскости и некоторую прямую $l$, заданную направлением $v$, то проекция $x$ на прямую $l$ может быть найдена по формуле 

$$pr_l(x)=\cos \alpha  ||x|| \cdot v= \frac{\lan x,v\ran}{||v||^2} v.$$

В случае евклидовых пространств ограничение положительно определённой формы на любое подпространство невырождено и, таким образом, нахождение базиса, в котором форма имеет канонический вид несколько облегчается.  Это приводит к тому, что мы можем уточнить сам результат и немного изменить алгоритм нахождения подходящего базиса. Так же мы покажем, что ровно тот же алгоритм работает в унитарных пространствах и позволяет найти такой базис, что матрица скалярного произведения в этом базисе диагональна и даже единична.

Итак пусть дан набор векторов $e_1,\dots, e_n $ евклидового или унитарного пространства $V$. Ортогонализацией набора $e_1,\dots,e_n$ называется  новый набор векторов $f_1,\dots,f_n$ такой, что\\
1) $f_i \bot f_j$, если $i\neq j$\\
2) $\forall\,\, 1\leq k\leq n\,\,\lan e_1,\dots,e_k\ran=\lan f_1,\dots,f_k\ran$\\
3) $\|f_i\|=1$.

\dfn Набор векторов со свойством 3) называется нормированным. со свойствами 1),3) -- ортонормированным.
\edfn 

\rm Заметим, что если мы нашли набор ненулевых векторов со свойствами 1) и 2), в котором нет нулевых векторов, то несложно сделать из него нормированный набор, взяв вектора $\frac{f_i}{\|f_i\|}$. 
\erm

\thrm Пусть $V$ -- евклидово или унитарное пространство. Задача ортогонализации разрешима для линейно независимого набора векторов из $V$.
\proof

Перейдём к решению задачи добиваясь только условий 1) и 2). Будем последовательно искать вектора $f_i$ в виде $f_i=e_i+\lambda_1 f_1 +\dots + \lambda_{i-1} f_{i-1}$. Этот подход приводит к ответу
$$f_i=e_i-\sum_{j<i} \frac{\lan f_j,e_i\ran}{\lan f_j,f_j\ran}f_j.$$
Так как вектора линейно независимы, то $f_i\neq 0$. Это означает, что можно поделить на его норму и добиться нормированности.
\endproof
\ethrm




\crl В евклидовом и унитарном пространстве любой ортонормированный набор векторов можно дополнить до ортонормированного базиса.
\ecrl

Процесс ортогонализации позволяет строить ортогональный базис для различных подпространств. Кроме того, очень удобно находить координаты в ортогональном базисе.



\utv[Нахождение координат в ортогональном базисе] Пусть набор $e_1,\dots,e_n$ --- ортогональный базис $V$. Если $c_i$ -- это координаты вектора $x$ в базисе $e$, то 
$$c_i= \frac{\lan e_i,  x\ran}{\lan e_i, e_i\ran}.$$
В случае нормированного базиса формула упрощается -- исчезает знаменатель. Кроме того, в этой ситуации $\|x\|^2=\sum |c_i|^2$
\proof Пусть $x=\sum c_i e_i$. Тогда $\lan e_i, x\ran = c_i \lan e_i,e_i \ran$, что и требовалось.
Предположим, что $e$ -- ортонормированный базис. Тогда, раскрывая скобки в выражении $||x||^2=\lan x, x\ran$ приходим к $\sum |c_i|^2 $.
\endproof
\eutv


\rm Для любого подпространства в унитарном пространстве $U \leq V$ так же определено его ортогональное дополнение $U^{\bot}=\{ v\in V\,|\, \lan u,v\ran =0 \text{ для всех } u\in U\}$. $V$  раскладывается в прямую сумму $V=U\oplus U^{\bot}$ -- действительно их пересечение $0$, а уравнения на элементы из $U^\bot$ по прежнему задаются при помощи базисных элементов из $U$, что даёт нужное соотношение на размерность.
\erm


\crl Пусть $ e_1,\dots, e_n$ --- ортогональный базис $V$, а  подпространство $U$ порождено $ e_1,\dots,e_k$. Тогда 
\enm
\item $ pr_U x= \sum \frac{\lan x,e_i\ran}{\lan e_i,e_i\ran} e_i.$
\item $||pr_{U^{\bot}} x||^2 + ||pr_U x||^2=||x||^2.$
\eenm
\proof Рассмотрим базис $e_1,\dots, e_k$ и дополним его векторами $e_{k+1},\dots,e_n$ до ортогонального базиса всего $V$. Тогда $$x= \sum_{i=1}^k \frac{\lan e_i,  x\ran}{\lan e_i, e_i\ran}e_i + \sum_{i=k+1}^n \frac{\lan e_i,  x\ran}{\lan e_i, e_i\ran}e_i $$
Первая часть суммы лежит в $U$, а вторая в ортогональном дополнении. По единственности такого разложения получаем требуемое.
\endproof
\ecrl







Для чего может пригодиться понятие ортогональной проекции? Прежде всего для вычисления расстояния. Расстояния между подпространствами. Начнём с общего определения.

\dfn Пусть $A$ и $B$ подмножества метрического пространства. Тогда расстоянием $\rho(A,B)$ положим равным
$$\rho(A,B)=\inf_{\substack{x\in A\\ y \in B}} \rho(x,y).$$
\edfn

Наша задача --- научиться считать расстояние между аффинными подпространствами $A_1$ и $A_2$ в евклидовом пространстве, то есть подмножествами вида $L+x$, где $L$ -- это произвольное подпространство (линейное), а $x$ -- некоторый вектор из $V$. Попробуем это сделать. Представим $A_1=L_1+x$ и $A_2=L_2+y$. 

Разберём сначала случай расстояния от точки до линейного подпространства. 

\thrm Пусть $U \leq V$ подпространство, $x \in V$. Тогда расстояние $\rho(x,U)$ достигается на проекции  $pr_U(x)$ и равно $\|x-pr_U(x)\|=\|pr_{U^{\bot}}(x)\|$.
\proof Рассмотрим $u \in U$, и представим $x=y+z$, где $pr_Ux=y \in U$, $pr_{U^{\bot}}x =z \in U^{\bot}$. Тогда $$\rho(x,u)^2= ||x-u||^2= ||y - u ||^2+ ||z||^2 \geq ||z||^2=||pr_{U^{\bot}}x||^2 .$$
С другой стороны равенство достигается при $u=y=pr_{U} x$.
\endproof
\ethrm



\rm Если размерность $U^{\bot}$ мала, то может быть легче найти проекцию на $U^{\bot}$, найдя ортогональный базис $U^{\bot}$.
\erm

Всё это немедленно приводит к решению общей задачи.

\utv Пусть  $A_1=L_1+x$ и $A_2=L_2+y$ -- аффинные подпространства. Тогда $\rho(A_1,A_2)=\rho(y-x, L_1+L_2)$. То есть задача сводится к ранее разобранной.
\proof $$\inf_{\substack{u+x\in L_1+x\\ v+y \in L_2+y}} ||u+x-v-y||=\inf_{\substack{u-v\in L_1+ L_2}}||u-v - (y-x)||=\inf_{\substack{u\in L_1+ L_2}}||u- (y-x)||.$$
\endproof
\eutv


\dfn Пусть $e_1,\dots, e_k$ набор векторов $V$. Тогда матрицей Грама называется матрица 
$$G(e_1,\dots,e_k)_{ij}= \lan e_i, e_j\ran.$$
Матрица Грама отличается от матрицы скалярного произведения как би(полутора)линейной формы, только тем, что определяется она для произвольного набора векторов. 
\edfn

Как мы помним, процедура ортогонализации Грама-Шмидта аналогична той, которую мы обсуждали в общем контексте билинейных форм. Главные миноры матрицы Грама и, в частности, определитель матрицы Грама дают коэффициенты матрицы после ортогонализации. А что значат эти коэффициенты? Разберёмся в случае $\mb R^n$.

\utv
Пусть $v_1,\dots,v_n$ -- набор векторов в $\mb R^n$. Тогда для стандартного скалярного произведения имеем
$$ \det G(v_1,\dots,v_n)=(\Vol(v_1,\dots,v_n))^2$$
\proof Пусть матрица $A$ составлена из столбцов $v_i$. Тогда $G(v_1,\dots,v_n)=  A^{\top}A$ и
$$ \det G(v_1,\dots,v_n)= \det A^{\top}A=\det A^{\top} \det A= (\det A)^2= \Vol(v_1,\dots,v_n)^2.$$
\endproof
\eutv

\rm Таким образом, видно, что понятие расстояния точно определяет понятие объёма параллелепипеда. Хотя и не даёт возможности задать ориентацию пространства.
\erm

\rm Определитель матрицы Грама обнуляется тогда и только тогда, когда вектора $v_i$ линейно зависимы.
\erm


\upr Пусть $A$ -- это матрица Грама набора векторов $e_1,\dots,e_k \in \mb R^n$, а $B$ -- набора векторов $e_1,\dots,e_{k+1}$. Покажите, что $$\frac{\det B}{\det A}=\rho(e_{k+1},U)^2, \text{ где } U=\lan e_1,\dots,e_k \ran.$$
\eupr





\subsection{Метод наименьших квадратов}


Допустим, мы хотим узнать некоторый закон природы в виде $y=f(x)$. Мы провели много измерений $y_i=f(x_i)$. Если мы предполагаем, что функция $f$ есть, например, многочлен, то на коэффициенты этого многочлена возникает система линейных уравнений. 

К сожалению, нет никаких шансов, что эта система разрешима: количество измерений велико а наши измерения не точны. Иными словами, на самом деле выполнены равенства $y_i= f(x_i) + \eps_i$ для маленьких $\eps_i$. Что делать если мы не можем точно решить систему? Будем искать наиболее близкое её решение. В качестве меры близости разумно выбрать сумму
$$\sum_i |y_i-f(x_i)|^2.$$
Обобщая, мы приходим к следующей задаче: пусть есть матрица $A\in M_{m\times n}(\mb R)$, столбец $b\in R^m$ и мы хотим найти $x$ такой, что  
$$||Ax-b|| \text{ минимальна.}$$
Заметим, что в качестве $Ax$ может выступать произвольный элемент $\im A$. Это означает, что для решения нашей задачи мы должны найти ближайший к $b$ элемент $y \in \Im A$ и для этого $y$ найти соответствующий ему $x$. У этой задачи есть довольно простое решение. А именно, заметим, что элемент $y=Ax=pr_{\Im A} b$ должен иметь одинаковые с $b$ скалярные произведения со всеми столбцами матрицы $A$ (потому что они порождают $\Im A$). Это равносильно матричному соотношению
$$A^\top A x=A^\top b.$$
Как теперь найти сам вектор $x$? Рассмотрим ситуацию, когда $\Ker A=\{0\}$. В этом случае, матрица $A$ осуществляет взаимооднозначное соответствие между векторами из $\mb R^n$ и векторами из $\Im A \leq \mb R^m$.
Матрица $A^\top A$ -- это матрица Грама для столбцов матрицы $A$ и значит невырождена (столбцы $A$ линейно независимы). Значит у этой системы есть единственное решение 
$$x=(A^\top A)^{-1}A^\top b,$$
которое и даёт искомый единственный вектор $x$. Если же $\Ker A \neq 0$, то ситуация усложняется. 

\begin{comment}

Мы по прежнему ищем $y\in \Im A$ ближайший к $b$. Но для данного $y$ есть много $x\in \mb R^n$, что $Ax=y$. Какой из них выбрать? Выберем $x$ с наименьшей  длиной. Что это означает? Представим произвольное решение этого уравнения как $x_0+z$, где $z\in \Ker A$. Наименьшее длина такого вектора будет достигаться при $z= pr_{\Ker A} x_0$. В этом случае $x=x_0+z$ будет равен проекции $x_0$ на $(\Ker A)^{\bot}$. Способ нахождения такого элемента мы обсудим позже.

\end{comment}

\dfn Если $\Ker A =0$, то матрица $(A^{\top }A)^{-1}A^{\top}$ называется псевдообратной к $A$. В общей ситуации определение более сложное.
\edfn

На математической статистике вам объяснят, почему такой выбор является наилучшим при некотором предположении на распределений ошибок в каждом равенстве. Однако метод наименьших квадратов удобно использовать и для анализа данных и прогнозирования их поведения.


\section{Ортогональные и унитарные операторы}

Основным отличием структуры евклидового и унитарного пространства от просто векторного пространства является понятие расстояние и поэтому некоторое время мы посвятим преобразованиям, это расстояние сохраняющим.




\dfn Пусть $V$ --- евклидово (унитарное) пространство. Ортогональным (соответственно унитарным) оператором на $V$ называется такой линейный оператор $L \colon V \to V$, что $||Lx||=||x||$.
\edfn




\thrm Пусть $L \colon V \to V$ -- линейный оператор на евклидовом или унитарном пространстве $V$. Тогда следующие условия эквивалентны:\\
1) $L$ -- ортогональный (унитарный) оператор.\\
2) $\lan Lx,Ly\ran=\lan x,y \ran$ для всех $x,y \in V$.\\
3) $L$ переводит любой ортонормированный базис в ортонормированный базис.\\
4) В любом ортонормированном базисе $A$ -- матрица $L$ -- удовлетворяет условию $\ovl{A}^{\top} A = E_n$.\\
5) $L$ переводит некоторый ортонормированный базис в ортонормированный базис.\\
6) В некотором ортонормированном базисе $A$ --  матрица $L$ -- удовлетворяет условию $\ovl{A}^{\top} A = E_n$.
\proof Покажем $1\to 2$. Пусть $x$ и $y$ из $V$, тогда $$||x||^2+ ||y||^2+ 2\Re\lan x,y\ran= ||x+y||^2= ||L(x+y)||^2= ||x||^2+||y||^2+2\Re \lan Lx,Ly\ran.$$
Итого вещественные части скалярного произведения сохраняются. Теперь взяв вместо $x$ вектор $ix$ получаем $\Re \lan ix,y\ran = - \Re i\lan x,y\ran=\Im \lan x,y\ran$ и аналогично $\Re \lan L(ix),Ly\ran =\Im \lan Lx,Ly \ran$ откуда и мнимые части совпадают.
$2\to 3$ ясно.


Покажем, что $3 \leftrightarrow 4$ и $5 \leftrightarrow 6$. Для этого покажем, что если фиксировать ортонормированный базис $e_1,\dots, e_n$, то $L(e_i)$ ортонормирован, тогда и только тогда, когда $A$ -- матрица $L$ удовлетворяет условию $\ovl{A}^{\top}A=E_n$. Для этого необходимо и достаточно заметить, что $$\lan Le_i, Le_j \ran = (\ovl{A}^{\top}A)_{ij}.$$
Действительно $v_j$ -- $j$-ый столбец $A$ составлен из координат $Le_j$. $i$-ая строка $\ovl{A}^{\top}$ равна тогда $\ovl{v}_i^{\top}$. Но тогда $$\lan Le_i,Le_j\ran = \ovl{v}_i^{\top}E_n v_j= \ovl{v}_i^{\top}v_j= (\ovl{A}^{\top}A)_{ij}.$$
Во втором равенстве $E_n$ играет роль матрицы Грама ортонормированного базиса $e_1,\dots, e_n$.
Итак $3 \leftrightarrow 4$ и $5 \leftrightarrow 6$. Из 3) очевидно следует 5). $6\to 1$. $$||Lx||^2=\ovl{[x]_e}^{\top} \ovl{A}^{\top} A [x]_e= \ovl{[x]_e}^{\top} [x]_e= ||x||^2.$$
\endproof
\ethrm

\crl В частности, ортогональный оператор  $L$ сохраняет углы между векторами.
\ecrl

\crl Пусть $e_1,\dots,e_n$ --- ортонормированный базис $V$. Линейный оператор $L$, который в базисе $e_i$ имеет матрицу, составленную из столбцов $v_1,\dots,v_n$, является ортогональным (унитарным) тогда и только тогда, когда $v_1,\dots,v_n$ --- ортонормированный базис $\mb R^n$. 
\proof
Пусть $B$ -- матрица $L$ -- составлена из столбцов $v_i$ в ортогональном базисе $e$. Тогда соотношение $\ovl{B}^{\top}B=E$ эквивалентно ортогональности и нормированности $v_i$.
\endproof
\ecrl




\dfn Матрица $A\in M_n(\mb R)$ называется ортогональной, если $A^{\top}A=E_n$. Множество всех ортогональных матриц размера $n$ обозначается $\O_n(\mb R)$. Такие матрицы описывают все линейные изометрии $\mb R^n$  и поэтому образуют подгруппу в группе $\GL_n(\mb R)$.
\edfn

\rm Это определение можно применить и к комплексным матрицам, в результате чего получится группа $\SO_n(\mb C)$. Однако вместо неё популярнее другая группа матриц:
\erm

\dfn Определим группу унитарных матриц $\U_n(\mb C)$, как подгруппу в $\GL_n(\mb C)$, состоящую из матриц, удовлетворяющих равенству $\ovl{A}^{\top}A=E_n$.
\edfn

\rm Заметим, что определитель ортогональной матрицы либо плюс, либо минус единица. Определим подгруппу $\SO_n(\mb R) \leq \O_n(\mb R)$ -- специальную ортогональную группу состоящую из матриц $$\SO_n(\mb R)= \{ A \in \O_n(\mb R)\, | \, \det A=1 \}.$$
сохраняющих ориентацию. Это подгруппа индекса 2. Её называют группой вращений $\mb R^n$.
Аналогично определяется группа $SU_n$
\erm 


\subsection{QR разложение}

Пусть $A \in M_{m\times n}(\mb R)$ -- матрица ранга $n$. Посмотрим на неё как на набор столбцов $e_1,\dots,e_n$. Применим к этим столбцам процедуру ортогонализации. При применении процедуры ортогонализации мы используем элементарные преобразования столбцов. Более того, мы всегда столбцы с меньшим номером прибавляем к столбцам с большим номером. Таким образом, мы всегда домножаем матрицу $A$ на верхнетреугольную матрицу. Итого существует верхнетреугольная матрица $R$, что 
$$AR=Q,$$ 
где столбцы $Q$ образуют ортонормированный базис $\mb R^n$. Тогда  $Q$ -- ортогональная матрица. Домножив на $R^{-1}$ получаем, что 
$$A=QR^{-1},$$
$R^{-1}$ -- верхнетреугольная невырожденная, $Q$ состоит из ортонормированного набора столбцов. Таким образом мы доказали:

\thrm Для любой матрицы  $A\in M_{m \times n}$ существуют матрицы $R\in UT_n(\mb R)$ и $Q\in M_{m\times n}(\mb R)$,  что столбцы $Q$ ортонормированны, $R$ -- невырождена и  
$$A=QR.$$
Такое разложение называется $QR$  разложением. Если $A$ -- квадратная, то матрица $Q$ -- ортогональна. 
\ethrm


Если вам известно $QR$ разложение матрицы, то это позволяет вам легко вычислить псевдообратную (в частности, обратную) матрицу. Действительно: $A^\top A= R^{\top} Q^\top Q R= R^\top R$. Значит
$$(A^\top A)^{-1}A^\top= R^{-1} (R^\top)^{-1}R^\top Q^\top= R^{-1}Q^\top.$$
Процесс ортогонализации считается более устойчивым к ошибкам округления, чем метод Гаусса. Поэтому, такое обращение для вещественных матриц предпочтительней.


Вы можете спросить: есть ли тут что-то общее с QR-кодом? Ответ: ничего кроме букв.
 





\section{Сопряжённые линейные отображения}

Геометрия евклидовых и унитарных пространств позволяет сводить определённые вопросы про билинейные формы к вопросам про операторы и наоборот. А именно, всякому оператору $L$ соответствует билинейная (полуторалинейная) форма $\lan x,Ly\ran$. Несложно понять, что любая билинейная форма имеет такой вид (из соображений, что и оператор и форма однозначно  задаются квадратными матрицами) и по этой форме можно обратно восстановить $L$.

Кроме того, $L$ задаёт другую билинейную (полуторалинейную) форму $\lan Lx,y\ran$. Восстановив по этой форме обратно оператор, используя уже обсуждённое ранее соответствие мы получим вообще говоря отличный от $L$ оператор. Его обозначают обычно как $L^*$. 

Разберёмся с этими соответствиями поближе. Нам удобно будет расширить контекст с операторов на одном евклидовом (унитарном) пространстве, до произвольных линейных отображений между такими пространствами.

\dfn Пусть $L$ -- линейное отображение $L\colon U \to V$ между евклидовыми или унитарными пространствами. Тогда сопряжённым отображением к $L$, называется такое линейное отображение $L^*$, что $\lan L^*x,y\ran = \lan x,Ly\ran$ для всех $x\in V$ и $y \in U$.
\edfn

\thrm Сопряжённое линейное отображение единственно. Более того, если в $U$ и $V$ выбрать ортонормированные базисы $u$ и $v$, \ матрица $L$ в этих базисах есть $A$, то матрица сопряжённого отображения будет равна $\ovl{A}^{\top}$.
\proof Достаточно доказать необходимость и достаточность последнего соотношения, чтобы показать единственность и существование. Выберем ортонормированные базисы в $U$ и $V$ -- $u_i$ и $v_j$. Обозначим матрицу кандидата на $L^*$ за $B$. Тогда для равенства из определения сопряжённости необходимо и достаточно, его выполнения на базисных. Иными словами необходимо и достаточно, чтобы $\lan L^*e_i,e_j\ran=\lan e_i,Le_j\ran$. Но первая часть даёт $\ovl{B_{ji}}$, а вторая -- $A_{ij}$. Итого необходимо и достаточно, чтобы $\ovl{B}^\top=A$, то есть $B=\ovl{A}^\top$.   
\endproof
\ethrm



\crl Сопряжённый оператор к оператору $L$ существует и единственен. Более того, если задан ортонормированный базис $e_1,\dots,e_n$ и $A$ -- матрица $L$, то матрица $L^*$ есть $\ovl{A}^{\top}$.
\ecrl









\lm[Общие свойства]
$(L+T)^*=L^*+T^*$\\
$(LT)^*=T^*L^*$\\
$(\lambda L)^*=\ovl{\lambda}L^*$.\\
$(L^{-1})^*=(L^*)^{-1}$.\\
$L^{**}=L$.
\proof Фиксируем ортонормированный базис. Тогда все свойства следуют из свойств транспонирования и сопряжения ($\ovl{AB} = \ovl{ A} \,\ovl{B}$).
Однако их можно показать и из определения сопряжённого оператора. Например,
$$\lan x, ABy\ran = \lan A^*x, By\ran = \lan B^*A^*x,y\ran,$$
откуда видно, что $(AB)^*=B^* A^*$. 
\endproof
\elm




\dfn Пусть $L$ оператор на евклидовом или унитарном пространстве $V$ называется самосопряжённым, если $L^*=L$.
\edfn


\rm Пусть $e_1,\dots e_n$ ортонормированный базис, тогда оператор $L$ cамосопряжён тогда и только тогда, когда его матрица в этом базисе удовлетворяет условию $\ovl{L}^{\top}=L$.
\proof Равенство операторов равносильно равенству их матриц в ортонормированном базисе.
\endproof
\erm



\exm\\
0) Любая симметричная матрица $A=A^{\top}$ задаёт самосопряжённый оператор на $\mb R^n$ относительно стандартного скалярного произведения.\\
1) Условие ортогональности оператора можно переписать в виде $L^*L=1$ или, что эквивалентно, $L^*=L^{-1}$. Таким образом, сопряжённый оператор к ортогональному -- это обратный оператор.\\
2) Пусть $\mb R[x]$ пространство многочленов и $g(x,y) \in \mb R[x,y]$ -- многочлен. Тогда сопряжённый к оператору $f \to \int_{a}^b f(y)g(x,y)dy$ это оператор $f\to\int_{a}^b f(x)g(x,y)dx$.\\
3) Сопряжённый оператор к ортогональному -- это обратный к нему.




\section{Спектральные теоремы}

Как всегда при обсуждении линейных операторов разумно задать вопрос про их собственные числа. Для того, чтобы облегчить нашу задачу и не повторять несколько раз одни и те же рассуждения, заметим, что унитарные, самосопряжённые и косоэрмитовы операторы обладают следующим свойством.

\dfn Пусть $L$ оператор на евклидовом или унитарном пространстве $V$. Оператор $L$ называется нормальным, если $LL^*=L^*L$.
\edfn



Мы хотим связать инвариантные пространства относительно $L$ и инвариантные пространства относительно $L^*$. Какой бы ни был оператор верен факт.

\lm \label{normal} Если подпространство $U$ инвариантно относительно $L$, то $U^{\bot}$ инвариантно относительно $L^{*}$.
\proof Пусть $v\in U^{\bot}$. Тогда для всех $u\in U$ верно $\lan L^* v, u\ran = \lan v, Lu\ran =0 $ так как $Lu\in U$, что и требовалось.
\endproof
\elm

\lm Пусть $L$ и $T$ два оператора на комплексном векторном пространстве $V$, которые коммутируют между собой, то есть $LT=TL$. Тогда у $L$ и $T$ есть общий собственный вектор. 
\proof Пусть $\lambda$ -- собственное число $L$. Покажем, что $\Ker L -\lambda E$ -- инвариантное пространство, относительно $T$. Действительно, пусть $v\in \Ker L-\lambda E$. Тогда $(L-\lambda E)Tv=T(L-\lambda E)v=0$.

Теперь, так как $\Ker L-\lambda E$ инвариантно относительно $T$, то у $T$ есть собственный вектор в $\Ker L-\lambda E$. Он же собственный для $L$.
\endproof
\elm

\thrm 
Оператор $L$ на  унитарном пространстве $V$ нормален тогда и только тогда, когда существует ортонормированный базис $e_1,\dots,e_n$ в котором матрица $L$ диагональна.
\proof
Доказательство идёт индукцией по размерности. Если оператор нормален, то у $L$ и $L^*$ есть общий собственный вектор $v_1$, так как они коммутируют.

Возьмём к нему ортогональное дополнение $U=\lan v_1\ran^{\bot}$. Это будет инвариантное подпространство для $L$ и $L^*$. При этом ограничение $L^*$ на $U$ -- это сопряжённый к $L|_U$. По индукции это даёт ортонормированный базис для $L$ на $U$, а вместе с $v_1$ базис из собственных векторов на всём $V$.
\endproof
\ethrm

\crl Если $L$ -- нормальный оператор на унитарном пространстве $V$, то $$V=\bigoplus_{\lambda - \text{ с.ч. } L} \Ker L-\lambda E.$$
При этом $\Ker L-\lambda_i E$ попарно ортогональны при различных $\lambda_i$.
\ecrl


Прежде чем перейти к вещественному случаю лемму про общее строение вещественных матриц.



\lm Пусть $A$ вещественная квадратная матрица размера $n$. Тогда если $v \in \mb C^n$ собственный вектор $A$ с собственным числом $\lambda$, то $\ovl{v}$ собственный вектор $A$ с собственным числом $\ovl{\lambda}$. В частности, комплексное сопряжение осуществляет биекцию между $\Ker A-\lambda E$ и $\Ker A-\ovl{\lambda} E$.
\proof Действительно
$$\ovl{\lambda}\ovl{v}=\ovl{\lambda v}=\ovl{Av}=\ovl{A}\ovl{v}=A\ovl{v}.$$
\endproof
\elm 




\thrm
Оператор $L$ на евклидовом пространстве $V$ нормален  тогда и только тогда, когда существует ортонормированный базис в котором его матрица $A$ блочно-диагональная, при этом блоки имеют  или размер $1\times 1$ или $2\times 2$ вида
$$\begin{pmatrix}
a  & b\\
-b & a
\end{pmatrix}.$$
Такое представление единственно с точностью до порядка блоков.
\ethrm
\proof
Прежде всего покажем единственность блочной структуры. Пусть $e_i$ -- ортонормированный базис в котором $L$ имеет матрицу в указанном виде. Тогда блочное представление матрицы даёт возможность посчитать характеристический многочлен. Клетка $1\times 1$ даёт множитель $t-\lambda$, то есть соответствует вещественному собственному числу. Клетке $2\times 2$ соответствует множитель в характеристическом многочлене $t^2-2at+a^2+b^2$. Этот многочлен раскладывается на множители $$t^2-2at+a^2+b^2= (t-a-bi)(t-a+bi).$$
Если $b\neq 0$, то корни этого многочлена не вещественны, а если равно 0, то мы попадаем в уже разобранную диагональную ситуацию. Тогда количество клеток вида 
$$\begin{pmatrix}
a  & b\\
-b & a
\end{pmatrix}$$
с $b\neq 0$ равно кратности корня $a+bi$ у характеристического многочлена.



Перейдём к доказательству существования. Достаточно доказать существование такого базиса для матриц $A\in M_n(\mb R)$, что $A^\top A=AA^\top$. Такая матрица задаёт нормальный оператор на только на $\mb R^n$, но и на $\mb C^n$. Этим и воспользуемся.

Рассмотрим все собственные числа $\lambda$ матрицы $A$. Если $\lambda\in \mb R$, то $\Ker A-\lambda E$ имеет ортонормированный базис из векторов с вещественными компонентами. Пусть  $\lambda = a+bi$ с $b\neq 0$. Пусть  $v=e_1 + i e_2 $ -- собственный вектор для $\lambda$. Здесь $e_1=\Re v$, а $e_2=\Im v$ -- вещественные вектора. Тогда $\ovl{v}=e_1-ie_2$ собственный вектор для $\ovl{\lambda}=a-bi$. Тогда
$$\lan e_1, e_2\ran = \frac{1}{4} \lan v+\ovl{v}, -i( v-\ovl{v})\ran= \frac{-i}{4}(||v||^2-||\ovl{v}||^2)=0.$$
Покажем, что норма $e_i$ равна $\frac{1}{\sqrt{2}}$, например, для случая $e_1$. 
$$||e_1||^2=\lan e_1, e_1\ran = \frac{1}{4} \lan v+\ovl{v},  v+\ovl{v}\ran= \frac{1}{4}(||v||^2+||\ovl{v}||^2)=\frac{1}{2}.$$

Итого, вектора $\sqrt{2}e_1,\sqrt{2}e_2$ вещественны, а так же независимы ортогональны и нормированы над $\mb C$ и, следовательно, над $\mb R$. Подпространство, порождённое ими инвариантно, относительно $A$. На нём в базисе $e_1, e_2$ матрица $A$ действует как 
$$e_1 \to \frac{1}{2}(\lambda v + \ovl{\lambda}\ovl{v})=ae_1 - be_2 $$
$$e_2 \to \frac{1}{2i}(\lambda v - \ovl{\lambda}\ovl{v})=\frac{1}{2i}( 2i b e_1 + 2i  a e_2) $$
Разумеется, она действует так же и в базисе $\sqrt{2}e_1,\sqrt{2}e_2$. Откуда получаем, что пространство $\lan e_1,e_2\ran $ инвариантно относительно $A$ над $\mb R$, и в базисе $\sqrt{2}e_1,\sqrt{2}e_2$ матрица $A$  действует как  
$$\begin{pmatrix}
a & b\\
-b & a
\end{pmatrix}$$
Из этих соображений построим ортонормированный базис всего $\mb R^n$. Для собственных чисел $\lambda \in R$ возьмём вещественный ортонормированный базис $\Ker A-\lambda E$, а для пары сопряжённых комплексных собственных чисел $\lambda,\ovl{\lambda}$ возьмём $v_1,\dots,v_k$ -- ортонормированный базис $\Ker A-\lambda E$ и по каждому вектору построим вещественные вектора $e_{i,1}=\Re v_i$ и $e_{i,2}=\Im v_i$. Тогда $\sqrt{2}e_{1,1}, \sqrt{2}e_{1,2},\dots, \sqrt{2}e_{k,1},\sqrt{2}e_{k,2}$ -- это ортонормированный набор векторов. При этом, подпространства, порождённые каждой парой векторов с одинаковым первым индексом инвариантны относительно $A$ и линейый оператор, заданный $A$ на этом подпространстве имеет матрицу нужного вида.

Вместе, эти вектора дают искомый ортонормированный базис $R^n$.

\endproof






Теперь можно легко получить характеризацию самосопряжённых, унитарных и вещественных ортогональных.

\thrm Пусть $L$ -- оператор в евклидовом (унитарном) пространстве $V$. Тогда $L$ -- самосопряжённый тогда и только тогда, когда существует ортонормированный базис $V$ состоящий из собственных векторов оператора $L$ и все собственные числа $L$ -- вещественны.
\proof Пусть $L$ оператор на унитарном пространстве. Возьмём ортонормированный базис из его собственных векторов и распишем условие самосопряжённости. Оно означает, что $\ovl{A}^{\top}=A$. Но $A$ диагональна и на диагонали стоят собственные числа. Итого на них получается уравнение $\ovl{\lambda}=\lambda$, что гарантирует их вещественность.

Пусть теперь  $L$ оператор на евклидовом пространстве. Тогда есть ортонормированный базис, в котором матрица $L$ составлена из блоков $1\times 1$ или $2\times 2$ вида 
$$\pmat a& b \\ -b & a \epmat.$$
Но матрица $L$ в ортогональном базисе должна быть симметрична. Отсюда $b=-b$, то есть $b=0$, что и требовалось.
\endproof
\ethrm



\zd Докажите спектральную теорему в случае самосопряжённого оператора напрямую.
\ezd

\thrm Оператор $L$ -- унитарный тогда и только тогда, когда его собственные числа по модулю равны 1 и существует ортонормированный базис из собственных векторов. 
\proof Рассмотрим ортонормированный базис из собственных векторов $e$. Матрица $A$ оператора $L$ в этом базисе диагональна и удовлетворяет соотношению $\ovl{A}^{\top}A=E_n$. Для собственных чисел $A$ это означает, что $$|\lambda|^2=\ovl{\lambda}\lambda=1.$$ 
\endproof
\ethrm






\thrm Оператор $L$ на евклидовом пространстве $V$ ортогональный  тогда и только тогда, когда существует ортонормированный базис в котором матрица $A$ блочно-диагональная, при этом блоки имеют размер $1$ и состоят из $\pm 1$ или имеют размера $2$ и имеют вид
$$\begin{pmatrix}
\cos \varphi & \sin \varphi\\
-\sin \varphi &\cos \varphi
\end{pmatrix}.$$
\ethrm
\proof
По вещественной версии спектральной теоремы для самосопряжённых операторов есть ортонормированный базис в котором матрица $L$ состоит из блоков $1\times 1$ или $2\times 2$ вида 
$$\pmat a& b \\ -b & a \epmat.$$
Но матрица $L$ в ортогональном базисе должна быть ортогональной и, следовательно, вещественной и унитарной. Откуда получаем, что её собственные числа по модулю равны 1. Если эти числа вещественные, то они равны $\pm 1$. Если же они не вещественные, то имеют вид $a+bi=\cos \ffi + i \sin \ffi$, что даёт необходимый вид для $a$ и $b$.

Обратно, оператор $L$ ортогонален так как его матрица ортогональна в ортонормированном базисе по условию теоремы. 
\endproof













\section{Задачи на максимизацию}

Теперь обратимся к вопросам, связанным с вещественными самосопряжёнными операторами.

Рассмотрим один из вопросов, связанных с такой конструкцией, а именно, рассмотрим задачу о нахождении нормы линейного оператора $L \colon U \to V$ между двумя евклидовыми пространствами. Для того, чтобы найти  норму необходимо найти
$$\max_{x\neq 0}\sqrt{\frac{\lan Lx,Lx\ran}{\|x\|^2}}=\sqrt{\max_{x\neq 0}\frac{\lan L^*Lx,x\ran}{\|x\|^2}}=\sqrt{\max_{\|x\|=1} \lan L^*Lx,x\ran}.$$
Таким образом, нахождение нормы оператора свелось к задаче максимизации некоторой квадратичной формы на единичной сфере. Заметим, что максимум действительно достигается благодаря компактности сферы.

Оказывается, что довольно легко найти максимум или минимум произвольной квадратичной формы на сфере. Для этого мы заметим, что по любому самосопряжённому оператору $A$ на евклидовом пространстве $V$ строится квадратичная форма $q(x)=\lan x, Ax\ran$ и обратно, оператор можно восстановить по этой квадратичной форме. Точнее, если $v_1,\dots,v_n$ -- ортонормированный базис, то матрица $A$ в этом базисе совпадает с матрицей $q$. Теперь докажем теорему.



\thrm Пусть $V$ -- евклидово пространство, $A$ -- самосопряжённый оператор на $V$, а $q(x)=\lan x,Ax\ran$ -- соответствующая квадратичная форма. Тогда 
$$\max_{ x\in V } \frac{q(x)}{||x||^2}=\max_{\substack{ x\in V \\ ||x||=1}} q(x)=\lambda_1,$$
где $\lambda_1$ - наибольшее собственное число оператора $A$ и достигается на собственном векторе $v_1$, соответствующему $\lambda_1$. Аналогично минимум равен минимальному собственному числу $A$. 
\proof
Пусть $v=\sum c_i e_i$, причём $1=||v||^2=\sum c_i^2$. Тогда $\lan Av,v\ran = \sum c^2_i \lambda_i $, что меньше $\lan A e_1,e_1\ran= \lambda_1= \sum \lambda_1 c_i^2$.
\endproof
\ethrm

Эта теорема, кроме, собственно, решения задачи, даёт геометрическую характеризацию первого собственного числа для самосопряжённого оператора. Вопрос: можно ли аналогично охарактеризовать другие собственные числа? Ответ получается не таким простым, но, тем не менее, полезным.

\thrm[Куранта-Фишера] Пусть $q(x)=\lan x, Ax\ran$, где $A$ -- самосопряжённый оператор на евклидовом пространстве. Тогда $k$-ое по убыванию собственное число $\lambda_k$ для $A$ есть 
$$\lambda_k=\max_{\dim L=k} \min_{\substack{ x\in L \\ ||x||=1}} q(x) = \min_{\dim L=n-k+1} \max_{\substack{ x\in L \\ ||x||=1}} q(x).$$
Причем максимум достигается на инвариантном подпространстве, содержащем собственные вектора для $\lambda_1,\dots,\lambda_k$.
\ethrm
\proof Будем доказывать только первое описание. Прежде всего заметим, $\lambda_k$ достигается, если взять $U=\lan v_1,\dots,v_k\ran$. Дейстительно, в этом случае, так как $U$ инвариантно относительно $A$ форме $q|_U$ просто соответствует ограничение $A|_U$, а его максимум уже легко посчитать так как нам известны его собственные числа.

Пусть $U$ --- подпространство на котором достигается максимум, причём допустим, что максимум больше $\lambda_k$. Тогда рассмотрим подпространство $W=\lan v_k,\dots,v_n\ran$, где $v_i$ --- собственный вектор соответствующий $i$-ому по убыванию собственному числу. Заметим, что $\dim U\cap W\geq 1$, из формулы Грассмана. С другой стороны, если мы возьмём вектор $x\in U \cap W$ с нормой $1$, то значение $q(x)$ с одной стороны меньше или равные $\lambda_k$, так как $x\in W$, а с другой стороны строго больше, так как $x\in U$. Полученное противоречие и доказывает теорему.  
\endproof




\crl Пусть $U$ некоторое подпространство размерности $m$ евклидового пространства $V$, а $q(x)=\lan x, Ax\ran$, где $A$ -- самосопряжённый оператор. Пусть собственные числа $A$ --- это $\lambda_i$, а собственные числа оператора, соответствующего $q(x)|_U$ -- это $\mu_i$, упорядоченные по убыванию. Тогда 
$$\lambda_{i+n-m}\leq \mu_i\leq \lambda_i.$$  
\proof Заметим, что
$$\mu_i=\max_{\substack{L\leq U\\ \dim L=i}} \min_{\substack{ x\in L \\ ||x||=1}} q(x),$$
что очевидно меньше, чем 
$$\max_{\substack{L\leq V\\ \dim L=i}} \min_{\substack{ x\in L \\ ||x||=1}} q(x)=\lambda_i.$$
Неравенство в другую сторону получается из второго описания в теореме Куранта-Фишера.
\endproof
\ecrl

В основном это удобно применять для оценки собственных чисел у симметричной подматрицы.

\crl Пусть $A\in M_n(K)$ -- симметричная матрица. Пусть $\Gamma \subseteq \{1,\dots, n\}$ размера $m$. Обозначим за $A^{\Gamma}_{\Gamma}$ матрицу $A$, в которой оставили только элементы из строк и столбцов из $\Gamma$. Пусть $\lambda_i$ -- это собственные числа $A$, а $\mu_i$ -- собственные числа $A^{\Gamma}_{\Gamma}$. Тогда
$$\lambda_{i+n-m}\leq \mu_i\leq \lambda_i.$$
\ecrl 


Введём не совсем стандартное определение:
\dfn
Пусть $q$ -- квадратичная форма на евклидовом пространстве. Рассмотрим ортонормированный базис $u_i$ пространства $V$. Тогда положим 
$$\Tr q= \sum q(u_i).$$ Если в базисе $u_i$ форме $q(x)=x^{\top} Ax $ соответствует симметричная матрица $A$, то $\Tr q(x)=\Tr(A)$.
\edfn

\rm Определение не зависит от выбора ортонормированного базиса. Действительно, если замена координат ортогональна, то матрица $q$ в новой системе координат имеет вид $C^{\top}AC=C^{-1}AC$. Осталось заметить, что след последней матрицы очевидно равен следу $A$.
\erm

\rm Если форме $q$ соответствует самосопряжённый оператор $A$, то $\Tr q=\sum \lambda_i$, где $\lambda_i$ -- собственные числа $A$.
\erm




\crl Пусть $q(x)=x^{\top} Ax$, $U$ --- некоторое подпространство, $\dim U=k$. Пусть собственные числа $A$ --- это $\lambda_i$. Тогда $$\Tr q|_U\leq \sum_{i=1}^k \lambda_i= \Tr q|_{V_k},$$
где $V_k$ подпространство натянутое на первые $k$ собственных векторов $q$.
\proof Нам известны неравенства на собственные числа $\mu_i$, ограничения $q|_U$. А именно, $\mu_i\leq \lambda_i$. Но тогда и $$\sum_{i=1}^k \mu_i \leq \sum_{i=1}^k \lambda_i.$$
\endproof
\ecrl 






\section{Метод главных компонент}

Рассмотрим следующую задачу: имеется массив данных --- набор векторов $x_1,\dots,x_s \in \R^n$. Подразумевается, что точек очень много. Предположим, что при идеальных измерениях между координатами этих векторов есть линейные зависимости. Все отклонения от этой погрешности вызваны небольшими погрешностями. Задача состоит в том, чтобы восстановить линейную зависимость. 


Переформулируем задачу геометрически. Пусть наши вектора удовлетворяют линейным условиям $Ax_i=b$ для некоторой матрицы $A$ ранга $n-k$. Это значит, что они лежат в аффинном подпространстве $\Ker A + a_0$ размерности $k$. Если же нам даны вектора с погрешностями, то задачу таким образом можно поставить в виде: найти аффинное подпространство размерности $k$ наиболее близкое к данным точкам $x_1,\dots,x_s$. Понятие <<наиболее близкое>> требует конкретизации. Вообще говоря, тут есть выбор. Мы будем считать, что подходящее пространство $W=U+a_0$ должно давать минимум следующего выражения:
$$\sqrt{\sum_{i=1}^s \rho(x_i,W)^2} \to \min.$$
Совершенно ясно, что корень квадратный тут для красоты, и минимизировать нужно $\sum_{i=1}^s \rho(x_i,W)^2$.

Прежде всего установим, что какой бы <<минимайзер>> $W=U+a_0$ мы не нашли, в $W$ всегда будет лежать среднее $\frac{1}{s}\sum_{i=1}^s x_i$ и, следовательно, в качестве $a_0$ всегда можно взять среднее. Действительно, распишем условие, что сумма 
$$\sum_{i=1}^s \rho(x_i-a_0, U)^2=\sum \|pr_{U^{\bot}} (x_i-a_0)\|^2$$
минимальна.  Продифференцируем по координатам $a_0$. Получим $$\sum -2pr_{U^{\bot}} x_i + 2s \,pr_{U^{\bot}} a_0=0$$
 Это условие означает, что проекции $a_0$ и среднего $\frac{1}{s}\sum x_i$ на подпространство $U^{\bot}$ совпадают. То есть эти две величины отличаются на элемент $U$. Тогда среднее лежит в $W$. 

Итак, вычтя из всех $x_i$ их среднее можно считать $a_0=0$, а все точки $x_i$ удовлетворяют равенству $\sum_{i=1}^s x_i=0$. Замечу, что это равенство нигде в дальнейшем не будет использовано. Благодаря такой замене мы свели задачу к поиску подпространства $U$ размерности $k$, которое минимизирует 
$$\sum_{i=1}^s \rho(x_i, U)^2=\sum_{i=1}^s \|pr_{U^{\bot}} (x_i)\|^2.$$

Воспользуемся тем, что $\|x\|^2=\|pr_U x\|^2+\|pr_{U^{\bot}} x\|^2$ или в другом виде $\|x\|^2-\|pr_U x\|^2=\|pr_{U^{\bot}} x\|^2$. Получаем, что выражение
$$\sum_{i=1}^s \|pr_{U^{\bot}}(x_i)\|^2=\sum_{i=1}^s \|x_i\|^2-\|pr_U x_i\|^2$$
должно быть минимально. Сумма $\|x_i\|^2$ постоянна. Значит  необходимо и достаточно, чтобы выражение $\sum_i \|pr_U x_i\|^2$ было максимально.


Для того чтобы посчитать проекцию на $U$ в каждом слагаемом выберем в $U$ ортонормированный базис $u_1,\dots,u_k$. Перепишем
$$\sum_{i=1}^s \|pr_U x_i\|^2=\sum_{i=1}^s\sum_{j=1}^k \lan x_i,u_j\ran^2=\sum_{j=1}^k \sum_{i=1}^s \lan x_i,u_j\ran^2.$$

Внутренняя сумма теперь есть значение некоторой квадратичной формы на векторе $u_j$. Разберёмся, что это за форма. Рассмотрим матрицу $X$ размера $s\times n$, чьи строки это  вектора $x_i$ $i\in\ovl{1,s}$. Тогда вектор $d_j=Xu_j$ состоит из скалярных произведений  $\lan x_i, u_j\ran$. Значит 
$$\lan d_j,d_j\ran = (Xu_j)^{\top}Xu_j = \sum_{i=1}^s \lan x_i,u_j\ran^2,$$ 
что совпадает со слагаемым нашей суммы. Рассмотрим симметричную матрицу $A=X^{\top}X$ и обозначим соответствующую ей форму за $q$. Тогда нам надо максимизировать выражение

$$\sum_{j=1}^k q(u_j).$$

Таким образом мы ищем максимум $\Tr q|_{U}$ по всем подпространствам $U$ размерности $k$, где форма $q$ соответствует матрице $X^{\top} X$. А его мы искать умеем. Сформулируем  ответ. 


\thrm  Пусть есть набор векторов $x_1,\dots,x_s \in V$. Определим симметричную, положительно полуопределённую матрицу $A$, как $A=X^{\top}X$, где строчки матрицы $X$ -- это вектора $x_i$. Тогда минимум по всем аффинным подпространствам $V$ размерности $k$ выражения $\sum_{i=1}^s \rho(x_i,W)^2$ достигается при $a_0=\frac{1}{s}\sum x_i$  и $U=\lan v_1,\dots,v_k\ran$, где $v_i$ --- собственные вектора $A$, причём соответствующие собственные числа упорядочены по убыванию. 
\ethrm

\rm Стоит немного сказать про вероятностную интерпретацию полученного ответа. Предположим, что точки $x_i\in \R^n$ сгенерированы каким-то случайным процессом и подчиняются некоторому общему распределению. Тогда вектор $a_0$ -- это просто оценка на математическое ожидание этого распределения.

После того как мы нашли $a_0$ мы центрируем набор $x_i$ и минимизируем $\sqrt{\sum \rho (x_i, U)^2}$. Представим себе, что $U=\{0\}$. Что значит эта сумма? Это то, что мы назвали бы оценкой дисперсией величины (конечно, надо ещё усреднить). 
А что если $U \neq \{0\}$? Тогда $\sqrt{\sum \rho (x_i, U)^2}$ -- это заготовка для оценки дисперсии проекций случайных величин на $U^\bot$. То есть мы ищем подпространство $U$, так что на ортогональное дополнение приходится минимально возможная дисперсия (а при проекции на само $U$ должен достигаться максимум дисперсии среди всех подпространств той же размерности).

Какой же смысле имеет матрица $X^\top X$, её собственные вектора и собственные числа. Матрица $X^\top X$ -- это с точностью до константы ($1/s-1$) эмпирическая матрица ковариации. Если мы хотим посчитать квадрат дисперсии в направлении $v$, то мы считаем $v\top X^\top X v$. Значит собственные вектора этой матрицы -- это такие вектора, для которых достигается экстремум квадрата дисперсии. А собственные числа -- это собственно квадраты дисперсии.
\erm

\rm Отдельно отметим, что в приложениях принято с самого начала центрировать данные и нормировать каждую компоненту $x_i$ так, чтобы дисперсия вдоль каждого направления была единичной.
\erm



\section{SVD-разложение}

При вычислении с помощью метода главных компонент часто используют несколько другие конструкции. Для того, чтобы в этом разобраться посмотрим на матрицу  $X^{\top}X$  и её собственные числа.  

\dfn Пусть $A$ -- линейное отображение между евклидовыми пространствами $U \to V$. Тогда сингулярными значениями $A$ называются корни из положительных собственных чисел оператора $A^*A$.
\edfn



Теперь обсудим важную конструкцию, проясняющую геометрический смысл сингулярных значений.


\thrm[SVD разложение] Пусть $L$ -- линейное отображение $L\colon U \to V$ между евклидовыми пространствами. Тогда существуют такие ортонормированный базисы $U$ и $V$, что матрица $L$ имеет вид 
$$\Sigma=\pmat \sigma_1 &\dots& 0 & 0\\
 \vdots & \ddots &\vdots & \vdots\\
 0 & \dots & \sigma_r & 0\\
 0 &  \dots & 0 & 0 \epmat,$$
 где $r$ -- ранг $L$. Числа $\sigma_1, \dots, \sigma_r$ на диагонали обязаны быть равными сингулярным значениям $L$.
На языке матриц это означает, что для любой матрицы $A \in M_{m\times n}$ существуют ортогональные матрицы  $P$ -- размера $m$ и $Q$ -- размера $n$,  что
$$A= P \Sigma Q.$$
 
\proof Рассмотрим оператор $B = L^{*}L$. Тогда существуют ортонормированный базис $e_1,\dots,e_n$ в котором оператор $B$ диагонален, с неотрицательными числами на диагонали $d_1\geq\dots\geq d_n\geq 0$. Имеем  $d_i=\sigma_i^2$ для единственного положительного $\sigma_i$. 
Посмотрим на вектора $Le_i \in U$. Они ортогональны. Действительно
$$\lan Le_i, Le_j\ran = \lan L^{*}Le_i,e_j \ran = \lan d_i e_i,e_j\ran,$$
что равно нулю, если $i\neq j$. В случае $i=j$ получаем $\|e_i\|^2=d_i$. Возьмём 
$$f_i=\frac{Le_i}{\sqrt{d_i}}$$
и дополним этот набор до ортонормированного базиса пространства $U$. Итого имеем $e_1,\dots,e_n$ ортонормированный базис $U$ и $f_1,\dots,f_m$ -- ортонормированный базис $V$.
Посмотрим матрицу $A$ в этих базисах. По определению $Le_i=\sqrt{d_i}f_i$. Это и даёт требуемый вид матрице оператора $L$.


Напоследок осталось решить вопрос, как выглядит матрица $Q$. В нашей конструкции матрица $Q$ есть матрица замены координат из стандартного базиса в базис из собственных векторов $e_i$ матрицы $A^{\top}A$. Если за $C$ обозначить матрицу из столбцов $e_i$, то $Q=C^{-1}$, но $C$ ортогональна и поэтому можно написать $Q=C^{T}$, то есть строки $Q$ -- собственные вектора $A^{\top}A$.
\endproof
\ethrm

\upr Получите аналогичное описание для $P$.
\eupr

\dfn Базисные вектора такой системы координат в $U$ называются левыми сингулярными векторами $A$, а базис в $V$ -- правыми.  
\edfn

Наличие SVD-разложения означает, что для всякого линейного отображения можно так выбрать декартову систему координат, что в этой системе координат это отображение будет выглядеть как растяжение вдоль каких-то осей.





SVD-разложение используется в практическом решении задачи из метода главных компонент и позволяет сразу найти не только пространство, но и проекцию начальных точек на него. Формализуется это так: рассмотрим матрицу $X$, чьи строки равны $x_i^{\top}$. Тогда если все её строки заменить на их проекции на оптимальное подпространство $V_k$, то получится матрица ранга $k$ или меньше.

Если составить из этих проекцией матрицу $X^{(k)}$, то $\rho(\{x_i\}, V_k)^2= \|X-X^{(k)}\|_F^2 $, где 
$$\|X\|_F=\sqrt{\sum_{i,j} x_{ij}^2}=\sqrt{\Tr X^{\top}X}$$
есть нормы на пространстве матриц. Эта норма, называется нормой Фробениуса.

Оказывается, что так построенная матрица $X^{(k)}$, является ближайшей матрицей ранга $\leq k$ к матрице $X$
Действительно, рассмотрим матрицу $Y^{(k)}$, которая приближает $X$ не хуже $X^{(k)}$. То есть 
$$\|X -Y^{(k)}\|_F^2 \leq \| X-X^{(k)}\|_F^2= \sum \rho(x_i, V_k)^2.$$
Возьмём $L=\Im Y$ -- подпространство размерности $k$. Тогда матрица ${Y^{(k)}}'$ чьи столбцы есть проекции столбцов $X$ на $L$ ближе к $X$, чем $Y^{(k)}$ и равенство достигается, только если $Y^{(k)}={Y^{(k)}}'$. В этой ситуации оптимальная матрица как раз и приходит из пространства $L$ что нам бы и хотелось. Покажем это равенство.

Заметим по построению $\| X - {Y^{(k)}}'\|_F = \sum \rho (x_i, L)^2$, что обязано быть больше или равно чем $\sum \rho(x_i, V_k)^2$. Значит имеет место равенство. Но тогда и матрицы $Y^{(k)}$ и ${Y^{(k)}}'$ одинаково хорошо приближают $X$. Значит $Y^{(k)}={Y^{(k)}}'$ приходят из оптимального подпространства в методе главных компонент. Почему мы не говорим, что эти матрицы не обязательно равны $X^{(k)}$? Потому что оптимум не единственен (если есть кратные собственные числа у $X^\top X$).


Таким образом, нахождение проекций точек на оптимальное, с точки зрения метода главных компонент подпространство можно переформулировать, как нахождение ближайшей к $X$ матрицы ранга меньше или равного $k$. Это легко сделать, зная SVD-разложение.

Прежде чем использовать SVD-разложение заметим, что в такой формулировке задача может быть поставлена в бескоординатном виде: для данного линейного отображения $L$ между евклидовыми пространствами найти наиболее близкое к нему линейное отображение ранга $k$ относительно нормы $\|L\|=\sqrt{\Tr L^*L}$.

\thrm Пусть $L$ -- линейное отображение между евклидовыми пространствами. Пусть $\Sigma$ -- матрица с сингулярными значениями $L$ на диагонали. Тогда ближайшее к $L$ отображения ранга $k$ имеет матрицу $\Sigma^{(k)}$ в базисах из правых и левых сингулярных векторов. Здесь  на диагонали $\Sigma^{(k)}$ стоят вначале $\sigma_1,\dots,\sigma_{k}$, а остальное -- нули.
\proof Перейдём в базис из правых и левых сингулярных векторов. В этих базисах матрица $A$ -- это $\Sigma$.
Значит теперь мы решаем задачу нахождения ближайшей матрицы к матрице $\Sigma$. Для этого нам надо взять проекции строк $\Sigma$ на подпространство порождённое первыми $k$ собственными векторами $\Sigma^\top \Sigma$. После этого из этих проекций надо составить матрицу. Но первые $k$ собственных векторов $\Sigma^\top \Sigma$ это просто первые $k$ координатных векторов. Значит составленная из проекций матрица -- это $\Sigma^{(k)}$.
\endproof
\ethrm 

\crl Пусть $A$ имеет сингулярное разложение $P\Sigma Q$. Тогда ближайшая к $A$ матрица ранга $k$ (в смысле нормы Фробениуса) имеет вид $A^{(k)}=P\Sigma^{(k)}Q$, где на диагонали $\Sigma^{(k)}$ стоят вначале $\sigma_1,\dots,\sigma_{k}$, а остальные элементы матрицы -- нули.
\ecrl

\upr На самом деле матрица $A^{(k)}$ ближайшая к $A$ среди матриц ранга $k$ и в смысле обычной матричной $l_2$-нормы. 
\eupr



\section{Немного вычислительной линейной алгебры}


Перед нами встаёт вопрос: как аккуратно вычислить те объекты линейной алгебры, которые мы определили. Какие для этого есть методы и чем они отличаются. Совершенно понятно, что это огромный круг вопросов и ответить на них на все невозможно. Поэтому я постараюсь обрисовать основные примеры и основные подходы к ответам.

Рассмотрим прежде всего задачу о решении системы линейных уравнений $Ax=b$ с вещественными коэффициентами. 

Во всех реальных приложениях вектор $b$, а часто и матрица $A$ даны не точно, а с некоторой погрешностью. Кроме того, при вычислениях с плавающей точкой мы создаём ещё больше погрешностей. Посмотрим  пример
$$ \pmat 1 & 1 \\ 1 & 1.0001 \epmat x = \pmat 2\\ 2.0001 \epmat$$
У этой системы есть точное решение $x=(1,1)$. Предположим, однако, что из-за ошибок округления вектор $b$ стал равен 
$$b_{new}=\pmat 2 \\ 2 \epmat$$
Решение для $b_{new}$ равно $x=(2,0)^\top$. Видно, что малые изменения коэффициентов системы могут значительно изменить её решение.

Попробуем разобраться, что происходит с общей системой при малых возмущениях её коэффициентов. Для начала разберёмся с погрешностями в $b$.

Обозначим погрешность в значениях свободного члена за $\Delta b$. То есть на самом деле нам дана система $Ay=b+\Delta b$. Самое лучшее, что мы можем сделать -- это точно решить эту новую систему. Насколько решение $y$ этой приближённой системы может отличаться от решения исходной? Пусть $y=x+\Delta x$. Тогда вычитая одно уравнение из другого получаем
$$A \Delta x= \Delta b.$$
Нас будет интересовать прежде всего не абсолютная, а относительная погрешность $\frac{\|\Delta x\|}{\|x\|}$. Оценим её
$$\frac{\|\Delta x\|}{\|x\|}=\frac{\|A^{-1} \Delta b \|}{\|x\|}=\frac{\|A^{-1} \Delta b \|}{\|b\|} \frac{\|b\|}{\|x\|} \leq \|A\| \|A^{-1}\| \frac{\|\Delta b\|}{\|b\|}.$$
Заметим, что здесь мы пользовались определением нормы матрицы при помощи нормы на исходном пространстве, но сам вид нормы на исходном пространстве нам не важен.

Получается, что чтобы мы не делали, просто из-за изначальных погрешностей в данных мы не можем рассчитывать  на погрешность меньшую $\kappa(A)\frac{\|\Delta b\|}{\|b\|}$
(чуть позже мы увидим, что эта оценка точная). 

\dfn Число $\kappa(A)$ называется числом обусловленности матрицы $A$.
\edfn

Оказывается, что число обусловленности отвечает и за влияние погрешностей в коэффициентах матрицы $A$.

Действительно, если посмотреть на решение возмущённой системы $(A+\Delta A)(x+\Delta x)=b$, то вычитая точную систему получаем
$$\Delta x= -A^{-1}\Delta A (x+\Delta x)$$
Переходя к нормам получаем
$$\frac{\|\Delta x\|}{\|x+\Delta x\|}\leq \|A^{-1}\| \|A\| \frac{\|\Delta A\|}{\|A\|}$$

Как найти $\kappa(A)$? Ответ зависит от выбранной нормы. Нам проще всего понять, как найти $\kappa(A)$ в случае евклидовой нормы на $\R^n$. В этой ситуации $$\kappa_2(A)=\frac{\sigma_1}{\sigma_n}.$$
Двойка снизу тут в честь $l_2$-нормы.
Покажем теперь, что оценка погрешности при помощи $\kappa_2(A)$ точная. Для этого перейдём в базисы из сингулярных векторов. Тогда можно считать, что $A=\Sigma$ -- диагональна. Возьмём вектора $x,b,\Delta x$ и $\Delta b$ следующим образом:
$$x=\pmat 1 \\ 0\\ \vdots \\ 0 \epmat,\, b=Ax=\pmat \sigma_1 \\ 0\\ \vdots \\ 0 \epmat, \, \Delta b = \pmat 0 \\ \vdots \\ 0 \\ \eps \epmat, \, \Delta x=  \pmat 0 \\  \vdots \\ 0 \\ \eps/\sigma_n\epmat.$$
Для таких векторов оценка достигается.

Можно ли заранее оценить $\kappa(A)$? Можно. Но это не так просто и мы это обсуждать не будем. Замечу только, что главная сложность состоит в оценке $\|A^{-1}\|$.

\dfn
Говорят, что система $Ax=b$ хорошо обусловлена, если $\kappa(A)$ -- мало.
\edfn

Однако мы пока ничего не сказали про особенности методов, могут ли они существенно добавить к ошибкам? Оказывается, что могут. Рассмотрим систему 
$$\pmat 0.0001 & 1 \\ 1 & 1 \epmat x= \pmat 1 \\ 2 \epmat $$
Допустим мы храним только три значащих цифры. Применяя напрямую метод Гаусса последнее уравнение преобразуется к виду $-9999 x_2=-9998 $. Округление его точного решения есть $x_2=0.9999$ (если округлить до 4-го знака), откуда получаем $x_1=1$ (что тоже будет верным до 4-го знака). Но если мы используем округление для коэффициентов матрицы на первом шаге, то предыдущее равенство принимает вид $10000x_2 = 10000$. Получаем, что $x_2'=1$ и $x_1'=0$. Что значительно отличается от точного решения.

\rm Отметьте, что $\kappa_2(A)$ в этом примере всего лишь $2.61...$.
\erm

\dfn Метод называется устойчивым, если ошибки в ходе вычисления имеют порядок, сравнимый с ошибками от возможных погрешностей в исходных данных.
\edfn

Метод Гаусса не является устойчивым методом решения систем линейных уравнений. Можно его доработать, на каждом шаге переставляя строки матрицы так, чтобы каждый раз выбирать в качестве главного элемента  наибольший элемент в столбце. Однако это всё равно решает не все проблемы (см. Wilkinson).

В качестве альтернативы можно посмотреть различные методы, которые находят $QR$ разложение. Однако и $QR$ разложение может давать побочные эффекты.





Есть другой класс методов, позволяющих довольно легко найти приближённое решение. Речь идёт про  итерационные методы. Разберём самый простой пример. Пусть нам дана система линейных уравнений вида $x=Ax+b$, где $A$ -- квадратная матрица. Заметим, что любая система может быть представлена в таком виде. 

Предположим в дальнейшем, что собственные числа $A$ по модулю меньше $1$. Возьмём начальный вектор $x_0$ и построим последовательность
$$x_i=Ax_{i-1}+b.$$
Покажем, что при указанных условиях, эта последовательность стремится к решению системы. Прежде всего заметим, что матрица $E-A$ обратима, так как $0$ не является её собственным числом, и, следовательно, система имеет единственное решение. Пусть это решение --- $x'$. Обозначим $h_i=x_i-x'$. Для $h_i$ получается соотношение:
$$x'+h_i=Ax'+Ah_{i-1}+b.$$
Откуда получаем, что $h_i=Ah_{i-1}$. Но тогда $h_n=A^n h_0$ стремятся к нулю при $n\to \infty$. Значит $x_n\to x'$.


Можно показать, что этот метод устойчив, если имеет место сходимость. Кроме того, заметим, что каждый шаг итерации вычисляется за $O(n^2)$ или даже меньше, если в матрице $A$ много нулей.

Есть более интересные вариации на тему метода итераций. Например, метод Гаусса-Зейделя. Оказывается, что совершенно не обязательно смотреть на представление системы именно в виде $x=Ax+b$. Пусть матрица $A$ есть сумма $A=L+D+U$ (нижнетреугольная -- диагональная -- верхнетреугольная). Представим систему $Ax=b$ в виде 
$$(L+D)x=-Ux+b$$
Если мы определим последовательность $x_i$ по правилу $(L+D)x_{i+1}=-Ux_i+b$, то за сходимость такой последовательности будут отвечать собственные числа матрицы $-(L+D)^{-1}U$. 

Метод Гаусса-Зейделя чаще сходится и скорость сходимости для него обычно более высокая, чем для простого метода итераций (но не всегда). Кроме того, так как матрица $(L+D)$ нижнетреугольная, то решение системы с такой матрицей требует $O(n^2)$ операций. То есть в итоге, каждый шаг стоит $O(n^2)$ операций как и в методе простых итераций.







\subsection{Нахождение собственных чисел и собственных векторов}

Хотелось бы уметь оценивать размер собственных чисел матрицы $A$. Можно ли это сделать? Оказывается, что можно. 


Пусть $A$ -- вещественная или комплексная квадратная матрица. Рассмотрим строки $A$. Зададим элементы $r_i=\sum_{j\neq i} |a_{ij}|$. Рассмотрим множество замкнутых кругов радиуса $r_i$ с центром в $a_{ii}$ на комплексной плоскости. Эти круги называются кругами Гершгорина. 

\thrm Все собственные числа матрицы $A$ лежат в объединении кругов Гершгорина. 
\ethrm
\proof Пусть $\lambda$ -- собственное число $A$, а $v$ -- соответствующий собственный вектор. Тогда $Av=\lambda v$. Посмотрим на максимальный по модулю элемент $v$ -- $v_i$ и соответсвующую ему строчку. Покажем, что $\lambda$ лежит в круге Гершгорина с номером $i$. Имеем $\sum a_{ij}v_j =\lambda v_i$  и, значит, $\sum_{j\neq i} a_{ij}v_j= (\lambda - a_{ii})v_i $. Имеем неравенство $$|\lambda -a_{ii}||v_i| \leq |v_i|\sum_{j\neq i} |a_{ij}|$$
Сокращая на $v_i$ получаем требуемое. 
\endproof

\rm В частности, это рассуждение даёт критерий невырожденности матрицы -- матрица $A$ невырождена, если сумма модулей её внедиагональных элементов каждой строки меньше модуля диагонального элемента. Такие матрицы называются матрицами с диагональным преобладанием
\erm

\rm
Переходя от матрицы к транспонированной можно получить ещё одну оценку, которая получается из рассмотрения столбцов $A$.
\erm

А можно ли точно сказать, в каком из кругов находится собственное число? Оказывается, что иногда можно. Для того чтобы это показать нам нужна некоторая аналитическая подготовка.

\fct[Корни непрерывно зависят от коэффициентов] Пусть $f(x,t)$ -- комплексный многочлен от двух переменных со старшим коэффициентом $1$ при $x^n$, как многочлена над $\C[t]$. Пусть $\lambda_1,\dots,\lambda_n$ -- корни $f(x,0)$, а $\mu_1,\dots, \mu_n$ -- корни $f(x,1)$. Тогда cуществуют такие непрерывные на $[0,1]$ функции $\lambda_i(t)$, что $f(\lambda_i(t),t)=0$ и $\lambda_i(0)=\lambda_i$, а $\lambda_i(1)=\mu_{\sigma(i)}$ (для некоторой перестановки $\sigma \in S_n$).
\efct

\thrm Если $k$ кругов Гершгорина матрицы $A\in M_n(\C)$ не пересекаются с остальными, то в них лежат $k$ собственных чисел с учётом кратности.
\ethrm
\proof Рассмотрим матрицу $A_t= D+ tB$, где $D$ -- это матрица из диагональных элементов матрицы $A$, а $B$ -- из внедиагональных. Применим факт для характеристического многочлена матрицы $A_t$. Пусть  $\lambda_1(t),\dots,\lambda_k(t)$ -- непрерывные функции, соответствующие выбранным кругам Гершгорина $B_1,\dots,B_k$. Покажем, что каждая из них не вышла за пределы объединения этих кругов в момент $t=1$. 

Пусть скажем $\lambda_1$ при $t=1$ оказался в другом круге. Круги Гершгорина замкнуты. Пусть 
$$r=\rho(B,\cup_{i=1}^k B_i),$$
где $B$ -- это объединение оставшихся кругов. Рассмотрим функцию $f(t)=\rho(\lambda_1(t),B)$,  Это непрерывная по $t$ функция при этом $f(1)=0$. Значит есть такое $t$, что $0<f(t)<r$. С одной стороны, эта точка должна лежать в кругах Гершгорина, но с другой, из определения $r$ следует, что она должна лежать вне всех кругов.  
\endproof

Как это применить к оценки возмущения собственных чисел? Предположим, что $A$ диагонализуема:
$$A=C\pmat \lambda_1 && \\ & \ddots & \\ && \lambda_n \epmat C^{-1}=CDC^{-1}.$$
Рассмотрим сумму $A+\Delta A$. Как оценить собственные числа этой матрицы, через собственный числа $A$?

Пусть $\| \Delta A\|=\eps$. Рассмотрим матрицу
$$C^{-1}(A+\Delta A) C= D+C^{-1}\Delta A C $$
Надо оценить её собственные числа. Для этого заметим, что позиции  $i,j$ матрицы $C^{-1}\Delta A C$ стоит элемент размера не более $\| C^{-1}\| \|C\| \eps$. По теореме о кругах Гершгорина собственные числа $A+\Delta A$ находятся в кругах радиуса $n \| C^{-1}\| \|C\| \eps $ c центрами в $\lambda_i$. Кроме того, для достаточно маленького $\eps$ пары таких кругов не пересекаются или вложены друг в друга. Отсюда

\crl Для достаточно маленького возмущения $\Delta A$ в круге радиуса  $n \| C^{-1}\| \|C\| \|\Delta A\|$ с центром в с.ч.  $\lambda$ матрицы $A$ находится ровно $k$ собственных чисел $A+\Delta A$, где $k$ -- это кратность $\lambda$ у $A$.
\ecrl

Отсюда видно, что  роль, аналогичную числу обусловленности, в оценке на собственные числа играет число $\|C\|\|C^{-1}\|$. На самом деле для каждого собственного числа можно ввести своё число обусловленности. В любом случае, видно, что оценить это число заранее не находя матрицу $C$ не получится. Тем не менее что-то сделать можно. Для произвольной системы это число уже можно оценить, если мы приближённо вычислили $C$ и $C^{-1}$. Однако, есть матрицы для которых это число и так наилучшее возможное. Что это за матрицы?


Раньше, для того, чтобы найти спектр оператора мы считали характеристический многочлен. Однако, его вычисление довольно трудоёмко. Кроме того, после этого необходимо найти ещё и корни характеристического многочлена.


Однако, есть ряд других методов нахождения собственных чисел. С первой идеей мы уже знакомы -- это вариация метода итераций: рассмотрим случайный вектор $x$. Посмотрим на последовательность $A^kx/\|A^kx\|$ $k\to \infty$. Она стремится к максимальному собственному вектору для почти любого $x$. Легко оценить и собственное число.

Если предположить симметричность матрицы $A$. Заметим, что  для почти любого $x$ 
$$\frac {\lan A^{k+1}x, A^k x \ran}{\lan A^k x, A^k x \ran} \to \lambda, $$
где $\lambda$ -- максимальное собственное число.

Есть ещё множество методов для нахождения собственных чисел. 

Есть и метод нахождения всех одновременно собственных чисел: $QR$-алгоритм. Он заключается в следующем: вначале возьмём $A_0=A$. На $k$-ом шаге для матрицы $A_{k-1}$ строится $QR$-разложение $A_{k-1}=Q_kR_k$, а затем строится матрица $A_k=R_kQ_k$. Оказывается, что этот процесс в случае симметричной $A$ сойдётся к диагональной матрице, чьи диагональные элементы  и есть собственные числа $A$. 

Почему этот алгоритм вообще работает и как до него догадаться.

\lm Имеет место соотношение $A_k=Q_k^\top A_{k-1} Q_k$.
\elm

Определим $Q^{(k)}=Q_1\dots Q_k$ и $R^{(k)}= R_k \dots R_1$. В частности, из предыдущей леммы получаем, что $$A_k=Q_k^\top \dots Q_1^\top A Q_1 \dots Q_k= {Q^{(k)}}^\top A Q^{(k)}.$$

\lm  Имеет место соотношение. 
$$A^k=Q^{(k)}R^{(k)}.$$
\elm
\proof Докажем по индукции. 
$$A^k=A A^{k-1}= Q^{(k)}A_k{Q^{(k)}}^\top Q^{(k-1)}R^{(k-1)} = Q^{(k)}A_k Q_k^\top R^{(k-1)}=Q^{(k)} R^{(k)}$$
\endproof
Для того, чтобы просто сформулировать и доказать результат о сходимости QR-алгоритма мы ограничимся положительно определёнными симметричными матрицами.

\thrm Пусть $A$ -- симметричная матрица с различными положительными собственными числами. Тогда столбцы $Q^{(k)}$ стремятся к собственным векторам $A$, а матрицы $A_k$ -- к диагональной матрице, где на диагонали расположены собственные числа $A$.
\ethrm
\proof  Пусть $u_1,\dots,u_n \in \R^n$ -- ортонормированный базис из собственных векторов $A$ упорядоченный по убыванию собственных чисел.
Разложим стандартный базисный вектор $e_j$ по базису $u_i$
$$e_j = \sum_i c_{ij} u_i $$
Применим матрицу $A^k$. 
$$A^k e_j = \sum_i c_{ij} \lambda_i^k u_i.$$
Предположим, что главные миноры матрицы $C$ не равны нулю.
Как мы уже доказали, матрица $Q^{(k)}$ -- есть результат ортогонализации (с точностью до $\pm 1$) столбцов матрицы $A^k$, то есть векторов $A^k e_j$. Обозначим столбцы $Q^{(k)}$ за $q^{(k)}_j$ Посмотрим на первый столбец $A^k$. Исходя из нашего предположения, $c_{11}$ не ноль и
$$q^{(k)}_1=\frac1{\|A^ke_1\|} A^k e_1 = \pm u_1 + o(1),$$
при $k\to \infty$. Меняя при необходимости направление $u_1$ на противоположенное избавимся от $\pm$ в формуле. Посмотрим, что происходит при ортогонализации второго столбца. Вычислим скалярное произведение:
$$\frac{\lan A^k e_2, A^k e_1 \ran}{\lan A^k e_1, A^k e_1 \ran} = \frac{c_{12}}{c_{11}}+ O\left( \lambda_2^{2k}/\lambda_1^{2k}\right)
=\frac{c_{12}}{c_{11}}+ o \left(\left(\lambda_2/\lambda_1 \right)^k\right)  $$
Вычитая получаем
$$ A^k e_2 - \left(\frac{c_{12}}{c_{11}}+ o \left(\left(\lambda_2/\lambda_1\right)^k\right)\right) A^k e_1= o(\lambda_2^k)u_1+ c_{22}\lambda_2^ku_2 - \frac{c_{12}}{c_{11}}c_{21}\lambda_2^k u_2 +o(\lambda_2^k).$$
Коэффициент $c_{22}-\frac{c_{12}}{c_{11}}c_{21}$ по условию не равен $0$. После нормировки этот вектор стремится к $\pm u_2$. Продолжая так далее, получим, что столбцы $Q^{(k)}$ сходятся к собственным векторам $A$ в порядке убывания собственных чисел. Тогда в $i,j$ элементе матрицы $A_k={Q^{(k)}}^\top A Q^{(k)}$  стоит
$$\lan q^{(k)}_i, Aq^{(k)}_j\ran \to \lan u_i, \lambda u_j \ran = \lambda_i \delta_{ij}.$$
Значит, матрица $A_k$ действительно сходится к нужной диагональной матрице.
\endproof



\section{Спектры графов}
С графом $G$ связаны несколько разных матриц. Как свойства этих матриц, их собственные числа отражаются в комбинаторных свойствах графа $G$ и может ли это помочь?
Приведём пример, как знание спектра графа даёт некоторые оценки на сложно вычисляемые величины.

\rm Граф $G$ является $k$-регулярным тогда и только тогда, когда вектор $(1,\dots,1)^\top$ является собственным вектором с собственным числом $k$ для $A(G)$.
\erm

\thrm Пусть $G$ -- $k$-регулярный граф. Тогда размер максимального независимого множества в графе оценивается как
$$\alpha(G)\leq n\frac{-\lambda_n}{k-\lambda_n}.$$
\ethrm
\proof Рассмотрим характеристический вектор $v$ для независимого множества размера $\alpha$. Имеем $v^{\top}Av=0$ и при этом $v^{\top}v=\alpha$. Так как граф $k$-регулярный, то у него есть нормированный собственный вектор $u_1=\frac{1}{\sqrt{n}}(1,\dots,1)$. Тогда $\lan v,u_1\ran = \frac{\alpha}{\sqrt{n}}$. Разложим вектор $v=c_1u_1 + \dots + c_n u_n$ по ортонормированной системе собственных векторов. Тогда из условия независимости получаем
$$0=v^{\top}Av=\sum c_i^2 \lambda_i= \lambda_1\frac{\alpha^2}{n}+ \sum_{i\geq 2} \lambda_i c_i^2\geq \lambda_1\frac{\alpha^2}{n}+ \lambda_n \sum_{i\geq 2} c_i^2.$$
Мы знаем, что $\sum c_i^2=\alpha$, откуда $\sum_{i\geq 2} c_i^2=\alpha - \frac{\alpha^2}{n}$. Итого 
$$0\geq \lambda_1\frac{\alpha^2}{n}+\lambda_n(\alpha- \frac{\alpha^2}{n})$$
Сокращая на $\alpha$ получаем 
$$(\lambda_1-\lambda_n)\frac{\alpha}{n}\leq -\lambda_n,$$
что эквивалентно нужному неравенству.
\endproof




\exm\\
1) Полный граф $K_n$ имеет спектр $n-1,-1,\dots,-1$. В полном соответствии с тем, что размер максимального независимого множества равен $1$.\\
2) Граф Петерсена имеет спектр 3, 1, 1, 1, 1, 1, -2, -2, -2, -2. Размер независимого множества в точности равен $4$.\\
3) Оценка точна в общем виде для графов Кнезера $K(n,r)$.


Из оценки на размер независимого множества можно вывести оценку на хроматическое число.

\zd Покажите, что $\alpha(G) \chi(G) \geq n$. Выведите отсюда, что для регулярного графа $\chi(G)\geq 1+ \frac{\lambda_1}{-\lambda_n}$. Покажите, что для графа Петерсена оценка точная.
\ezd

\rm Удивительно, но ровно та же оценка верна и для нерегулярных графов тоже.
\erm

\zd Покажите, что $\chi(G) \leq [\lambda_1]+1$ для любого графа $G$. (Адаптируйте классическое доказательство с максимальной степенью).
\ezd


\subsection{Дополнительно: ещё трюки и применения}

\thrm Рассмотрим граф $K_{10}$. Тогда его невозможно покрыть тремя копиями графа Петерсена.
\ethrm
\proof Пусть граф $K_{10}$ покрыт тремя графами Петерсена. Тогда его матрица смежности удовлетворяет соотношению 
$$A(K_{10})=P_1+P_2+P_3.$$
Для нашего удобства перепишем это в виде 
$$-A(K_{10})=-P_1-P_2-P_3.$$
Заметим, что вектор $(1,\dots,1)^\top$ -- собственный для всех трёх матриц. Значит мы может ограничить указанное равенство на его ортогональное дополнение $U$.

Мы находимся в 9-тимерном пространстве. Воспользуемся неравенствами на собственные числа суммы (которые мы не доказывали). Тогда $i$-ое собственное число  суммы трёх матриц $A,B,C$ может быть оценено как $\lambda_t +\mu_s+ \nu_r$, где $r+s+t=i+2$. У операторов $-P_i|_U$ собственные числа равны $$2,2,2,2,-1,-1,-1,-1,-1.$$
Возьмём  $r=s=5$ и $t=1$. Получим оценку на $9$-ое собственное число $-A(K_{10})|_U$. Но мы его знаем: это $1$. С другой стороны получается, что оно должно быть меньше $0$. Противоречие! 
\endproof

Кроме оценок на хроматическое число и покрываемость при помощи спектров можно доказать негамильтоновость графа, оценивать его рёберное хроматическое число переходя к рёберному графу и т.д.

\zd Пусть $B$ -- матрица инцидентности графа $G$. Покажите, что $B^\top B - 2 E$ есть матрица смежности для рёберного графа $G$, а $B B^\top$ есть $A(G)+D$, где $D$ -- это матрица со степенями вершин графа $G$ на диагонали. 
\ezd

\zd Покажите, что в рёберном графе графа Петерсена нет индуцированного цикла длины 10 и выведите отсюда, что граф Петерсена негамильтонов.
\ezd

Есть, однако, две темы, связанные с собственными числами графов, которые приводят к непосредственным применениям. К сожалению, как часто бывает, у нас нет шансов внимательно посмотреть результаты этих разделов, поэтому мы ограничимся лишь очерчиванием их границ.




\dfn Для $A$ -- подмножества $V(G)$ определим $\partial A$ как множество всех рёбер ведущих из $A$ в вершины не из $A$. Константа Чигера или константа расширения графа  $G$ это 
$$h(G) := \min \left\{ \left. \frac{| \partial A |}{| A |} \right|   A \subseteq V(G), 0 < | A | \leq \tfrac{1}{2} | V(G)| \right\} .$$
\edfn

Вычислить константу Чигера очень сложно. Однако её можно оценить.

\begin{thmm}[Неравенство Чигера] Пусть $G$ -- $d$-регулярный граф и $\lambda_2$ -- его второе по максимальности собственное число. Тогда
$$\tfrac{1}{2}(d - \lambda_2) \le h(G) \le \sqrt{2d(d - \lambda_2)}.$$
\end{thmm}



\dfn Семейство графов $G_n$ называется $(n,d,\alpha)$ (алгебраическим) экспандером, если $G_n$ -- $d$-регулярный граф на $n$ вершинах с $\lambda=\max(|\lambda_2|,|\lambda_n|)\leq \alpha d$.
\edfn

Где могут применяться такие графы:\\
1) Коды, исправляющие ошибки\\
2) Псевдослучайные генераторы\\
3) Дерандомизация

Несмотря на то, что большая часть регулярных графов степени $d$ обладает указанными свойствами  до некоторого времени не было ни одного явного примера семейства экспандеров. Первый пример семейства экспандеров дал Григорий Маргулис. \\


\exm\\
1) Рассмотрим $\Z/n \times \Z/n$ и проведём для пары $(x,y)$ ребро в $(x \pm 2y,y), (x \pm (2y+1),y), (x,y \pm 2x), (x,y \pm (2x+1))$. Получается граф-экспандер с $\lambda \leq 5\sqrt{2}$\\
2) Можно подметить, что рёбра экспандеров из предыдущего примера связаны с некоторыми обратимыми преобразованиями, то есть с какой-то группой. Это общее место для конструкции большинства экспандеров.



\dfn Определим Лапласиан графа как $L(G)$ как $D-A(G)$.
\edfn

\zd $L(G)=B B^\top$, где $B$ -- матрица инцидентности произвольным образом ориентированного графа $G$.
\ezd

\zd $L$ -- неотрицательно определена и  $\rk L= \rk B=n-c$, где $c$ -- это количество компонент связности графа $G$.
\ezd



Кроме собственно графов можно рассматривать взвешенные графы. Понятно, как для них определить матрицы $D$, $A(G)=W(G)$ -- матрица весов, $L(G)$. Кроме этих матриц нам понадобится ещё и нормализованная матрица Лапласа.


\dfn Рассмотрим матрицу $D^{-1/2} L(G) D^{-1/2}$. Это нормализованный Лапласиан.
\edfn

На основе нормализованного Лапласиана можно построить алгоритм фрагментации изображений. Как и любая естественная задача она не имеет чёткой формулировки. В 2000 году Jianbo Shi и  Jitendra Malik \href{https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf}{предложили} следующую версию формулировки задачи: по картинке строится взвешенный граф $G$  -- его множество вершин $V$ -- это пиксели, рёбра проводятся, если вершины близки друг к другу и вес зависит от того, насколько пиксели похожи друг на друга.

Теперь рассматриваются все подмножества пикселей $A\subseteq V$ и минимизируется величина разреза, который отрезает $A$ от остального графа.
$$\min_{A} \frac{assoc(A,V\setminus A)}{assoc(A,V)}  + \frac{assoc(A, V\setminus A)}{assoc(V\setminus A,V)}, $$
$$\text{ где } \,\, assoc(A,B)=\sum_{i\in A, j\in B} w_{ij}$$.


Вопрос стоит в выборе оптимального $A$. Сопоставим множеству $A$ вектор $y$, такой что $$y_i = \begin{cases}-b=-\frac{\sum_{v\in A} d_v}{\sum_{v \not\in A} d_v}, \text{ если } i\not\in  A \\
1, i\in A 
\end{cases}$$

Оказывается, что в этом случае задача сводится к минимизации по всем таким $y$ выражения 
$$\frac {y^\top L(G) y} {y^\top D y } \text{ при этом выполнено } y D \bf 1 = 0.$$ 

Здесь $\bf 1$ -- это столбец из единиц. Решить эту дискретную задачу (допустимых $y$-ков конечное число) трудно, поэтому авторы переходят к непрерывной, отбрасывая ограничения на значения $y_i$. Минимум такого выражения достигается на $D^{-1/2}v$, где $v$ -- собственный вектор нормализованного Лапласиана для минимального положительного собственного числа ($D^{-1/2}\bf 1$ -- это собственный вектор для собственного числа $0$).

Теперь, найдя $v$ можно разделить точки по признаку положительности и отрицательности компонент $v$ (например).

\rm Похожий метод для матрицы Лапласа приводит к оценке связности графа и позволяет приближённо найти минимальный вершинный разрез графа дающий несколько компонент связности. (см. вектор Фидлера).
\erm



\section{Кватернионы}



Наша цель сейчас рассказать про геометрию трехмерного пространства используя при этом определённые алгебраические конструкции. А именно, ещё в XIX веке Уильям Роуэн Гамильтон стал искать аналогичную комплексным числам алгебраическую систему на трёхмерном пространстве.  
Однако, подходящий аналог удалось найти только в четырёхмерной ситуации.


Рассмотрим вещественное подпространство в алгебре матриц $M_2(\C)$ вида
$$\H = \left\{\pmat \alpha & \beta \\ -\ovl{\beta} & \ovl{\alpha} \epmat \right\}.$$
Базис этого пространства, как вещественного векторного пространства, состоит из матриц 
$$ 1=\pmat 1 & 0 \\ 0& 1 \epmat, i= \pmat i & 0 \\ 0& -i \epmat, j=\pmat 0& 1 \\ -1 & 0 \epmat, k=\pmat 0 & i \\ i & 0\epmat. $$ 
Покажем, что это вещественная подалгебра в $M_2(\C)$ и следовательно ассоциативное кольцо. 
Для этого достаточно показать, что произведение базисных снова лежит в $\H$. Имеем $$i^2=j^2=k^2=-1 \text{ и } ij=k=-ji,$$ откуда $$ik= iij=-j=jii=-ki \text{ и } jk=-jji=i=-kj.$$ Таким образом $\H$ образует ассоциативную алгебру размерности 4 над $\R$.
 
\dfn[Алгебра кватернионов] $\H$ называется алгеброй кватернионов. 
\edfn
Мы больше не будем думать (кроме одного нюанса) про кватернионы как про матрицы, а будем записывать их через $i,j,k$. Именно так обычно кватернионы и вводят -- как формальный суммы $a+bi+cj+dk$, для произведения которых выполнены тождества $i^2=j^2=k^2=-1$ и $ij=-ji=k$. Посмотрев на кватернионы как на матрицы мы сэкономили на доказательстве ассоциативности умножения.

\zd Алгебра кватернионов не снабжается структурой $\C$-алгебры.
\ezd






\dfn[Векторная и скалярная часть, сопряжённый кватернион] Пусть $x= a+bi+cj+dk$ кватернион. Определим вещественную или скалярную часть $\Re x=a$ и векторную часть $v= bi+cj+dk$ кватерниона. Сопряжённым кватернионом называется $\ovl{x}= a-bi-cj-dk= \Re x - \Im x =a-v$. 
\edfn



Посмотрим, как перемножаются кватернионы. Если оба кватерниона  $x$, $y$ разделить на скалярную и векторную части $x=a+v$, $y=b+u$ то $xy=ab+au+bv+ vu$. Нам осталось разобраться с умножением векторных частей. 

Рассмотрим произведение двух чисто мнимых кватернионов $uv=-\lan u,v\ran+[u,v]$. Его вещественная часть совпадает с минус скалярным произведением векторов. Про мнимую часть мы поговорим отдельно.

\dfn[Векторное произведение] Пусть $u,v \in \R^3$ два вектора. Тогда их векторным произведением называется вектор $[u,v]$.
\edfn

Если расписать в координатах $u=x_1i+x_2j+x_3k$ и  $v=y_1i+y_2j+y_3k$, то векторное произведение задаётся формулой

$$[u,v]= (x_2y_3-x_3y_2)i + (x_3y_1-x_1y_3)j + (x_1y_2- x_2y_1)k= \begin{vmatrix} i& j&k \\ x_1 & x_2 & x_3 \\ y_1 & y_2 & y_3 \end{vmatrix} $$

\rm Операция $(u,v) \to [u,v]$ является билинейной и антисимметричной, то есть $[u,u]=0$ и, следовательно, $[u,v]=-[v,u]$.
\erm

Последнее замечание позволяет нам легко вычислить $x \ovl{x}= a^2+ \lan v,v \ran$. Это приводит нас к определению:

\dfn[Норма кватерниона] Определим норму кватерниона как $$\|x\|=\sqrt{x\ovl{x}}=\sqrt{ a^2+b^2+c^2+d^2}=\sqrt{\ovl{x}x}.$$
\edfn 


Норма кватерниона, как и модуль комплексного числа всегда положительны для ненулевых элементов. Это позволяет заметить, что

\dfn[Обратный кватернион] Если $0\neq x \in \H$, то $x^{-1}=\frac{\ovl{x}}{\|x\|^2}$. 
\edfn

Таким образом мы получили первый (и для нас единственный) пример некоммутативного кольца с делением. Такие кольца называются телами. Напоминаю, что алгебра для нас ассоциативна и с единицей. Неассоциативные алгебры представляют интерес. Например, можно взять $\R^3$, где в качестве умножения взято векторное произведение. Это пример неассоциативной алгебры или, точнее, алгебры Ли. В этом курсе мы не  обсуждаем неассоциативные алгебры в связи с тем, что им обычно находится применение либо внутри физических дисциплин, либо внутри самой математики и редко где ещё. 

Какие ещё свойства есть у отображения нормы? Если следовать параллели с комплексными числами, то стоит посмотреть, что происходит с нормой произведения. Для того, чтобы не обременяться вычислениями сделаем небольшой трюк и на секунду вспомним матричное представление кватернионов. Заметим, что на матричном языке, норма -- это $\|x\|=\sqrt{\det x}$, откуда получаем

\lm[Норма мультипликативна] $\|xy\|=\|x\|\|y\|$. В частности, $\|x^{-1}\|=\|x\|^{-1}$.
\proof Вспомним в последний раз, что кватернионы задаются матрицами из $M_2(\C)$. Пусть 
$$x=\pmat \alpha & \beta \\ -\ovl{\beta} & \ovl{\alpha}\epmat.$$
Тогда $$\|x\|^2=|\alpha|^2+|\beta|^2=\det \pmat \alpha & \beta \\ -\ovl{\beta} & \ovl{\alpha}\epmat,$$
а определитель мультипликативен.
\endproof
\elm



Продолжим. Используя мультипликативность нормы легко доказать 
\lm Отображение $x \to \ovl{x}$ является антиизоморфизмом алгебр, то есть $\ovl{ab}=\ovl{b}\ovl{a}$.
\proof Линейной ясна. Пусть $x,y \neq 0$. Тогда $$\frac{\ovl{y}\,\ovl{x}}{\|y\|^2\|x\|^2}=y^{-1}x^{-1}=(xy)^{-1}=\frac{\ovl{xy}}{\|xy\|^2}.$$
\elm

На самом деле и здесь можно было воспользоваться матричным представлением. А именно, можно заметить, что операция сопряжения кватернионов совпадает на этом языке с транспонированием и сопряжением соответствующей комплексной матрицы. Вернёмся теперь к векторному произведению.





\lm[Свойства векторного произведения] Верны следующие свойства\\
1) Для любых $u,v \in \R^3$ верно $u\bot [u,v]$. Точнее $$u[u,v]= -\|u\|^2v+ \lan u,v\ran u$$
2) $\| [u,v]\|= \|u\|\|v\| \cdot |\sin \ffi |$, где $\ffi$ --  это угол между $u$ и $v$.
\elm
\proof Для того, чтобы посчитать скалярное произведение $\lan u, [u,v]\ran$ необходимо посчитать скалярную часть $u[u,v]$. 
$$u[u,v]= u (uv+ \lan u,v \ran)= u^2 v+ \lan u,v \ran u= -\|u\|^2 v+ \lan u,v \ran u$$
Последнее выражение, очевидно, чисто векторное.
Теперь
\begin{align*}
&\|[u,v]\|^2= -[u,v][u,v]= (uv + \lan u,v\ran)(vu + \lan u,v\ran)=\\
&=\|u\|^2\|v\|^2+ \lan u,v\ran^2 + \lan u,v\ran (uv+vu)=\|u\|^2\|v\|^2 - \lan u,v\ran^2= \|u\|^2\|v\|^2(1-\cos^2 \ffi)
\end{align*}
\endproof

\dfn Обозначим за $\H_{1}$ группу кватернионов, по норме равных единице.
\edfn

\thrm Отображение $\H_{1}\to \GL_3(\R)$ заданное по правилу $x\to (y \to xyx^{-1})$ корректно определено и даёт сюръективный  гомоморфизм из группы кватернионов единичной нормы в $\SO_3(\R)$. Ядро этого гомоморфизма состоит из $\{\pm 1\}$. Точнее, если единичный кватернион $x$  представим в виде $x=a+bv$, то соответствующее ему вращение есть вращение относительно  оси $\lan v \ran$ на угол $2\ffi$, где $\cos \ffi= a$, $\sin \ffi= b$ или тождественное преобразование в случае $v=\pm 1$.
\ethrm
\proof Рассмотрим преобразование $L_x \colon \H \to \H$ вида $y \to xyx^{-1}$. Прежде всего покажем, что мы получили ортогональное преобразование $\R^4$. Имеем
 $$\|xvx^{-1}\|=\|x\| \|v\| \|x^{-1}\| = \|v\|.$$
Теперь заметим, что преобразование $L_x$ сохраняет на месте вектор 1 и, следовательно, его ортогональное дополнение, то есть $\R^3$. Таким образом $L_x$ ограничивается на $\R^3$. Далее, очевидно, $L_xL_y= L_{xy}$. Осталось посчитать ядро гомоморфизма и явный вид отображения $L_x$. Заметим, что если $x=a+bv$, то $L_x$ оставляет $v$ на месте. Действительно, при $b\neq 0$ 
$$xbvx^{-1}=x(x-a)x^{-1}= x-a=bv.$$
Вычислим угол поворота. Для этого рассмотрим нормированный вектор  $u\bot v$ и $[u,v]$, которые образуют ортонормированный базис дополнения и посчитаем $xux^{-1}$ и $x[u,v]x^{-1}$. Из условия ортогональности следует, что $uv=[u,v]=-[v,u]=-vu$ и значит $v[u,v]=-[u,v]v=-uv^2=u$. Теперь
\begin{align*}
xux^{-1}&=(a+bv)u(a-bv)= (a+bv)(au-buv)=\\
&=a^2u -ab[u,v]+ab[v,u]- b^2vuv=a^2u-2ab[u,v]-b^2\|v\|^2u=\\ &=(a^2-b^2)u-2ab[u,v]=\cos2\ffi u+ \sin 2\ffi [u,v]
\\
\\
x[u,v]x^{-1}&=(a+bv)[u,v](a-bv)= (a[u,v]+bu)(a-bv)=\\
&=a^2[u,v]+abu-ab[u,v]v-b^2uv=\\
&=(a^2-b^2)[u,v]+2abu=\cos 2\ffi[u,v]+\sin 2\ffi u
\end{align*}

Осталось показать, что только $x=\pm 1$ лежит в ядре этого отображения. Это возможно только тогда, когда $2\ffi \equiv 0 \mod 2\pi$. Значит $\ffi=2 \pi$ или $\ffi= \pi$. Первое соответствует $x=1$. Втораое --  $a=\cos \ffi = -1$, а $b=\sin \ffi = 0$, то есть $x=-1$. 
\endproof


\zd
Покажите, что отображение $(x,y) \to (z \to xzy^{-1})$ задаёт сюръективный гомоморфизм из декартового квадрата группы единичных кватернионов в группу $\SO_4(\R)$ с ядром $\{(1,1),(-1,-1)\}$.
\ezd

Обсудим теперь для чего могут понадобится кватернионы. Каждый кватернион, как мы установили кодирует вращение трёхмерного пространства или, что тоже самое -- новую декартову систему координат в $\R^3$ с центром в нуле. Такой тип данных встречается в компьютерной графике, если вы хотите зафиксировать ракурс, в котором вы смотрите на 3d-сцену или положение какого-то конкретного объекта в такой сцене.

В такой задаче кватернионы сложно превзойти. Действительно, задание сцены при помощи кватернионов очень экономно -- 4 коэффициента с одним соотношением на них (3 коэффициента, если очень нужно) против 9 коэффициентов у ортогональной матрицы.

А что с эффективностью операций? Тут вопрос состоит в том, про какие операции идёт речь. Если вы хотите сказать, что тот или иной вектор, который был повёрнут на кватернион $q=a+v$ надо повернуть ещё на кватернион $p=b+u$, то вам надо всего лишь вычислить $pq=ab+au+bv+uv$. Такое произведение считается за 16 умножений и 12 сложений. Если брать произведение матриц $3\times 3$, то там получается 27 умножений и 18 сложений (можно обойтись и 23 умножениями, но сильно увеличив число сложений).

Можно конечно использовать углы Эйлера, но тогда придётся использовать для вычислений косинусы и синусы этих углов, которые сами даются не бесплатно.

Если вам даны два ракурса в виде кватернионов, то легко понять, на какой кватернион надо домножить, чтобы из первого получить второй.

Но что если вы хотите просто повернуть при помощи кватерниона какой-то вектор из $\R^3$. Тут ситуация несколько хуже. Для этого вам необходимо посчитать $qxq^{-1}$. Предположив, что $q$ нормирован можно заменить обращение на сопряжение. Тем не менее такой подход довольно дорог -- 32 умножения и 24 сложения. Конечно, такой способ не оптимален. Например, не обязательно считать скалярную часть -- она должна стать нулевой. На самом деле, если $q=a+v$, то 
$$qxq^{-1}=x+ 2[v, [v,x]+ ax]$$
что даёт 15 умножений и 15 сложений (если считать умножение на 2 сложением). Это конечно отличается от матриц, где необходимо 9 умножений и 6 сложений.

Впрочем, отличается не сильно. Кроме того у кватернионного представления есть плюс произведение нескольких кватернионов -- это кватернион несмотря на ошибки округления. Что не так для ортогональных матриц.

Наконец, представим себе задачу, что нам нужно плавно перейти от ракурса $q$ к ракурсу $p$. Желательно с "равномерной" скоростью. На языке кватернионов это становится понятно. Для этого заметим, что единичные кватернионы -- это всего лишь точки на трёхмерной сфере в четырёхмерном пространстве. Несложно понять, что их соединяет часть дуги сферы, равномерное движение по которой и приводит к желаемой смене ракурса. Теперь несложно параметризовать равномерное движение по этой дуге. 
$$ \frac{\sin(t\theta)p + \sin ((1-t)\theta) q}{\sin \theta},$$
где $\theta$ -- угол между $p$ и $q$ (острый, не забываем, что представление кватернионами вращений немного не однозначно).





\section{Двойственное пространство}

Начнём наше триумфальное возвращение из мира вещественных чисел в мир произвольных полей. В дальнейшем $K$ обозначает произвольное поле. 

\dfn Определим двойственное пространство к пространству $V$ как $V^*=\Hom (V, K)$.
\edfn

\dfn Пусть $e_1,\dots,e_n$ -- базис $V$. Определим $e^1,\dots,e^n$ как базис пространства $V^*$, заданный следующим соотношением: 
$$e^j(e_i)=\delta_{ij}.$$
Иными словами, $e^i$ -- это $i$-ая координатная функция в базисе $e$.
\edfn

\utv Пусть $V$ -- векторное пространство над полем $K$. Тогда имеет место естественный изоморфизм пространств $V\to {V^*}^*$.
\eutv
\proof Построим отображение по правилу $v \to (f \to f(v))$. Это отображение инъективно. Из равенства размерностей следует, что этого и достаточно.
\endproof

Если мы перешли от одного базиса к другому в пространстве $V$, то что произошло в $V^*$? 

\thrm Пусть $e_1,\dots,e_n$ старый базис $V$, а $\hat{e}_1,\dots,\hat{e}_n$ -- новый. Пусть $C$ -- матрица перехода из старого базиса в новый. Тогда матрица перехода из базиса $e^1,\dots,e^n$ в базис $\hat{e}^1,\dots,\hat{e}^n$ есть ${C^{\top}}^{-1}$.
\proof Вспомним нашу конвенцию: матрица перехода $C$ это, столбцы которой составлены из координат нового базиса в старом базисе. Если у нас есть вектор $v$, то это условие означает, что его новый столбец координат $y$ и старый столбец координат связаны уравнением $x=Cy$. Переписывая получаем соотношение 
$$C^{-1}x=y.$$
Но это есть соотношение между координатными функциями в новом базисе и в старом базисе, то есть, между векторами двойственных базисов. Почти то, что нужно. Осталось из этого соотношения найти матрицу перехода. Для этого запишем $$y_i=\sum_j (C^{-1})_{ij}x_j$$
Значит, элементы $(C^{-1})_{ij}$ должны стоять в $i$-ом столбце матрицы перехода из $\hat{e}$ в $\hat{e}^*$. Это значит, что эта матрица равна ${C^{-1}}^{\top}$.
\endproof
\ethrm

В особые отношения со своим двойственным пространством вступают евклидовы и унитарные пространства. 

\utv Пусть $V$ -- евклидово или унитарное пространство. Тогда любой линейный функционал имеет вид $\lan v, \_ \ran$ для единственного вектора $v$. 
\eutv
\proof Пусть $f$ -- линейный функционал. Рассмотрим $U=\Ker f^\bot$. Это одномерное пространство. Рассмотрим $v\in U$ отличный от нуля и подгоним при помощи константы так, чтобы  $f(v)=\lan v,v \ran$. Этого достаточно благодаря линейности каждого выражения.  
\endproof

\section{Немного про квантовую механику}

Вкратце напомню парадигму квантовой механики. Физическая система в квантовой механике задаётся при помощи некоторого унитарного пространства $V$. Каждая прямая этого унитарного пространства задаёт некоторое возможное физическое состояние системы. На каждой прямой есть вектор, по норме равный $1$. Итого, каждый вектор, равный по норме $1$ описывает некоторое физическое состояние. 

Квадрат модуля скалярного произведения двух таких нормированных векторов  $|\lan u,v \ran|^2$ интерпретируется как вероятность того, что  $v$ при измерении будет находится в состоянии $u$ (если вы можете померить, что что-то находится именно в состоянии $u$). Таким образом, каждый вектор $u\in V$ задаёт линейный функционал $\lan u, \_ \ran $ на $V$, который по состоянию $v$ выдаёт информацию о вероятности найти $v$ в состоянии $u$.

Каждой измеримой физической величине соответствует самосопряжённый оператор $T$ на $V$. Пусть $\psi_i$ -- это ортонормированный базис из собственных векторов $T$ с собственными числами $\lambda_i$. Тогда вероятность, что находясь в состоянии $v$ при измерении величины $T$ мы получим $\lambda_i$ есть вероятность найти $v$ в состоянии $\psi$, то есть $|\lan \psi_i, v \ran|^2$. Несложно заметить, что, как и должно быть, сумма по всем возможным $i$ этих вероятностей даёт $1$. Такая конструкция объясняет квантованность (дискретность) значений разных физических величин, которая возникает при измерении.

Типичным примером квантовой системы является нерелятивистская (без учёта теории относительности) модель одной частицы в $\R^3$. В этом случае роль унитарного пространства играет 
$$V= L_2(\R^3)=\left\{  f\colon \R^3 \to \C\, \Big| \, f - \text{ измерима и } \int_{\R^3} |f|^2<\infty \right\}.$$
Технически тут ещё понадобится отождествить функции отличающиеся друг от друга на множестве меры ноль, но я это опускаю. Скалярное произведение таких функций задаётся привычной формулой 
$$\lan f,g \ran = \int_{\R^3} \ovl{f}g\, dx.$$
Состояние квантовой системы, соответствующее нормированной функции $f\in V$ (т.е. $\|f\|^2=1$)  имеет следующую интерпретацию: $\int_{B} |f|^2$ есть вероятность для частицы оказаться в области $B$. 

 

В квантовых вычислениях основным объектом является квантовый бит или кубит (qubit) -- это двумерная квантовая система. Первый её базисный вектор обозначается $\left|0\ran$, а второй $ \left|1\ran$. Предполагается, что это собственные вектора оператора энергии т.е. два эти состояния различаются при измерении энергии. Произвольное состояние (нормированное) есть смесь 
$$c_1 \left|0\ran+ c_2 \left| 1\ran, \text{ где } |c_1|^2+|c_2|^2=1.$$
 

Допустим, у нас есть две независимые системы с пространствами $V_1$ и $V_2$. Какое пространство соответствует  объединению двух таких систем? Обозначим это пространство за $V_1\otimes V_2$. Посмотрим как должны выглядеть состояния этого пространства. Понятно что для каждой пары состояний $u_1\in V_1$ и $u_2\in V_2$ должно существовать своё состояние объединённой системы. Обозначим его за $u_1 \otimes u_2$. Другими словами должно быть задано отображение $i\colon V_1\times V_2\to V_1 \otimes V_2$.

Пусть есть одна пара $(u_1,u_2)$ и вторая пара $(v_1,v_2)$. Какова вероятность того, что при измерении система в состоянии $u_1\otimes u_2$ перейдёт  в состояние $v_1 \otimes v_2$? Логично, чтобы это было $|\lan u_1,v_1 \ran \lan u_2, v_2\ran|^2$.

Какому скалярному произведению на таких парах это соответствует? Должно быть 
$$\lan u_1\otimes u_2, v_1\otimes v_2\ran = \lan u_1,v_1 \ran \lan u_2, v_2\ran.$$
То есть, скалярное произведение с состоянием $u_1\otimes u_2$ есть не линейная функция от пары $(v_1, v_2)$, а билинейная!



Линейные отображения можно складывать между собой. Сумме линейных отображений, заданных при помощи $u_1\otimes u_2$ на $V_1\otimes V_2$ должна соответствовать сумма билинейных форм на $V_1\times V_2$. Заметим, что любую билинейную форму можно представить в виде суммы форм заданных при помощи пар $(u_1,u_2)$. Таким образом, логично считать, что все линейные функционалы на $V_1\otimes V_2$ должны соответствовать билинейным формам на $V_1\times V_2$.





\section{Тензорное произведение}

Сейчас мы немного поговорили про пространство линейных функционалов, прошлом семестре мы подробно остановились на билинейных операциях. А квантовая механика дала нам необходимость в следующем определении:

\dfn Пусть дана пара пространств $U,V$ над полем $K$. Тогда их тензорным произведением называется пространство 
$U\otimes V$ вместе с билинейным отображением отображением
$$i \colon U \times  V \to U \otimes V,$$
удовлетворяющее условию что для любого билинейного отображения из $h\colon U \times V \to W$ существует единственное линейное отображение 
$$\hat{h}\colon U\otimes V \to W,$$
что 
$$\hat{h}\circ i=h.$$
Иными словами, отображение $i$ -- это <<универсальное>> билинейное отображение.
\edfn 


\lm Пусть $U\leq V$, а $\pi \colon V \to V/U$ -- это каноническая проекция на $V/U$. Пусть $L\colon V \to W$ -- линейное отображение. Тогда, для того, чтобы существовало $\hat{L}\colon V/U \to W$, что $L=\hat{L}\circ \pi$ необходимо и достаточно, чтобы $L(U)=\{0\}$.
\elm





\thrm Пусть $U,V$ -- пара векторных пространств. Тогда имеет место следующая конструкция тензорного произведения:
$$U \otimes V \cong K\lan U \times V \ran / Rel,$$
где $Rel$ -- это подпространство порождённое формальными суммами
$$(\lambda u_1+u_2, v) - \lambda (u_1, v) - ( u_2,v) \text{ и } (u,\lambda v_1+v_2) - \lambda (u,v_1) - (u,v_2).$$ 
\proof Для удобства обозначим пространство
$$T=K\lan U \times V \ran / Rel.$$ Будем обозначать образы элементов $(u,v)$ в $T$ как  $u\otimes v$. Отображение $$i \colon U\times V \to T$$
заданное правилом  $(u,v) \to u \otimes v$
билинейно по самому определению соотношений из $Rel$. Пусть теперь дано пространство $W$ и билинейное отображение $$h \colon U \times V \to W.$$
Построим отображение $\hat{h}$ следующим образом: сначала определим $\hat{\hat{h}}\colon K\lan U \times V \ran \to W$, а затем покажем, что оно пропускается через $T$. По самому своему определению $K\lan U \times V\ran$ имеет базисом элементы $(u,v)$. Отображение $\hat{\hat{h}}$ достаточно задать на них. Положим $$\hat{\hat{h}}((u,v))=h(u,v).$$
Покажем, что оно однозначно пропускается через $T$. Как всегда единственность очевидна. Для того чтобы показать, что $\hat{\hat{h}}$ пропускается через $T$ необходимо показать, что все соотношения лежат в ядре $\hat{\hat{h}}$. Но это так, потому что $h$ билинейно! 
\endproof
\ethrm




Мы показали, что тензорное произведение существует. Неплохо бы понять, что тензорное произведение однозначно определено с точностью до изоморфизма. 


\utv Если тензорное произведение существует, то оно единственно с точностью до изоморфизма. Конкретно, пусть $U,V$ -- векторные пространства и есть два пространства $T_1,T_2$ и $i_1 \colon U\times V \to T_1$ и $i_2\colon U\times V \to T_2$, играющие роль тензорного произведения, то существует единственный изоморфизм $h\colon T_1\to T_2$, что $h\circ i_1=i_2$.
\proof Это типичное <<категорное>> доказательство. Пусть есть два пространства $T_1$ и $T_2$ вместе с отображениями $i_1 \colon U\times V \to T_1$ и $i_2 \colon  U\times V \to T_2$, для которых выполнены аксиомы тензорного произведения. Заметим, что так как $i_2$ полилинейное, то есть такое $\hat{i_2} \colon T_1 \to T_2$, что $$\hat{i_2}\circ i_1= i_2.$$ 
Аналогично существует $\hat{i_1} \colon T_2 \to T_1$, что
$$\hat{i_1}\circ i_2= i_1.$$ 
Покажем, что $\hat{i_1} $ и $\hat{i_2}$ взаимно обратны. Действительно, имеем
$$\hat{i_1}\circ \hat{i_2}\circ i_1= \hat{i_1}\circ i_2= i_1,$$
что означает, что отображение $\hat{i_1}\circ \hat{i_2}$ есть то самое единственное отображение $T_1\to T_1$, которое гарантируется благодаря полилинейности $i_1$ и того, что само $T_1$ -- тензорное произведение. Но есть другой кандидат на эту роль -- это $\id_{T_1}$. По единственности 
$$\hat{i_1}\circ \hat{i_2}=\id_{T_1}.$$
Аналогично проверяется равенство для второй композиции.
\endproof
\eutv





Теперь необходимо посчитать что-то про тензорное произведение. Например, научиться считать размерность тензорного произведения и находить его базис.
\thrm Пусть $e_1,\dots,e_n$ базис $U$, а $f_1,\dots,f_m$ -- базис $V$. Тогда $e_i\otimes f_j$ базис $U \otimes V$. В частности, 
$$\dim U \otimes V= \dim U \cdot \dim V.$$ 
\proof Прежде всего заметим, что набор $e_i \otimes f_j$ является порождающей системой для тензорного произведения. Значит оно на самом деле конечномерно. Далее, по определению тензорного произведения,
$$\Hom(U;V, K) \simeq \Hom(U\otimes  V,K).$$
Размерность последнего пространства совпадает с размерностью $U\otimes V$. С другой стороны, полилинейное отображение $h \in \Hom(U,V, K)$ однозначно задаётся $\dim U \cdot \dim V$  параметрами $h(e_i,f_j)$. Комбинируя эти два факта получаем, что размерность $\dim U \otimes V$ есть $\dim U \cdot \dim V$. Отсюда, любая порождающая система такого размера есть базис. В частности, набор $e_i \otimes f_j$.
\endproof
\ethrm


\rm Пространство состояний квантовой системы из $n$ кубит имеет размерность $2^n$. Это ключевой факт, который проясняет потенциальную эффективность квантовых компьютеров: вместо $n$ бит мы имеем дело с $2^n$ чисел -- координатами вектора из $2^n$-мерного пространства.
\erm


Определим теперь тензорное произведение линейных отображений.

\dfn Пусть дана пара линейных отображений $L \colon U_1 \to V_1$ и $S\colon U_2 \to V_2 $. Определим отображение $$L\otimes S \colon U_1\otimes U_2 \to V_1\otimes V_2$$ по  правилу $$(L \otimes S) (u_1\otimes  u_2) = L(u_1)\otimes S(u_2).$$
Отображение с таким свойством единственно.
\edfn

\rm Указанное отображение существует и единственно. Действительно,  отображение $L\otimes S$ должно соответствовать билинейному отображению $L\times S$ заданному правилом $L\times S ((u_1,u_2)) = L(u_1)\otimes S(u_2)$. 
\erm

\rm Пусть заданы наборы линейных отображений $L_1, L_2$ и $S_1, S_2$, так что определены композиции $L_i\circ S_i$. Тогда
$$(L_1\otimes S_1) \circ (L_2 \otimes S_2)=(L_1\circ L_2)\otimes (S_1\circ S_2).$$
\erm 

А как устроена матрица тензорного произведения линейных отображений?


\lm Пусть $L_1 \colon V_1 \to U_1$, а $L_2 \colon V_2 \to U_2$. Пусть $e_1,\dots, e_{n_1}$ базис $V_1$,  $e_1',\dots, e_{n_2}'$ базис $V_2$,  и $f_1,\dots, f_{m_1}$ -- базис $U_1$, а $f_1',\dots, f_{m_2}'$ -- базис $U_2$. 
Упорядочим базисы тензорных произведений -- удобно это сделать, например, в лексикографическом порядке (номер первой координаты важнее).
Тогда матрица  $L_1\otimes L_2$  разобьётся на $n_1m_1$ блоков в каждом из которых будет стоять $ A_{ij} B$, где $i,j$ -- номер блока, а $A$ и $B$ матрицы $L_1$ и $L_2$ соответственно.
\proof Действительно, отображение $A\otimes B$ отправляет $e_i\otimes e'_j$ в 
$$Ae_i \otimes Be'_j = \sum_{k,l} A_{ki}B_{lj} \, f_k\otimes f'_l.$$
\endproof
\elm

\dfn Такая матрица называется кронекеровым или тензорным произведением матриц $A$ и $B$ и обозначается как $A\otimes B$.
\edfn

А что если стартовать с операторов, а не с линейных отображений?

\rm Если есть операторы $A\colon V \to V$ и $B \colon W \to W$. Тогда задан оператор $A\otimes B$ на $V\otimes W$.
\erm

\lm Собственные числа оператора $A\otimes B$  -- это попарные произведения собственных чисел для $A$ и $B$. 
\proof Перейдём к матрицам и, если нужно, будем считать, что мы находимся над алгебраически замкнутым полем. Рассмотрим жорданов базис для $A$ и $B$ -- $v_1,\dots,v_n$ и $u_1,\dots, u_m$. Тогда, рассмотрев базис $v_i\otimes u_j$, заметим, что под диагональю будут стоять нули, а на диагонали -- попарные произведения собственных чисел $A$ и $B$.
\endproof
\elm


 

\dfn[Произведение графов] Пусть $G$ и $H$ -- два графа (возможно ориентированных). Тогда их категорным произведением называется граф чьи вершины есть пары вершин $G$ и $H$ и ребро между парами $(u_1,v_1)$ и $(u_2,v_2)$ проводится только если есть рёбра $u_1 \to u_2$ и $v_1 \to v_2$.

Декартовым произведением графов $G$ и $H$ называется граф на тех же вершинах с ребром между парами если $u_1=u_2$ и есть ребро $v_1\to v_2$ или, симметрично, $v_1=v_2$ и есть ребро $u_1 \to u_2$. Разумеется для неориентированных графов эта конструкция снова выдаёт неориентированный граф.
\edfn


\crl Спектр категорного произведения графов состоит из всех возможных попарных произведений собственных чисел графов.
\proof Заметим, что матрица смежности категорного произведения графов -- это тензорное произведение матриц смежности исходных графов.
\endproof
\ecrl



\zd Чему равен спектр декартового произведения графов?
\ezd

\zd Чему равен спектр графа-решётки
\ezd

А можно ли как-то охарактеризовать тензорное произведение нескольких пространств?

\rm Если рассмотреть тензорное произведение $V_1\otimes \dots \otimes V_n$, то линейные отображения из него в $W$ будут соответствовать полилинейным отображениям $\Hom(V_1;\dots ;V_k, W)$.
\erm

С понятием тензорного произведения связан ряд канонических отождествлений между разными на первый взгляд пространствами в духе изоморфизма $V \simeq V^{**}$.

\thrm Имеют место следующие естественные изоморфизмы: 
$$(U \otimes V) \otimes W \simeq U \otimes V \otimes W \simeq U \otimes (V \otimes W)$$
$$ U \otimes V \simeq V \otimes U $$
$$ \Hom (U,V) \simeq  V \otimes U^*$$
$$(U \otimes V)^{*} \simeq U^{*}\otimes V^{*} $$
$$\Hom (U\otimes V,  W) \simeq \Hom (U, \Hom (V,W))$$
\proof Наиболее интересная часть этой теоремы состоит в бескоординатном построении этих отображений.

Построим отображение $V\otimes U^{*} \to \Hom (U,V)$ по правилу $$v\otimes f \to (u \to f(u)v).$$
Это соответствие полилинейно по $v,f$ поэтому задаёт корректное линейное отображение. Полезно посмотреть, как оно действует на базисных векторах. Пусть $e_i$ базис $V$, $f_j$ -- базис $U$, а $f^j$ базис $U^{*}$. Тогда $e_i\otimes f^j$ соответствует линейное отображение с матрицей 
$$ e_{ij}= \bordermatrix{
 & &j&& \cr
 &0&\cdots&\cdots&0\cr
 &\vdots&\ddots && \vdots\cr
i&\vdots& 1 & \ddots& \vdots\cr
 &0&\cdots& \cdots&0
}$$
Такие линейные отображения образуют базис $\Hom(U,V)$. Значит указанное отображение -- изоморфизм.

\endproof 
\ethrm


\subsection{Тензоры на пространстве и их классическое определение}

Понятие тензора появилось в классической механике для ответа на вопрос, как меняются те или иные величины при замене координат. Для того, чтобы обсудить этот аспект тензоров дадим определение:

\dfn Тензором валентности $(p,q)$ на пространстве $V$ называется элемент пространства ${V^{*}}^{\otimes p} \otimes V^{\otimes q}$. Так же будем говорить, что такие элементы -- это $p$ раз ковариантные и $q$ раз контравариантные тензоры. Тензорами валентности $(0,0)$ называются элементы поля $K$ -- скаляры.
\edfn

Теперь я утверждаю, что более менее все встречавшиеся нам структуры на векторном пространстве $V$ являются тензорами.



\exm\\
1) Вектор $v\in V$ является 1 раз контравариантным тензором.\\
2) Элемент двойственного пространства $f \in V^{*}$ является 1 раз ковариантным тензором. Вообще ковариантными называют тензоры, которые соответствуют полилинейным формам на пространстве $V$. Это историческая традиция. Точнее:\\
3) Так как пространство ${V^{*}}^{\otimes p} \simeq \left(V^{\otimes p}\right)^*\simeq \Hom(V;\dots;V,K)$, то тензор валентности $(p,0)$ соответствует полилинейному отображению $V\times\dots \times V \to K$.\\
4) В частности, тензор валентности $(2,0)$ -- это билинейная форма.\\
5) Линейный оператор -- это элемент $\Hom(V,V)\simeq V^{*}\otimes V$, то есть тензор валентности $(1,1)$.\\
6) Структура алгебры на $V$ (без требования ассоциативности) задаётся билинейным отображением $V \times V \to V$, то есть линейным отображением $V\otimes V \to V$ или же элементом $V^{*}\otimes V^* \otimes V$, то есть тензором типа $(2,1)$.\\

Как записать тензор в координатах? Выберем базис $e_1,\dots,e_n$ пространства $V$ и возьмём в двойственном пространстве двойственный базис $e^1,\dots,e^n$. Теперь построим базис тензорного произведения ${V^{*}}^{\otimes p}\otimes V^{\otimes q}$. Он имеет вид $e^{j_1}\otimes\dots\otimes e^{j_p}\otimes e_{i_1}\otimes \dots \otimes e_{i_q}$. Тогда произвольный тензор $T$ валентности $(p,q)$ имеет вид 
$$ T= \sum_{\substack{i_1,\dots,i_q \in \ovl{1,n}\\ j_1,\dots,j_p \in \ovl{1,n}} } \,T_{j_1,\dots,j_p}^{i_1,\dots,i_q}\,\, e^{j_1}\otimes\dots\otimes e^{j_p}\otimes e_{i_1}\otimes \dots \otimes e_{i_q}.$$
Элементы $T_{j_1,\dots,j_p}^{i_1,\dots,i_q}$ называются координатами тензора $T$. Как меняются координаты тензора при замене базиса? 

\thrm Пусть $e_1,\dots,e_n$ старый базис $V$, а $\hat{e}_1,\dots,\hat{e}_n$ -- новый. Пусть $C$ -- матрица перехода из нового базиса в старый, а $D={C^{\top}}^{-1}$. Тогда координаты тензора $T$ в базисе $\hat{e}$ выражаются через старые координаты следующим образом:
$$\hat{T}_{j_1,\dots,j_p}^{i_1,\dots,i_q}=\sum_{\substack{i'_1,\dots,i'_q \in \ovl{1,n}\\ j'_1,\dots,j'_p \in \ovl{1,n}}} \,\,
\prod_{t\in \ovl{1,p}} D_{j_t,j'_t} \prod_{s\in \ovl{1,q}} C_{i_s,i'_s}  \,\,T_{j'_1,\dots,j'_p}^{i'_1,\dots,i'_q}.$$
\ethrm

Важность тензоров в теоретической физике обуславливается тем, что практически все физические объекты -- это тензоры. Точнее: с точки зрения теории относительности пространство-время это некоторое четырёхмерное многообразие $M$ (в двумерной ситуации подошла бы обычная сфера или тор). С каждой точкой $x$ этого многообразия связано касательное пространство в этой точке -- некоторое четырёхмерное пространство $T_x$. Представим себе, что в каждой точке пространства задана плотность вещества (на самом деле не так, но допустим) -- это даёт вам функцию $f \colon M \to \R$ -- скаляр в каждой точке, то есть тензор типа $(0,0)$. 

Направление движения материи можно задать взяв в каждой точке касательный вектор, то есть тензор валентности $(0,1)$ на $T_x$. Дальше, у каждого такого вектора можно считать его <<длину>> и углы между векторами. Для этого надо задать для каждой точки $x$ билинейную форму на касательном пространстве, то есть элемент $T_x^{(2,0)}$. И т.д. Чаще всего такие объекты называют тензорными полями, если хочется подчеркнуть, что в разных точках это тензор вообще говоря на разных пространствах.

Важно, что уравнения в физике не должны зависеть от выбора координат. Можно, конечно, писать какие-то уравнения при помощи координат тензоров и каждый раз проверять, что выбрав новые координаты уравнение будет того же вида. Однако, чем сложнее наука тем сложнее становятся проверки. Становится важно работать с тензорами не рассматривая их координаты. Поэтому на тензорах вводят стандартные операции, которые заведомо не зависят от выбора координат.


\section{Ранг тензора}

Нам хорошо знакомо понятие ранга билинейной формы и ранга линейного отображения. Но и билинейная форма и линейное отображение тесно связаны с понятием тензорного произведения. Можно ли обобщить понятие ранга на произвольные тензоры? Оказывается, что да. 

\dfn Пусть $U_1,\dots, U_n$ пространства и дан $f\in U_1 \otimes \dots \otimes U_n$. Рангом тензора $f$ называется наименьшее такое $r$, что $f=f_1+\dots+f_r$, где $f_i$ -- элементарный тензор.
\edfn

\zd Это определение совпадает с определением ранга для билинейных форм (как элементов $U^*\otimes V^*$) и для линейных отображений (как элементов $U^* \otimes V$).
\ezd

Наибольшую популярность понятие ранга тензора приобрело после работ Штрассена по умножению матриц. А именно, рассмотрим конкретный тензор. Пусть $V$ -- это пространство квадратных матриц размера $k$. Тогда матричное умножение задаёт на $V$ тензор валентности $(2,1)$, то есть элемент $M\in V^*\otimes V^* \otimes V$. Представим себе, что ранг этого тензора равен $r$.

Это означает, что мы можем выбрать такие элементы $f_1,\dots,f_r, g_1, \dots, g_r \in V^*$, и $v_1,\dots,v_r \in V$, что
$$M= \sum f_i\otimes g_i \otimes v_i.$$
Что это означает с точки зрения произведения матриц? Две матрицы -- это два элемента $A,B \in V$. Тогда 
$$AB=\sum f_i(A)g_i(B)v_i$$
$f_i$ и $g_i$ -- это линейные выражения от коэффициентов матриц и значит  $f_i(A)$ и $g_i(B)$ могут быть вычислены при помощи сложения чисел и умножения чисел на фиксированные скаляры. Далее, мы вычисляем произведения $P_i=f_i(A)g_i(B)$. Для этого нужны полноценные операции произведения. После этого, умножаем $P_i$ на $v_i$. Так как коэффициенты $v_i$ постоянны, то для этой операции тоже не нужно честное умножение. Итого, реально используется $r$ настоящих умножений. 

На самом деле, верно и обратное: если если мы найдём формулу для умножения матриц, которая использует только $r$ честных умножений, то можно будет построить такое разложение тензора матричного умножения. 

Пример такого умножения построил Штрассен \cite{StrassenGauss} для матриц $2\times 2$. Пусть 
$$A=\pmat A_{1,1} & A_{1,2} \\ A_{2,1} & A_{2,2} \epmat \text{ а } B=\pmat B_{1,1} & B_{1,2} \\ B_{2,1} & B_{2,2} \epmat.$$
Определим вслед за Штрассеном
$$
\begin{aligned}
&f_1=A_{1,1}+ A_{2,2} &\quad & g_1= B_{1,1}+B_{2,2} &\quad & P_1=f_1\cdot g_1\\
&f_2=A_{2,1}+A_{2,2} &\quad & g_2=B_{1,1}&\quad & P_2=f_2\cdot g_2\\
&f_3=A_{1,1}&\quad & g_3=B_{1,2}-B_{2,2}  &\quad & P_3=f_3\cdot g_3\\
&f_4=A_{2,2}&\quad & g_4=B_{2,1}-B_{1,1} &\quad & P_4=f_4\cdot g_4\\
&f_5=A_{1,1}+A_{1,2}&\quad & g_5=B_{2,2} &\quad & P_5=f_5\cdot g_5\\
&f_6=A_{2,1}-A_{1,1}&\quad & g_6=B_{1,1}+B_{1,2} &\quad & P_6=f_6\cdot g_6\\
&f_7=A_{1,2}-A_{2,2}&\quad & g_7=B_{2,1}+B_{2,2} &\quad & P_7=f_7\cdot g_7.
\end{aligned}
$$


$$
v_1= \pmat 1 & 0 \\ 0 & 1 \epmat,  \quad
v_2=\pmat 0& 0\\0 & -1\epmat,  \quad
v_3=\pmat 0 & 1 \\ 0 & -1 \epmat,  \quad
v_4=\pmat 1& 0\\1 & 0 \epmat,  
$$
$$
v_5=\pmat -1& 1\\ 0& 0\epmat,  \quad
v_6=\pmat 0&0 \\0 & 1\epmat,  \quad
v_7=\pmat 1& 0\\ 0& 0\epmat,  \quad
$$
Элементы $f_i,g_i,v_i$ задают разложение тензора матричного умножения. Если $C=AB$, то для элементов $C$ справедливы формулы
$$ 
\begin{aligned}
&C_{1,1}=P_1+P_4-P_5+P_7\\
&C_{1,2}=P_3+P_5\\
&C_{2,1}=P_2+P_4\\
&C_{2,2}=P_1-P_2+P_3+P_6
\end{aligned}
$$

Элементы $f_i,g_i,v_i$ показывают, что ранг тензора умножения матриц $2\times 2$ не больше $7$. На самом деле ранг умножения матриц $2\times 2$ действительно равен $7$.

Но как это поможет для умножения матриц больших размеров? Как обычно предположим, что $n=2^k$. Тогда матрицы $A$ и $B$ можно разбить на подматрицы размера $2^{k-1}$ которые мы обозначим как $A_{i,j}, B_{i,j}$ как и раньше. Заметим, что теперь мы снова перемножаем матрицы $2\times 2$, элементы которых на этот раз не числа, а матрицы. 

Если мы посмотрим процедуру умножения матриц, которая произошла из разложения тензора матричного умножения , то увидим, что в этой процедуре мы не использовали коммутативности умножения элементов поля. Действительно, элементы $A$ всегда находились слева в формуле по сравнению с элементами $B$, а элементы $A$ как и элементы $B$ между собой не перемножались.

Значит указанная формула справедлива и для умножения блочных матриц. Мы можем применить указанный способ рекуррентно. Тогда необходимое  число умножений в данном алгоритме будет удовлетворять соотношению:
$$T(n)=7\cdot T(n/2).$$
То есть всего мы потратим $7^{k}=n^{\log_2 7}$ умножений.

\zd Оцените количество сложений в алгоритме как $6\cdot 7^k$.
\ezd

На текущий момент при помощи оценки рангов (правда не совсем тензора матричного умножения) получены оценки $O(n^{2.373})$ операций для умножений матриц $n\times n$.

Почему мы не знаем окончательного ответа на то, какой ранг у умножения матриц размера $k\times k$? Потому что ранг тензора тяжело считать. Прежде всего тут стоит отметить, что ранг тензора, зависит от того поля, над которым мы этот ранг считаем. 

Например, ранг умножения комплексных чисел над $\R$ равен $3$ (а не $4$, как вы могли бы подумать). Но ранг тензора умножения комплексных чисел над $\C$ равен $2$ (коэффициенты тензора вещественные, но мы их  рассматриваем как элементы из $\C$).  

Так вот. Вычисление ранга тензора над $\C$ и над $\R$ является NP-трудной задачей. А вычисление ранга тензора над $\Z$, то есть когда мы хотим, чтобы все коэффициенты были целочисленными, и вовсе алгоритмически не разрешим. Это есть следствие негативного решения 10 проблемы Гильберта. Вопрос алгоритмической неразрешимости над $\Q$ остаётся открытым (как и аналог 10 проблемы Гильберта).




\section{Внешняя и симметрическая алгебры}

В этом разделе мы будем рассматривать векторное пространство $V$ над полем характеристики $0$. Есть общая теория не только для полей, но и для произвольных колец, однако, даже базовые конструкции в этой теории сложнее и некоторые её утверждения просто неверны над полем ненулевой характеристики.

\dfn Определим пространство $\Lambda^k V$ как подпространство $V^{\otimes k}$. Это подпространство выделяется следующими условиями -- для любой перестановки из $\sigma \in S_k$ и любого тензора $a\in \Lambda^k V$ верно, что $\sigma(a)=\sgn(\sigma)a$. Под $\sigma(a)$ подразумевается действие перестановки $\sigma$ на тензор $a$ перестановкой его компонент. Аналогично определяется подпространство $\Sym^k V \leq V^{\otimes^k}$, чьи элементы удовлетворяют свойству: $\sigma(a)=a$.
\edfn

\lm Имеет место проектор $Alt \colon V^{\otimes k} \to \Lambda^k V$ (называется альтернированием) 
$$a \to Alt(a)=\frac{1}{k!} \sum_{\sigma \in S_k} \sgn (\sigma) \sigma(a).$$
Аналогично отображение  симметризации
$$ a \to S(a)= \frac{1}{k!} \sum_{\sigma \in S_k} \sigma(a)$$
есть проектор на подпространство $\Sym^k V$.
\proof Докажем только первую часть. Прежде всего заметим, что $Alt$ принимает значение в подпространстве кососимметричных тензоров. Действительно 
$$\tau(Alt(a))=\frac{1}{k!}\sum_{\sigma \in S_k}\sgn(\sigma) \tau(\sigma(a))= \sgn{\tau} \frac{1}{k!}\sum_{\tau\sigma \in S_k} \sgn(\tau\sigma) \tau(\sigma(a))=\sgn(\tau) Alt(a).$$
Далее, покажем, что для любого кососимметричного тензора $a$ верно, что $Alt(a)=a$. Это покажет, что $Alt$ -- есть проектор и в его образе лежат все элементы из $\Lambda^k(V)$, то есть ровно то, что осталось показать. Итак пусть $a\in \Lambda^k(V)$. Тогда 
$$Alt(a)=\frac{1}{k!} \sum_{\sigma \in S_k} \sgn (\sigma) \sigma(a)=\frac{1}{k!} \sum_{\sigma \in S_k} \sgn^2(\sigma) a=a.$$
\endproof
\elm

\dfn Пусть $e_1,\dots, e_k$ набор элементов из $V$. Определим элементы $e_1\wedge \dots \wedge e_k \in \Lambda^k V$ как образы при проекции $e_1\otimes \dots \otimes e_k$.
\edfn

\thrm Пусть $e_1,\dots, e_n$ базис пространства $V$. Тогда элементы $e_{i_1}\wedge \dots \wedge e_{i_k}$, где $i_1<i_2< \dots < i_k$ образуют базис пространства $\Lambda^k V$. Элемент $e_{i_1}\wedge \dots \wedge e_{i_k}$, где $i_1<i_2< \dots < i_k$  обозначим за $e_{\Gamma}$, где $\Gamma =\{i_1,\dots,i_k\}\subseteq \ovl{1,n}$ подмножество размера $k$. Это взаимооднозначное соответствие. В частности, размерность $\dim \Lambda^k V = C^k_n$. 
\proof Прежде всего заметим, что это действительно порождающая система. Для этого вспомним, что $e_{i_1,\dots,i_k}$ по всем возможным наборам $i_1,\dots,i_k$ порождают тензорное произведение $V^{\otimes k}$. Но раз они порождают тензорное произведение, то они порождают и его образ при проекции $Alt$ на пространство кососимметричных тензоров. Далее заметим, что $$Alt(e_{i_1,\dots i_k})= \sgn(\sigma) Alt(e_{i_{\sigma(1)},\dots i_{\sigma(k)}}).$$
В частности, если в наборе есть два повторяющихся индекса, то элемент проектируется в 0. Далее, это же соотношение даёт, что,  с точностью до знака $Alt$ от тензора для набора $i_1,\dots,i_k$ совпадает с проекцией для упорядочевания этого набора. Таким образом, из набора образующих можно исключить неупорядоченные наборы и наборы с повторениями, что и требовалось.

\noindent Покажем линейную независимость. Пусть  
$$\sum_{\Gamma} \alpha_{\Gamma} e_{\Gamma}=0.$$
Тогда расписывая эту сумму через $e_{i_1}\otimes \dots \otimes e_{i_k}$ -- базисные элементы $V^{\otimes k}$ получаем
$$\frac{1}{k!}\sum_{\Gamma} \alpha_{\Gamma} \sum_{\sigma \in S_{\Gamma}} \sgn(\sigma) e_{\sigma(i_1),\dots,\sigma(i_k)} =0 $$
Заметим, что все слагаемые соответствуют разным базисным элементам. Тогда, все коэффициенты равны нулю. В частности, коэффициенты при $ e_{i_1, \dots, i_k}$, которые равны $\frac{1}{k!}\alpha_{\Gamma}$. 

\endproof
\ethrm

\thrm Аналогично, пусть $e_1,\dots, e_n$ базис пространства $V$. Тогда элементы образы тензоров $e_{i_1}\otimes \dots \otimes e_{i_k}$, где $i_1\leq \dots \leq i_k$ образуют базис пространства $\Sym^k V$.
\ethrm

\dfn Определим $k$-ую внешнюю степень линейного отображения $L\colon V \to W$ -- отображение $\Lambda^{k} L  \colon \Lambda^k V \to \Lambda^k W$ заданное на тензорах по правилу $v_1\wedge \dots \wedge v_k \to L v_1 \wedge \dots \wedge L v_k$. 
\edfn

Для того, чтобы показать корректность такого определения покажем следующую теорему:

\thrm Рассмотрим отображение $g=Alt \circ i \colon V^{\times k} \to \Lambda^k(V)$. Тогда для любого полилинейного кососимметричного $h \colon V^{\times k} \to U$ существует единственное отображение $\hat{h} \colon \Lambda^k(V) \to U$, что $\hat{h} \circ g = h$.
\proof Зафиксируем подходящее $h \colon V^{\times k} \to U$. Так как это полилинейное отображение, то есть линейное $\hat{\hat{h}}\colon V^{\otimes k} \to U$, что $\hat{\hat{h}} \circ i =h$. Покажем, что ограничение $\hat{\hat{h}}$ на $\Lambda^k(V)$ есть искомое отображение. Действительно $$\hat{h}(v_1\wedge\dots\wedge v_k)=\hat{\hat{h}}(v_1\wedge\dots\wedge v_k)=\hat{\hat{h}} \left(\frac{1}{k!} \sum_{\sigma \in S_k} \sgn(\sigma) v_{\sigma(1)}\otimes \dots \otimes v_{\sigma(k)} \right) =$$
$$=\frac{1}{k!} \sum_{\sigma \in S_k} \sgn(\sigma) h(v_{\sigma(1)},\dots,v_{\sigma(k)})=\frac{1}{k!}k!h(v_1,\dots,v_k)=h(v_1,\dots,v_k).$$

Осталось показать единственность. Для этого заметим, что из условия  $\hat{h}$ однозначно задано на базисе $e_{\Gamma}$.
\endproof
\ethrm


\fct Полезно смотреть не на пространства $\Lambda^k (V)$ и $\Sym^k V$, а на пространства $\Lambda^k(V^*)$ и $\Sym^k(V^*)$, потому что они допускают привычную и наглядную интерпретацию --- их элементы это полилинейные функции со специальными свойствами.
\efct


\exm \\
1) Элемент $\Lambda^2(V^*)$ --- это просто кососимметрическая билинейная форма.\\
2) А элемент $\Sym^2 V^*$ -- это симметрическая билинейная форма или просто квадратичная форма.\\
3) Элемент $\Lambda^{\dim V} V^*$ -- это просто форма объёма на $V$.\\

Мы хотим ввести умножение на кососимметричных тензорах. Есть два подхода: первый из них -- потребовать, чтобы пара $v_1 \wedge \dots \wedge v_p , u_1\wedge \dots \wedge u_q$ переходила в $v_1 \wedge \dots \wedge v_p \wedge u_1\wedge \dots \wedge u_q$. С таким подходом возникает вопрос о корректности. С другой стороны, такое умножение удобно вычислять. Второй подход -- связать это умножение с формальным умножением тензоров. Проблема возникает в следующем -- если $T_1$ и $T_2$ -- кососимметрические тензоры, то $T_1\otimes T_2$, вообще говоря, не кососимметричный. Мы доведём до конца второй путь.

\thrm[Внешняя алгебра] Рассмотрим пространство $\Lambda(V)=\oplus_{k=0}^{\dim V} \Lambda^k(V)$ и введём на нём структуру ассоциативной алгебры по правилу $ f\wedge g= Alt(f\otimes g)$. Если $f\in \Lambda^p(V)$, а $g \in \Lambda^q(V)$, то $f\wedge g=(-1)^{pq}g \wedge f$. Такое свойство называется градуированной коммутативностью. Более того, пара $v_1 \wedge \dots \wedge v_p , u_1\wedge \dots \wedge u_q$ переходит при этом умножении в $v_1 \wedge \dots \wedge v_p \wedge u_1\wedge \dots \wedge u_q$. Такое умножение называется внешним произведением тензоров.
\proof Для этого удобно проверить тождество $Alt(Alt(T_1)\otimes T_2)= Alt(T_1\otimes T_2)= Alt(T_1 \otimes Alt(T_2))$, которое говорит, что внутри альтернирования можно свободно альтернировать сомножители не боясь ничего поменять. Действительно
$$\frac{1}{k!}\sum_{\sigma \in S_{k}}\sgn(\sigma) Alt(T_1^{\sigma}\otimes T_2)=\frac{1}{k!}\sum_{\sigma \in S_{k}} \sgn^2(\sigma) Alt(T_1\otimes T_2)=Alt(T_1 \otimes T_2).$$
Аналогично получается второе равенство. Теперь видно, что 
$$Alt(v_1 \wedge \dots \wedge v_p \otimes u_1\wedge \dots \wedge u_q)=Alt(v_1 \otimes \dots \otimes v_p \otimes u_1\wedge \dots \wedge u_q)=$$
$$=Alt( v_1 \otimes \dots \otimes v_p \otimes u_1\otimes \dots \otimes u_q) =v_1 \wedge \dots \wedge v_p  \wedge u_1\wedge \dots \wedge u_q .$$
Это показывает связь нашего определения умножения с ожидаемым определением. Ассоциативность теперь легко проверить на базисных элементах, как и градуированную коммутативность.
\endproof
\ethrm





\thrm[Симметрическая алгебра] Рассмотрим пространство $\Sym(V)=\oplus_k \Sym^k(V)$. Тогда на нём можно ввести структуру ассоциативной коммутативной алгебры задав умножение как $ f*g= S(f\otimes g)$. Более того, указанная алгебра изоморфна алгебре многочленов.
\ethrm



Изначально, внешняя алгебра была нужна для <<исчисления подпространств>> в пространстве $V$. А именно, подпространству $U\leq V$ размерности $k$ можно сопоставить прямую $\Lambda^k U$ в $\Lambda^k V$. Более того, задав на $U$ форму объёма можно выбрать на этой прямой определённую точку. Такие объекты теперь можно перемножать и складывать, хотя в общем случае может получиться и объект, не соответствующий никакому подпространству. 

Сейчас понятие внешней алгебры может служить удобным формализмом для определения интегрирования. А именно, по многообразию размерности $n$ можно проинтегрировать кососимметричное тензорное поле валентности $(n,0)$, то есть заданную в каждой точке многообразия форму объёма на касательном пространстве. В общем же виде кососимметричные тензорные поля типа $(k,0)$ называются дифференциальными формами порядка $k$. На таких дифференциальных формах задана операция взятия дифференциала, делающая из $k$-формы $k+1$-форму. Это ещё одна из канонических <<бескоординатных>> операций. Об этом вам расскажут в курсе анализа.

\subsection{Внешняя алгебра и определитель}

Покажем способ применения внешней степени для доказательства тождеств про определители. Введём обозначение.

\dfn Пусть $A$ -- матрица из $M_{m\times n}(K)$. Тогда если $\Gamma \subseteq \{1,\dots,n\}$. Тогда за $A_{\Gamma}$ обозначим матрицу состоящую из столбцов матрицы $A$ с элементами из $\Gamma$. Аналогично, если $\Gamma \subseteq \{1,\dots,m\}$ то за $A^{\Gamma}$ обозначим подматрицу $A$, из строк, чьи индексы лежат в $\Gamma$.
\edfn

Посмотрим, как устроены координаты внешнего произведения произвольного набора векторов. Пусть $e_1,\dots,e_n$ -- базис $V$, $v_1,\dots,v_k$ --  произвольный набор векторов. Пусть $A$ -- это матрица координат векторов $v_1,\dots,v_k$.
Рассмотрим $v_1 \wedge \dots \wedge v_k$. Это полилинейная кососимметрическая функция от $v_1,\dots,v_k$, а вместе с ней и координаты этого произведения. Ясно, что координаты при $e_{\Gamma}=e_{i_1}\wedge\dots\wedge e_{i_k}$ зависят только от строк матрицы $A$ с номерами из $\Gamma$. Если в этих строках стоит единичная матрица, то ясно, что координата при $e_{\Gamma}$ -- это единица. Вспоминая, что есть определитель, получаем, что в общем виде коэффициент при $e_{\Gamma}$ это определитель $A^{\Gamma}$. Итого 
$$v_1 \wedge \dots \wedge v_k= \sum_{\substack{\Gamma \subseteq \{1,\dots,n\}\\ |\Gamma|=k}} \det A^{\Gamma} e_{\Gamma}.$$

Несложно получить эту формулу непосредственно.

Пусть есть отображение $L \colon U \to V$, где $\dim U= \dim V = n$ и $A$ матрица $L$ в базисах $e_1,\dots e_n$ и $f_1,\dots,f_n$. Тогда $$\Lambda^n L(e_1\wedge \dots \wedge e_n) = \det A \,\,f_1 \wedge \dots \wedge f_n.$$
Из этого замечания уже легко получить мультипликативность определителя. Поступая аналогично можно доказать более общую теорему:



\thrm[Формула Бине-Коши] Рассмотрим две матрицы $A\in M_{m\times n}(K)$ и $B\in M_{n\times m}(K)$. Пусть $m\leq n$. Тогда
$$\det(AB)=\sum_{\substack{\Gamma \subseteq \{1,\dots,n\}\\ |\Gamma|=m}} \det A_{\Gamma} \det B^{\Gamma}.$$
\proof Рассмотрим линейные отображения, заданные матрицами $A\colon K^n \to K^m$ и  $B \colon K^m \to K^n$. Тогда $\Lambda^m (AB)$ есть оператор домножения на $\det AB$. С другой стороны $\Lambda^m(AB)=\Lambda^m(A) \Lambda^m(B)$. Вычислим матрицы этих отображений. Матрица $\Lambda^m(B)$ есть столбец высоты $C_n^m$, чьи элементы проиндексированы $\Gamma \subseteq \{1,\dots,n\}$ размера $m$. Аналогично матрица $\Lambda^m(A)$  есть строчка, чьи элементы проиндексированы аналогично.
Пусть $e_1,\dots,e_m$ -- стандартный базис $K^m$, $e=e_1\wedge \dots \wedge e_m$ единственный базисный элемент $\Lambda^m(K)$, а $f_1,\dots,f_n$ -- стандартный базис $K^n$. Тогда 
$$\Lambda^m(B)(e)=\sum_{\substack{\Gamma \subseteq \ovl{1,n} \\ |\Gamma|=m}} \det B^{\Gamma} f_{\Gamma}.$$
 С другой стороны, 
$$\Lambda^m(A)(f_{\Gamma})=\det(A_{\Gamma})e.$$
Осталось перемножить строчку на столбец.
\endproof
\ethrm

Покажем, как можно использовать формулу Бине-Коши. Вначале докажем общие факты про графы.

\dfn Определим матрицу оператора Лапласа для графа $G$ как $L(G)=D-A(G)$.
Ориентируем произвольным образом рёбра графа $G$. Тогда определим ориентированную матрицу инцидентности, как матрицу, строки которой соответствуют вершинам $G$, столбцы -- рёбрам $G$. В столбце соответствующем ребру стоит $1$ в позиции, конца этого ребра и $-1$ в позиции начала. 
\edfn

\lm Выполнено равенство $L(G)=B^\top B$ при любой ориентации рёбер графа.
\elm

Это сразу говорит нам, что матрица $L(G)$ неотрицательно определена. Однако у матрицы $L(G)$ всегда нетривиальное ядро. Действительно, столбец $(1,\dots,1)^\top$ лежит в ядре $L(G)$. Можно сказать точнее:

\lm Пусть у графа $G$ ровно $k$ компонент связности. Тогда $\dim \Ker L(G)\geq k$. На самом деле, выполнено равенство.   
\elm

\crl Если граф $G$ не связен, то алгебраическое дополнение любого элемента в матрице $L(G)$ равно $0$.
\ecrl

\lm Пусть граф $G$ -- дерево. Тогда алгебраическое дополнение любого элемента матрицы $L(G)$ равно 1.
\elm
\proof Пусть нам дан элемент с индексами $i,j$ в $L(G)$. Если $B$ -- ориентированная матрица инцидентности графа $G$, то этот определитель вычисляется как произведение  $\det B^{\ovl{i}}$ и $\det B^{\ovl{j}}$. Сведём вычисление каждого из этих определителей к ситуации, когда граф $G$ -- путь, а выкинута первая строка. Для того, чтобы разобраться с номером вершины просто поставим выкидываемую строчку на первое место. Теперь заметим, что оба определителя не меняются при элементарных преобразованиях столбцов $B$.
Теперь опишем процедуру <<выпрямления>> дерева на языке элементарных преобразований. 
\endproof

\crl[Теорема о деревьях] Количество остовных деревьев в графе $G$ есть алгебраическое дополнение любого элемента матрица $L(G)$.
\ecrl
\proof Прежде всего отметим, что если $G$ не связен, то и любой минор $L(G)$ равен $0$. $L(G)=BB^\top$, где $B$ -- ориентированная матрица смежности графа $G$. Найти какое-то алгебраическое дополнение $L$ -- это то же самое, что найти определитель $\hat{B}\hat{B}^\top$, где $\hat{B}$ -- матрица $B$ с одной выкинутой строкой. Определитель $\hat B \hat{B}^\top$ легко считается по формуле Бине-Коши. В этой формуле мы должны выбрать $\hat{B_{\Gamma}}$, то есть $n-1$ строк $\hat{B}$ и найти
$$(\det \hat{B}_{\Gamma})^2=\det(\hat{B}_{\Gamma} \hat{B}_{\Gamma}^\top)$$
Но $\det \hat{B}_{\Gamma} \hat{B}_{\Gamma}^\top$ -- это минор матрицы Лапласа графа, чьи рёбра соответствуют строкам матрицы $B$. Если этот граф  -- не дерево, то он не связен и этот определитель равен $0$. Если это дерево, то оно остовное и этот определитель равен единице. Так как по формуле Бине-Коши для вычисления определителя надо просуммировать такие слагаемые, то мы получаем ровно количество остовных деревьев.
\endproof


\crl[Формула Кэли] Количество различных помеченных деревьев на $n$ вершинах равно $n^{n-2}$.
\ecrl


\chapter{Делимость и многочлены от многих переменных}

Настала пора вернуться к незаконченной теме про делимость. В этой связи мы снова вернёмся к предположению, что все кольца коммутативны. На этот раз мы поговорим подробно о кольце многочленов от $n$ переменных. Мы знаем, что кольцо многочленов $K[x]$ над полем $K$ является областью главных идеалов и вывели из этого однозначность разложения на множители в $K[x]$. Есть ли надежда повторить тоже самое для многочленов от двух и более переменных? 

Ответ на этот вопрос даёт следующий пример: рассмотрим идеал, порождённый независимыми переменными $x,y$ в кольце $K[x,y]$. Этот идеал нельзя породить одним элементом.

Однако мы интуитивно представляем то, что разложение многочленов на множители однозначно. Математически такое свойство кольца называлось факториальностью.

\dfn
Область целостности $R$ называется факториальным кольцом, если для любого $a\in R$, что $a\neq 0$  существует представление $a=\eps p_1\dots p_k$, где $\eps \in R^*$, а $p_1,\dots,p_k$ -- простые элементы $R$.
\edfn

\rm Исходя из определение простоты, такое разложение единственно с точностью до ассоциированности. Действительно, если $f=\eps\prod p_i= \delta \prod q_j$, то $p_1 \di \prod q_j$ и благодаря простоте делит скажем $q_1$. Но тогда $p_1h=q_1$, откуда, благодаря неприводимости $q_1$ получаем, что $h$ обратим, то есть, что $p_1 \sim q_1$. Тогда можно сократить на $p_1$ и продолжить по индукции.
\erm


\rm В факториальном кольце есть НОД любых двух элементов (естественно, с точностью до ассоциированности).
\erm


Итак, наша ближайшая цель поговорить о факториальности колец многочленов. Однако, для приложений к теории чисел нам будет необходимо разработать теорию в достаточной общности, чтобы применить её и над $\Z$, а не только над полем. Заметим, что $\Z$ и поле $K$ являются факториальными кольцами. Мы покажем, что факториальность кольца влечёт факториальность кольца многочленов над ним, что полностью ответит на наши вопросы.

Для этого доказательства нам понадобится конструкция факторкольца, важность которой мы почувствуем позднее.

\dfn Пусть $R$ кольцо, а $I\leq R$ -- идеал. Тогда на факторгруппе $R/I$ можно ввести структуру кольца определив умножение по формуле $\ovl{a}\ovl{b}=\ovl{ab}$. 
\edfn

\dfn Идеал $I$ называется простым, если выполнено, что $ab\in I$, то $a\in I$ или $b\in I$.
\edfn

\rm Элемент $p\in R$ отличный от нуля прост тогда и только тогда, когда идеал $(p)$ прост, тогда и только тогда, когда $R/(p)$ -- область целостности.
\erm


\section{Многочлены над факториальным кольцом}

Наша задача обсудить, что происходит с кольцом многочленов от многих переменных.


\lm[Гаусс] Пусть $R$ -- кольцо. Тогда любой простой элемент $p$ из $R$ остаётся простым в $R[x]$.
\proof
Теоретически удобно воспользоваться следующим соображением: чтобы показать, что элемент прост надо показать, что идеал $(p)$ в $R[x]$ прост, а для этого необходимо и достаточно установить, что $R[x]/(p)$ есть область целостности. Как же описать $R[x]/(p)$? Я утверждаю, что оно изоморфно $(R/p)[x]$. Действительно, из $R[x]$ есть отображение в $(R/p)[x]$, которое берёт все коэффициенты по модулю $p$. Очевидно, в его ядре лежат ровно те многочлены, все коэффициенты которых делятся на $p$, то есть многочлены кратные $p$ в $R[x]$. Но ровно они и образуют идеал $(p)$. Осталось заметить, что кольцо $R/p$ и, вслед за ним, кольцо $R/p[x]$ являются областями целостности.

У этого доказательства есть другая, более элементарная реинкарнация. А именно, формально нам надо доказать, что если произведение двух многочленов $f(x)g(x)$ делится на $p$ (то есть все коэффициенты кратны $p$), то тогда какой-то из них делится на $p$. Пусть это не так. Возьмём тогда у $f$ и у $g$ самые младшие коэффициенты $a_i$ и  $b_j$, которые не делятся на $p$. Тогда посмотрим на коэффициент с номером $i+j$  в произведении. Он имеет вид $c_{i+j}= a_ib_j + \sum_{k \neq i} a_k b_{i+j -k}$. Я утверждаю, что $c_{i+j}$ не делится на $p$. Для этого заметим, что любое слагаемое в сумме делится на $p$, так как либо $k<i$ и тогда $a_i \di p$, либо $k>i$, то есть $i+j-k<j$ и следовательно $b_j \di p$. Противоречие с тем, что $c_{i+j}$ должен делиться на $p$.   
\endproof
\elm

\upr Поясните, почему оба доказательства одинаковы.
\eupr

\dfn Пусть $f(x)$ -- многочлен над факториальным кольцом $R$. Тогда содержанием $f$ называется $\cnt(f)=\Nod (a_i)$, где $a_i$ коэффициенты $f$. 
\edfn

Следующее следствие тоже называют леммой Гаусса.

\crl Если $f(x)=g(x)h(x)$, где $f,g,h \in R[x]$, то $\cnt(f)=\cnt(g)\cnt(h)$
\proof Для начала, упростим задачу, то есть сведём задачу к случаю $\cnt g= \cnt h =1$. Для этого надо рассмотреть многочлены $\frac{g}{\cnt g}$ и $\frac{h}{\cnt h}$. Их произведение есть $\frac{f}{\cnt{g}\cnt{h}}$ имеет содержание $\frac{\cnt f}{\cnt g \cnt h}$ и если показать его единичность, то мы добьёмся требуемого. Итак считаем, что $\cnt g= \cnt h=1$. Если $\cnt f$ не обратим, то $\cnt f \di p$, где $p$ простой элемент из $R$. Но тогда один из $g$ или $h$ делится на $p$ благодаря простоте $p$. 
\endproof
\ecrl


\lm Пусть для многочлена $f(x) \in R[x]$  имеет место разложение $f(x)=g(x)h(x)$, где  $g(x), h(x) \in Q(R)[x]$. Пусть $c \in Q(R)^*$, такая что $cg \in R[x]$ и $\cnt(cg)=1$. Тогда $c^{-1}h \in R[x]$, что означает, что $f(x)=cg(x)c^{-1}h(x)$ -- есть произведение двух многочленов из $R[x]$ пропорциональных исходным.
\proof Для начала отметим, что такая константа $c$ существует. Заменяя $g$ на $cg$ можно считать, что $c=1$. В этом предположении нам надо доказать, что $h\in R[x]$.
Домножим $h$ на $d\in R$, так что $dh \in R[x]$. Тогда $df=g dh$ и значит
$$d\cnt f = \cnt(g) \cnt(dh)=\cnt(dh).$$
Таким образом, коэффициенты $dh$ делятся на $d$, а значит исходный $h$ был из $R[x]$.
\endproof
\elm

Заметим, что для любого ненулевого многочлена $g(x)$ указанная в лемме константа всегда существует. Теперь нам легко доказать теорему.


\thrm Пусть $R$ -- факториальное кольцо. Тогда кольцо $R[x]$ факториально. Более того, имеет место следующее описание простых элементов кольца $R[x]$ с точностью до ассоциированности:\\
1) $f\in R[x]$ такой, что $cont(f)=1$ и $f$ неприводим в $Q(R)[x]$.\\
2) $f=p \in R$ -- простой в $R$.
\proof 
Для начала покажем, что все указанные ситуации приводят к простым элементам в кольце $R[x]$.
Итак, пусть $f \in R[x]$ неприводим в $Q(R)[x]$ и $cont(f)=1$. Если $gh\di f$ в $R[x]$, то это же верно в $Q(R)$. По условию $f$ неприводимый, а значит простой в $R[x]$ и можно считать, например, что $g\di f$ в $Q(R)[x]$. Тогда $g= fh$. Тогда $h\in R[x]$, по лемме. Значит, $g \di f$ в $R[x]$. Второй случай непосредственно следует из леммы Гаусса.


Теперь покажем, что любой элемент $R[x]$ раскладывается в произведение указанных простых. Для этого сначала разложим $f$ в $Q(R)[x]$ в произведение неприводимых $f=\prod g_i$, $g_i \in Q(R)[x]$. Далее сделаем из $g_i$ элементы $\hat{g}_i$ из $R[x]$ с $cont(g_i)=1$. Тогда $f=a\prod \hat{g}_i$, где $a\in Q(R)$. Заметим, что $a\in R$ по лемме. Осталось разложить $a$ на простые из $R$.
\endproof
\ethrm








\section{Признаки неприводимости для многочленов}

Теперь наша задача поговорить про неприводимость многочленов над целыми числами или над $\Q$. 
Прежде всего отметим, что обе задачи тесно связаны. А именно, если взять многочлен с рациональными коэффициентами, то домножив его на подходящую рациональную константу мы получим многочлен с целыми коэффициентами и содержанием 1, который по доказанному ранее неприводим тогда и только тогда, когда неприводим исходный. Обратно, неприводимость целочисленных многочленов интересна только в случае, когда содержание этих многочленов равно единице. А в этом случае это эквивалентно рациональной неприводимости. Однако все теоремы я буду формулировать в общем контексте.



\thrm[Редукционный критерий] Пусть $R$ факториальное кольцо, $f \in  R[x]$ многочлен, а $p$ -- простой элемент. Тогда, если старший коэффициент $f$ не делится на $p$ и $\ovl{f}$ неприводим в кольце $R/p[x]$, то он неприводим над $Q(R)$. 
\proof Прежде всего перейдём от многочлена $f$ к $\frac{f}{cont(f)}$ с содержанием равным 1. Достаточно доказать неприводимость  последнего над $Q(R)$, которая эквивалентна его неприводимости над $R$. Итак пусть $cont(f)=1$ и пусть $f=gh$, где $g,h$ --  не константы. Старшие коэффициенты $g$ и $h$ тоже не делятся на $p$. Имеем $\ovl{f}= \ovl{g}\ovl{h}$ и $\deg g = \deg \ovl{g}$ и $\deg h = \deg \ovl{h}$, что даёт нетривиальное разложение $\ovl{f}$ и приводит к противоречию.
\endproof
\ethrm

Вот примеры о том, как пользоваться этим критерием и что не надо забывать про условие со старшим коэффициентом. 


\exm\\
1) Многочлен $x^3+x+1$ неприводим над $\F_2=\Z/2$, потому что у него нет корней. Следовательно многочлены $3x^3+8x^2+5x+7$ и скажем, $5x^3-4x^2+x+15$ неприводимы над $\Q$.\\
2) Рассмотрим многочлен $px^2+x$. Он приводим, но по модулю $p$ -- неприводим.\\
3) Критерий из теоремы сформулирован не в самом сильном виде. А именно, представим себе, например, что по модулю 2 многочлен степени пять разложился в произведение двух неприводимых степени 2 и 3, а по модулю 3 -- в виде произведения степени 4 и 1. Ясно, что он неприводим.\\
4) Не стоит забывать, что если многочлен неприводим над $\R$, то он так же неприводим над $\Q$. Это, правда, очень слабый критерий, но в комбинации с пунктом 3) может что-то дать.\\
5) Есть, однако, такие многочлены, которые неприводимы, но раскладываются по модулю любого простого. Например, $x^4+1$. Действительно,
$$x^4+1=(x-e^{\frac{i\pi}{8}})(x-e^{\frac{3i\pi}{8}})(x-e^{\frac{5i\pi}{8}})(x-e^{\frac{7i\pi}{8}})= (x^2+i)(x^2-i)=(x^2+\sqrt{2}x+1)(x^2-\sqrt{2}x+1)=(x^{2}+\sqrt{-2}x+1)(x^{2}-\sqrt{-2}x+1).$$
Он не имеет рациональных корней, а любые множители степени 2 имеют нерациональный коэффициент. Значит он неприводим.

С другой стороны, по любому нечётному простому модулю либо из $-1$, либо из $2$ либо из $-2$ извлекается корень. Случай $p=2$ легко проверяется руками.
6) Но не всё так плохо. Если мы зафиксируем степень $d$ и рассмотрим множество многочленов $f(x)\in \Z[x]$ с коэффициентами, ограниченными некоторой константой $M$, то при $M$ стремящемся к бесконечности доля неприводимых многочленов будет стремиться к $1$, а среди них, доля многочленов, неприводимость которых можно проверить при помощи редукции так же стремится к $1$.\\


Покажем теперь некоторый критерий неприводимости, который применим в случае, если разложение по модулю $p$ получилось очень неудачное. А именно, представим себе, что $f(x) \equiv c x^n \mod p$. То есть многочлен развалился в произведение максимально возможного числа одинаковых множителей. Оказывается, что в этом случае неприводимость многочлена $f$ зависит от его класса по модулю $p^2$. Точнее:

\thrm[Признак Эйзенштейна] Пусть $R$ -- факториальное кольцо и $f(x)= a_0 + \dots + a_n x^n$. Если $a_n \ndi p$, все $a_i \di p$ при $i<n$, но $a_0\ndi p^2$, то многочлен $f(x)$ неприводим.
\proof
Предположим, что $f=gh$. Тогда $c x^n=\ovl{f}=\ovl{g}\ovl{h}$ в $R/p[x]$ и значит в $Q(R/p)[x]$. Но в поле разложение на неприводимые однозначно. Отсюда $\ovl{g}=ax^k$ и $\ovl{h}=b x^l$. Но тогда их младшие члены делятся на $p$ и, значит, $a_0\di p^2$. Противоречие. 
\endproof
\ethrm 

\upr Распишите это доказательство на языке элементов.
\eupr

Всё, что мы пока обсуждали не говорит ничего о том, как же разложить многочлен на неприводимые множители. Первое, что мы обсудим -- вопрос, почему эта задача в принципе разрешима.

Итак, пусть есть целочисленный многочлен $f(x)$ и мы хотим разложить его на множители. Мы будем искать разложение на целочисленные многочлены. Заметим, что хотя бы один из них обязан иметь степень меньшую, чем $[\frac{n}{2}]$. Вспомним о задаче интерполяции. Если $g$ -- искомый делитель $f$, то $g$ определяется своими значениями в $[\frac{n}{2}]+1$ точке, например в точках $0,1,\dots, [\frac{n}{2}]$. Более того, $f(i) \di g(i)$. Таким образом, набор $g(0),\dots, g([\frac{n}{2}])$ состоит из делителей $f(0),\dots,f([\frac{n}{2}])$. Найти все такие наборы -- конечный перебор. По каждому набору кандидат на делитель $g$ восстанавливается однозначно.

Прежде чем продвинуться дальше в исследовании разложения многочленов от одной переменной на множители стоит немного поговорить о задаче разложения многочленов от нескольких переменных. Сейчас мы увидим ещё один трюк от Кронекера, который позволит свести эту задачу к предыдущей.

\thrm[Трюк] Пусть $R$ -- кольцо. Тогда различным разложениям $f(x_1,\dots,x_n)\in R[x_1,\dots,x_n]$   соответствуют различные разложения $\hat{f}=f(x, x^d, x^{d^2}, \dots, x^{d^{n-1}})$ для $d$ больших $\max_{i=1}^n \{\deg_{x_i} f\}$.
\proof Пусть $f=g_1h_1=g_2h_2$ и пусть $g_1\neq g_2$. Покажем, что $\hat{g_1}\neq \hat{g_2}$. Для этого посмотрим что происходит с мономом $x^{\alpha}$ при указанном преобразовании если $\alpha_i < d$. Этот моном переходит в многочлен $x^{\alpha_1+\alpha_2d+\dots+\alpha_n d^{n-1}}$. Так как по условию все $\alpha_i<d$, то такая степень  $x^{\alpha_1+\alpha_2d+\dots+\alpha_n d^{n-1}}$ может быть получена при подстановке только из монома $x^{\alpha}$. Заметим теперь, что $\deg_{x_i} g_j \leq \deg_{x_i} f <d$. Следовательно мономы многочленов $g_j(x)$ однозначно восстанавливаются по мономам $\hat{g_j}$.
\endproof
\ethrm

Заметим так же, что в теореме указан и метод по которому из многочлена $\hat{g}$ можно восстановить многочлен $\hat h$,
К сожалению, не стоит ожидать взаимооднозначного соответствия между разложениями многочленов $f$ и $\hat{f}$. Например, многочлен $x_2^2$ раскладывается на два множителя одним способом. При $d=3$ его образ есть $x^6$ у которого 3 различных разложения.

Это очень неэффективный алгоритм разложения многочлена на множители. Он был предложен Кронекером ещё в 19-ом веке. В настоящее время известен полиномиальный алгоритм решения этой задачи (см. \href{http://www.math.leidenuniv.nl/%7Ehwl/PUBLICATIONS/1982f/art.pdf}{LLL-алгоритм}) \cite{LLL}. Обсудим, как этот алгоритм работает.

Для проверки неприводимости мы с успехом использовали информацию, полученную из разложения многочлена по модулю $n$. Вопрос -- нельзя ли эту же информацию использовать и в задаче разложения на множители? Оказывается можно.

Во-первых, если взять достаточно большой модуль $n$, заметно больший, чем коэффициенты в целочисленном разложении, то разложение $f$ по модулю $n$ с маленькими коэффициентами однозначно будет определять кандидата на целочисленное разложение. Это соображение встречается сразу с двумя проблемами -- первая -- не ясно какие есть ограничения на коэффициенты сомножителей, вторая -- разложений по модулю $n$ может быть много и нам не известно способа эффективно искать их.



Продвинемся в решении второго вопроса. Как же  выбрать достаточно большое число, по модулю которого удобно раскладывать многочлен $f$ на множители? Наибольшим удобством в решении задачи разложения обладают поля. В этом смысле возможно стоило бы искать разложение $f$ по модулю очень большого простого. Однако найти большое простое число довольно тяжело. Смотреть по модулю маленьких простых а потом пытаться склеивать разложение в духе китайской теоремы об остатках может банально не получиться (как в примере 3 -- неясно во что склеить два разных разложения). Оказывается наиболее оптимальный вариант такой: взять небольшое простое число $p$, разложить $f$ над $\Z/p$ а затем <<поднять>> это разложение по модулю $p^k$ для достаточно большого $k$. Как находить разложение многочлена $f$ на неприводимые над $\Z/p$ мы обсудим позже. А пока поясним как делать подъём такого разложения по модулю $p^k$.

\lm[Гензеля] Пусть $f \in \Z[x]$, со старшим коэффициентом 1. Пусть $\ovl{f}=gh$ в кольце $\Z/p[x]$, причём $(g,h)=1$ и у $g,h$ тоже единичные старшие коэффициенты. Тогда  для любого $k\geq 1$ существуют единственные по модулю $p^k$ многочлены $\hat{g}, \hat{h} \in \Z[x]$, cо старшим коэффициентом $1$, что
$$\ovl{f}=\hat{g} \hat{h} \mod p^k,\quad  \hat{g}\equiv g \mod{p}, \quad \hat{h}\equiv h \mod{p} \quad \text{и} \quad \deg h= \deg \hat{h}, \quad \deg g= \deg \hat{g}.$$
\proof Докажем это индукцией по $k$. Пусть по модулю $p^{k}$ уже построены подходящие многочлены $\hat{h}$ и $\hat{g}$ и мы хотим построить $\ovl{h}$ и $\ovl{g}$. Заметим, что благодаря единственности по модулю $p^k$, такие $\ovl{g}$ и $\ovl{h}$ обязаны совпадать с $\hat{h}$ и $\hat{g}$ по модулю $p^k$. Это означает, что по модулю $p^{k+1}$ 
$$\ovl{h} \equiv \hat{h}+p^ka(x)\pmod{p^{k+1}} \text{\,\, и\quad } \ovl{g} \equiv \hat{g} + p^kb(x)\pmod{p^{k+1}}.$$
Заметим, что многочлены $a(x)$ и $b(x)$  однозначно определяются по модулю $p$,  если известны $\ovl{g}$ и $\ovl{h}$ и по модулю $p$ должны иметь степени меньше чем степени $h(x)$ и $g(x)$ соответственно, для того, чтобы не изменить старший коэффициент. Покажем, что такие $a(x), b(x)$ существуют и единственны по модулю $p$. Заметим, что необходимо проверить лишь условие $f \equiv \ovl{g}\ovl{h} \pmod{p^{k+1}}$. Распишем
$$f\equiv \hat{g}\hat{h} + p^{k}(a(x)g + b(x)h) \pmod{p^{k+1}}.$$
Здесь мы заменили $\hat{h}$ и $\hat{g}$ по модулю $p$ и получили исходные многочлены $g$ и $h$ из $\Z/p[x]$. Заметим, что есть единственный такой многочлен $c(x)\in\Z/p[x]$, что $f-\hat{g}\hat{h}=p^kc(x) \pmod{p^{k+1}}$. Теперь для выполнения сравнения выше необходимо, чтобы  $$c(x)=a(x)g(x)+b(x)h(x)$$
У такого сравнения есть единственное решение в $\Z/p[x]$ при условии $\deg a(x)<\deg h(x)$ и $\deg b(x)< \deg g(x)$. Что и требовалось.
\endproof
\elm

Частным случаем разложения на множители служит разложение вида $f(x)=(x-x_1)g(x)$, соответствующее наличию корня. Именно его мы и разбирали раньше. 

\rm Стоит отметить, что условие на старший коэффициент $f$ не является обременительным, так как его легко изменить при помощи линейной замены переменных над $\Q$ и домножения на подходящую константу.
\erm

\zd Покажите, что не существует критерия неприводимости, который бы зависел только от остатков коэффициентов $f(x)$ по модулю фиксированной степени простого $p^k$ и работал бы для многочленов $f(x)$, у которых есть два различных неприводимых множителя по модулю $p$ 
\ezd


\section{Дополнительно: оценка на коэффициенты делителя}

Заведём  на пространстве многочленов скалярное произведение
$$\lan f(x), g(x)\ran = \sum a_ib_i,$$
где $a_i$ и $b_i$ -- коэффициенты $f$ и $g$. Теперь докажем лемму:

\lm Пусть $f\in \C[x]$ имеет вид $f=(x-\alpha)h$. Тогда $|f|=|(\alpha x-1)h|$.
\proof
Обозначим коэффициенты $h$ за $c_0,\dots, c_{n-1}$. Тогда 
$$|f|^2= |-c_0\alpha|^2+|c_0-c_1\alpha|^2+\dots+ |c_{n-1}|^2.$$
С другой стороны имеем
$$|(\alpha x-1)h|^2=|c_0|^2+|\alpha c_0-c_1|^2+\dots + |\alpha c_{n-1}|^2.$$
Распишем отдельно модуль $|\alpha c_i - c_{i+1}|^2$ и $|c_i-\alpha c_{i+1}|^2$:
$$|\alpha c_i - c_{i+1}|^2= |\alpha c_i|^2 + |c_{i+1}|^2 - 2\re \alpha c_i \ovl{c_{i+1}} \text{ и } |c_i-\alpha c_{i+1}|^2= |\alpha c_{i+1}|^2 + |c_{i}|^2 - 2\re \alpha c_i \ovl{c_{i+1}}.$$
Осталось заметить, что все слагаемые вида $|\alpha c_i|^2$, $|c_i|^2$ и $-2\re \alpha c_i \ovl{c_{i+1}}$ встречаются в первой и второй сумме ровно по одному разу.
\endproof
\elm

Сформулируем теорему:

\thrm
Пусть $f=a_0 + a_1x+\dots + a_n x^n \in \Z[x]$. Тогда для коэффициентов целочисленного делителя $f$ степени $m$ имеет место оценка
$$|b_j| \leq C_{m-1}^j |f| + C_{m-1}^{j-1}|a_n|.$$
\proof Пусть $g(x)$ -- некоторый многочлен. Имеем разложение $g(x)=b_m \prod_{i=1}^m (x-\alpha_i)$. Тогда рассмотрим многочлен 
$$h(x)=\prod_{|\alpha_i|\geq 1} (x-\alpha_i) \prod_{|\alpha_i|<1}(\alpha_ix-1).$$
Мы знаем по предыдущей лемме, что $|g|=|h|$. Определим величины
$$M(g)=\prod_{|\alpha_i|\geq 1} |\alpha_i|, \,\,\, m(g)=\prod_{|\alpha_i|<1}|\alpha_i|.$$
Тогда имеет место
$$|g|^2=|h|^2\geq |b_m|^2 (M(g)^2+m(g)^2).$$
Для этого заметим, что старший коэффициент $h$ по модулю есть $|b_n|m(g)$, а младший -- $|b_n|M(g)$, что и даёт неравенство. В частности применяя это к многочлену $f$ получаем, что
$$M(f) \leq \frac{|f|}{a_m},$$
То есть мы оценили некоторое выражение от корней многочлена $f$. Вернёмся пока к произвольному многочлену $g$. Имеем следующую оценку на его коэффициент:
$$|b_j| = |b_m| \left|\sum \alpha_{i_1}\dots \alpha_{i_{m-j}}\right| \leq |b_m| \left|\sum \beta_{i_1}\dots \beta_{i_{m-j}}\right|, $$
где $\beta=\max(1,|\alpha|)$.

Покажем лемму

\lm Пусть есть набор вещественных чисел $x_1,\dots,x_m\geq 1$, что $x_1\dots x_m=M$. Тогда для всех $0<j\leq n$ имеем $$\sigma_j(x_1,\dots,x_m) \leq C_{m-1}^j M+ C_{m-1}^{j-1},$$
где $\sigma_i(x_1,\dots,x_n)$ -- элементарный симметрический многочлен.
\proof Покажем, что максимум модуля $\sigma_j$ достигается, если $x_1=\dots=x_{m-1}=1$ и $x_m=M$. Рассмотрим, например, пару $x_1,x_2$. Если $x_1,x_2\neq 1$, то заменим её на пару $1,x_1x_2$. Покажем, что при этом значение $\sigma_j$ увеличилось. Действительно, в новой сумме изменились только слагаемые в которые входили только $x_1$ или только $x_2$. Выпишем их 
$$\sum_{2<i_2 \dots< i_j} x_1 x_{i_2}\dots x_{i_j}  + \sum_{2<i_2\dots} x_2 x_{i_2}\dots x_{i_j}= (x_1+x_2)\sigma_{j-2}(x_3,\dots,x_m)$$
и сделаем в них замену $x_1=1$ и $x_2=x_1x_2$. Получим
$$(1+x_1x_2)\sigma_{j-2}(x_3,\dots,x_m).$$
Осталось заметить, что $0< x_1x_2 -x_1 -x_2 + 1= (x_1-1)(x_2-1)$ в наших предположениях. Аналогично для других пар переменных.
Теперь 
$$\sigma_j(1,\dots,1,M)= C_{m-1}^{j-1}M + C_{m-1}^j.$$
\endproof
\elm

\noindent Заметим теперь, что произведение $\beta_i = M(g)$. Тогда по лемме
$$|b_j|\leq |b_m| (C_{m-1}^{m-j-1}M(g) + C_{m-1}^{m-j})=|b_m| (C_{m-1}^{j}M(g) + C_{m-1}^{j-1}).$$
Теперь пусть $f \di g $ в $\Z[x]$. Тогда $a_n \di b_m$ и $M(g)\leq M(f)$ так как корни $g$ являются корнями $f$. Отсюда
$$|b_j|\leq |b_m| (C_{m-1}^{j}M(g) + C_{m-1}^{j-1})\leq |a_n|(C_{m-1}^{j}M(f) + C_{m-1}^{j-1})\leq C_{m-1}^{j}|f| + C_{m-1}^{j-1}|a_n|.$$

\endproof
\ethrm



Это теорема показывает, что размер записи коэффициентов делителя полиномиально зависит от размера записи многочлена $f$.







\section{Результант и дискриминант}

Попробуем решить следующую задачу: как определить по коэффициентам многочлена, что они имеют общий множитель? Если имеет место нетривиальное разложение $f=hk_1$, $g=hk_2$, то степень $k_1$ меньше $\deg f=n$, а степень $k_2\leq \deg g=m$. Заметим, что тогда $fk_2-gk_1=0$. Обратно, если найдены такие $k_1$ и $k_2$, то у $f$ и $g$ есть общий множитель. Рассмотрим отображение $K[x]_{\leq m-1}\times K[x]_{\leq n-1} \to K[x]_{\leq n+m-1}$, заданное по правилу
$$(a(x),b(x)) \to a(x)f(x)+b(x)g(x).$$ 
Когда это отображение вырождено? Тогда и только тогда, когда есть многочлены маленьких степеней, что $a(x)f(x)=-b(x)g(x)$. Это происходит тогда и только тогда, когда у многочленов $f$ и $g$ есть общий множитель. С другой это происходит, если определитель матрицы этого линейного отображения в стандартных базисах обнуляется. Транспонирую эту матрицу и переставляя строки и столбцы приходим к определению:

\dfn Пусть многочлен $f(x)=a_0+\dots+a_nx^n$, а $g(x)=b_0+\dots+b_mx^m$. Тогда результантом многочленов $f$ и $g$ называется $$Res(f,g)=  \det 
\begin{pmatrix}
a_n & a_{n-1} & \cdots & a_0 & 0 & \cdots & 0 \\
0 & a_n & a_{n-1} & \cdots & a_0 & \cdots & 0 \\
\\
0 & \cdots &  a_n & a_{n-1} & a_{n-2} & \cdots &  a_0 \\
b_m & \cdots & b_1 & b_0 & 0 & \cdots & 0 \\
 \\
0 & \cdots & 0 & b_m & \cdots & b_1 & b_0 
\end{pmatrix}.$$
Эта матрица называется матрицей Сильвестра. 
\edfn

Результант двух многочленов может быть определён над любым кольцом. Посмотрим, что из этого можно вытащить. Что значит равенство нулю результанта по $y$ для двух многочленов $f(x,y) $ и $g(x,y)$ из $K[x,y]$? Их результант это многочлен $h(x)$. Рассмотрим точку $x_0$. Допустим, что старшие коэффициенты $f$ и $g$ не обращаются в 0 в точке $x_0$. Тогда равенство нулю результанта $h(x_0)$ -- это равенство нулю результанта многочленов $f(x_0,y)$ и $g(x_0,y)$, что означает, что у последних есть общий корень $y_0$. То есть у системы $f=g=0$ есть корень $(x_0,y_0)$. Таким образом корни результанта -- это  просто $x$-координаты решений системы, или точки, в которых старший коэффициент многочленов обращается в 0.


А что можно вывести из того факта, что у $Res(f,g)=n$ для двух многочленов из $\Z[x]$? Разложим $n$ на простые множители. Получим $n=p_1^{\alpha_1} \dots p_k^{\alpha_k}$. Допустим, что $p_i$ не делит старшие коэффициенты $f$ и $g$. Тогда $0=Res(f,g) \mod p_i $ есть $Res(\ovl{f}, \ovl{g})$ и следовательно по модулю $p_i$ у многочленов есть общий корень. Обратно, если у $f$ и $g$ есть общий корень по модулю $p_i$, то $Res(f,g)\di p_i$.

\dfn Дискриминантом многочлена $f=a_0+\dots +a_nx^n$ называется выражение 
$$D(f)=(-1)^{\frac{n(n-1)}{2}} a_n^{-1} Res(f,f').$$
Это полином от коэффициентов $f(x)$.
\edfn

\rm Над полем  дискриминант обнуляется тогда и только тогда, когда у многочлена есть есть общий делитель с его производной. Заметим, что в если у многочлена $f$ были кратные множители, то это условие выполнено. Это, как мы узнаем позднее, эти условия равносильны над любым конечным полем характеристики $p$ (это не всегда верно над бесконечным полем $K$). Правильная формулировка в этом случае такая: дискриминант многочлена над полем $K$ обнуляется тогда и только тогда, когда существует расширение поля $K$, в котором у этого многочлена есть кратные множители.  
\erm

Дискриминант даёт ответ на вопрос, когда многочлен не имеет кратных корней по модулю $p$. Действительно это происходит только тогда, когда $D(\ovl{f})=0$ (как мы скоро узнаем). Это происходит тогда и только тогда, когда $D(f)\ndi p$ или, если $a_n\di p$. Заметим, что это условие может нарушаться только в конечном числе $p$, если $D(f)\neq 0$. Таким образом, либо у целочисленного многочлена есть кратный множитель над $\Q$, либо у него нет кратных множителей по модулю почти всех простых $p$. Это обосновывает, что для применения леммы Гензеля для подъёма всегда можно выбрать подходящее простое, если сам многочлен $f\in \Z[x]$ был бесквадратный.


\subsection{Дополнительно: свойства результанта и дискриминанта}

\thrm Пусть многочлен $f(x)=a_0+\dots+a_nx^n$, а $g(x)=b_0+\dots+b_mx^m$ из кольца $K[x]$, где $K$ -- поле. Пусть так же в поле $K$ имеются разложения $f(x)=a_n\prod(x-x_i)$, а $g(x)=b_m\prod (x-y_j)$. Тогда
$$Res(f,g)=a_n^mb_m^n \prod_{i,j} (x_i-y_j),$$
\ethrm

\lm Пусть так же в поле $K$ имеются разложения $f(x)=a_n\prod(x-x_i)$, а $g(x)=b_m\prod (x-y_j)$. Тогда результант можно найти по формуле:
$$a_n^mb_m^n \prod_{i,j} (x_i-y_j)=(-1)^{mn}b_m^n \prod f(y_j)=a_n^m \prod g(x_i).$$ 
\elm

\upr Кроме того, если $f=gq+r$, где $\deg r=k$, то 
 $$Res(f,g)=(-1)^{(n-k)m}b_m^{n-k} Res(r,g).$$
\eupr

\lm Пусть $f(x)\in K[x]$ -- это многочлен вида $f(x)=a_n(x-x_1)\dots(x-x_n)$. Тогда дискриминант можно найти по формуле:
$$D(f)=a_n^{2n-2}\prod_{i< j} (x_i-x_j)^2.$$
\elm

\exm\\
1) $D(x^2+ax+b)=a^2-4b$.\\
2) $D(x^3+ax+b)=-4a^3-27b^2$.\\




\chapter{Конечные поля}

\section{Общие факты теории полей}

Мы с вами уже встречались с понятием расширения полей, то есть ситуацией, когда одно поле $K$ является подкольцом в поле $L$. В этой ситуации $L$ автоматически является векторным пространством над $K$ и более того $K$-алгеброй. Тогда у $L$ можно посчитать размерность над $K$, а у любого элемента $\alpha \in L$ можно искать минимальный многочлен. Вокруг этих двух понятий и будет идти разговор в этом разделе.  


\dfn[Степень расширения] Пусть $L$ расширение поля $K$ (в этой ситуации часто пишут $L/K$). Тогда $\dim_K L$ называется степенью $L$ над $K$ и обозначается как $[L:K]$. Если $[L: K]$ конечно, то говорят, что $L$ -- конечное расширение поля $K$. 
\edfn

\thrm[О башне полей] Пусть дана башня расширений $K\leq L \leq M$. Тогда 
$$[M: K]=[M: L][L: K].$$
В частности, если $M$ конечно над $L$, а $L$ конечно над $K$, то $M$ конечно над $K$.


\proof Пусть $u_1\dots,u_n$ -- базис $L/K$, а $v_1,\dots,v_m$ -- базис $M/L$. Утверждается, что набор $u_iv_j$ базис $M/K$.
Прежде всего заметим, что это порождающая система. Действительно, любой элемент $y\in M$ равен 
$$y=\sum_{j=1}^m \lambda_j u_j. $$
Здесь $\lambda_j \in L$, что значит, что для них есть разложение $\lambda_j=\sum_{i=1}^n \mu_{ij} u_i$, где $\mu_{ij}\in K$. Подставляя, получаем 
$$ y=\sum_{j=1}^m\sum_{i=1}^n \mu_{ij} u_jv_i.$$
Покажем теперь независимость этого набора. Пусть
$$\sum_{j=1}^m\left(\sum_{i=1}^n \mu_{ij}u_i\right)v_j=\sum_{ij}\mu_{ij} u_i v_j=0.$$
Тогда, так как набор $v_j$ образует базис $M/L$, то необходимо
$$\sum_{i=1}^n \mu_{ij}u_i=0 \text{ для всех j}.$$
Но так как $u_i$ -- базис $L/K$, получаем, что $\mu_{ij}=0$, что и требовалось.
\endproof
 
\ethrm

\crl Пусть $[L: K]$ -- конечное расширение степени $n$, а $[M:K]$ -- степени $d$. Тогда, если $d \ndi n$, то $M$ не может быть подрасширением $L/K$.
\ecrl

\rm Тут стоит быть осторожным, потому что одно и тоже поле может быть по разному реализовано как расширение базового. Рассмотрим простой пример: $\Q(x)$ содержит в качестве подполя $\Q(x^2)$, которое само изоморфно $\Q(x)$. В этой ситуации $\Q(x)$ реализовано как расширение самого себя двумя разными способами.
\erm

\dfn Пусть $L/K$ расширение полей, а $\alpha \in L$. Тогда наименьшее подрасширение $L$, содержащее $\alpha$, будем обозначать $K(\alpha)$, а наименьшую подалгебру, содержащую $\alpha$, будем обозначать $K[\alpha]$. Если есть несколько элементов $\alpha_1,\dots,\alpha_n\in L$, то аналогичные объекты будем обозначать как $K[\alpha_1,\dots,\alpha_n]$ и $K(\alpha_1,\dots,\alpha_n)$.
\edfn



Может показаться, что такое обозначение не естественно, потому что никак не включает в себя объемлющее поле $L$. Я покажу, что это не так страшно. Для этого нам понадобится определение:

\dfn Элемент $\alpha \in L$ называется алгебраическим над $K$, если существует многочлен $0\neq p(x)\in K[x]$, что $p(\alpha)=0$. Не алгебраические элементы называются трансцендентными.
\edfn 

Например, элемент $\sqrt{2} \in \R$ является алгебраическим над $\Q$, а элемент $\pi\in \R$ является трансцендентным.

Нам понадобится лемма:

\lm Пусть $f(x) \in K[x]$, а $L$ -- какая-то алгебра над $K$. Тогда задание  гомоморфизма $K$-алгебр $K[x]/f \to L$ равносильно нахождению элемента $\alpha \in L$, что $f(\alpha)=0$.
\elm
\proof Класс $\ovl{x}$ является корнем $f(x)$ в алгебре $K[x]/f$. При гомоморфизме он должен перейти в корень $f$ в $L$. Так как алгебра $K[x]/f$ порождена $\ovl{x}$, образ $\ovl{x}$ однозначно задаёт гомоморфизм. 
Осталось показать, что если найдётся подходящее $\alpha\in L$, то гомоморфизм существует. Для этого сначала построим гомоморфизм $K[x] \to L$ по правилу $h(x) \to h(\alpha)$. А потом заметим, что он продолжается на фактор. Заметим, что всё сказанное работает и при $f=0$.
\endproof


\thrm Пусть $L/K$ расширение полей, а $\alpha \in L$. Тогда если $\alpha$ алгебраический над $K$, то 
$$K(\alpha)=K[\alpha]\cong K[x]/p(x),$$
 где $p(x)$ минимальный многочлен для $\alpha.$
Если же $\alpha$ трансцендентное над $K$, то $$K[\alpha]\cong K[x] \text{ и } K(\alpha) \cong K(x).$$
\ethrm


\proof Итак, пусть $\alpha$ -- алгебраический над $ K$. Тогда минимальный многочлен $\alpha$ однозначно определён. Покажем, что он неприводим. Пусть $p(x)=h(x)q(x)$. Тогда $h(\alpha)q(\alpha)=0$. Но $L$ -- поле. Откуда либо $h(\alpha)=0$ либо $q(\alpha)=0$. Но тогда $p(x)$ не минимальный.

Существует единственный гомоморфизм $\ffi \colon K[x]/p(x) \to L$ переводящий $x \to \alpha$.  Это даёт гомоморфизм $K[x]/p(x) \to L$. Он инъективен, так как $K[x/p(x)$ -- поле. Значит  образ $K[x]/p(x)$ тоже поле. Оно состоит из линейных комбинаций $1,\alpha,\dots,\alpha^{n-1}$, где $n$ -- степень $p(x)$. Понятно, что любая подалгебра, содержащая $\alpha$ содержит такие элементы. Значит образ -- это наименьшая подалгебра содержащая $\alpha$. Отсюда
$$K[x]/p(x) \cong \im \ffi = K[\alpha]=K(\alpha).$$
Пусть $\alpha$ не алгебраический, то есть трансцендентный. Тогда отображение $K[x] \to L$ переводящее $x\to\alpha$ инъективно. Ясно, что его образ это $K[\alpha]$. Далее заметим, что это отображение продолжается до отображения $K(x) \to L$, потому что образы всех элементов $K[x]$, кроме 0 в $L$ обратимы. Ясно, что образ этого отображения есть подполе и изоморфен $K(x)$. С другой стороны, меньше чем образ этого отображения быть ничего не может. Тогда образ и есть $K(\alpha)$. 
\endproof

\crl Пусть $\alpha$ -- алгебраическое. Тогда $ [K[\alpha]:K]= [K[x]/p(x):K]= \deg p(x)$, где $p(x)$ -- минимальный многочлен $\alpha$.
\ecrl

\crl Все расширения, порождённые над $K$ корнем одного и того же неприводимого многочлена изоморфны. Часто я буду говорить, про расширение $K[\alpha]$, где $\alpha$ корень многочлена $p(x)$. Это корректно, так как такое расширение определено однозначно с точностью до изоморфизма.
\ecrl

Так же напомню вам факт, известный ещё с прошлого года:

\utv Пусть $K$ -- поле, $f(x)$ -- многочлен над $K$. Тогда существует такое конечное расширение $L/K$, что в $L$ многочлен $f$ раскладывается на линейные множители.
\eutv
\proof Индукция по степени многочлена. Рассмотрим многочлен $f$. Пусть у $f$ есть неприводимый множитель $g$. Построим расширение $L=K[x]/g$. В этом расширении у $g$, а значит и у $f$ есть корень $\alpha$. Посмотрим на многочлен $f_1=f/(x-\alpha) \in L[x]$. По индукции построим для $L$ расширение $M$ в котором $f_1$ раскладывается на линейные множители. Но тогда в $M$ и многочлен $f$ раскладывается на линейные множители.
\endproof

На самом деле построенное при помощи этой конструкции расширение есть <<наименьшее>> расширение, которое содержит все корни многочлена и можно показать, что оно единственно с точностью до изоморфизма. Такое расширение называется полем разложения многочлена.


\subsection{Дополнительно: построение при помощи циркуля и линейки}
Попробуем применить это следствие для доказательства невозможности определённых  построений, например при помощи циркуля и линейки. Напомню, что при построении циркулем и линейкой можно поставить пару начальных точек (задать масштаб), соединять две построенные  точки прямой и строить окружность с центром в построенной точке и с расстоянием, равным расстоянию между уже построенными двумя точками. Точка построена, если она есть точка пересечения построенных прямых и окружностей. 

Вещественное число $x$ называется построимым, если, стартуя с точек $(0,0)$ и $(1,0)$, можно построить отрезок $(x,0)$. 

\thrm Если вещественное число $x$ построимо, то оно алгебраическое и лежит в расширении $L/\Q$ степени $2^m$.
\proof Доказательство идёт индукцией по числу построений. Пусть уже построены прямые $l_i$ и окружности $O_j$. Заметим, что коэффициенты в уравнениях $O_i$ и $l_j$ по индукционному предположению лежат в подполе $L\subseteq \R$ степени $2^m$ над $\Q$. Это же касается и новой прямой $l$ (или окружности $O$). Посмотрим на пересечение окружности $O_j$ и новой прямой $l$. Оно задано системой уравнений $(x-a)^2+(y-b)^2=r^2$ и $cx+dy=f$. Пусть $c\neq 0$. Тогда первое уравнение переписывается в виде $$(f-dy)^2+c^2(y-b)^2=c^2r^2$$
Его коэффициенты из $L$, а решение $y$ лежит либо в $L$ либо в расширении степени 2 над $L$. Случай пересечения двух окружностей сводится к пересечению окружности и прямой.  Действительно, если написаны два уравнения окружности c разными центрами
$$(x-a_1)^2+(y-a_2)^2=r^2_1 \text{ и } (x-b_1)^2+(y-b_2)^2=r^2_2$$
то вычитая их получим уравнение прямой
$$2(a_1-b_1)x+2(a_2-b_2)y=r^2_2-r^2_1+a_1^2-b_1^2+a_2^2-b_2^2.$$

\endproof
\ethrm

\crl
Нельзя разбить произвольный угол на три части при помощи циркуля и линейки.
\proof Например, угол $\frac{\pi}{3}$ нельзя, потому что угол $\frac\pi 9$ не построить. Действительно, построимость угла и его косинуса равносильны. Косинус $\frac{\pi}{9}$ удовлетворяет уравнению $4x^3-3x=\frac{1}{2}$. Это неприводимый над $\Q$ многочлен степени 3 и его корни не могут лежать в расширении степени $2^m$. Следовательно, построение невозможно. 
\endproof
\ecrl

\section{Строение конечных полей}

В качестве затравки мы ограничим возможное число элементов в конечном поле.

\lm Пусть $K$ --- поле, $p=\chr K$ --- простое число. Тогда в $K$ есть подполе изоморфное $\Z/p$. Если к тому же $K$ --- конечное, то число элементов $|K|=p^n$ для некоторого натурального $n$. 
\elm


\proof Рассмотрим гомоморфизм $f\colon \Z \to K$. Ядро этого отображения это $p\Z$. Тогда $\im f\cong \Z/p$. Пусть теперь $K$ конечно. Тогда $K$ -- конечномерное векторное пространство над $\Z/p$ и в нём $p^n$ элементов.
\endproof
 
Теперь сформулируем основной результат про конечные поля, который мы и будем в дальнейшем доказывать.

\thrm Существует и единственно (с точностью до изоморфизма) поле из $p^n$ элементов. Такое поле будем обозначать $\F_{p^n}$.
\ethrm

Начнём с леммы:
\lm Пусть $K$ поле из $p^n$ элементов. Тогда все элементы $K$ удовлетворяют уравнению $x^{p^n}=x$.
\elm
\proof Группа $K^*$ состоит из $p^n-1$ элементов. Тогда все элементы из $K^*$ удовлетворяют уравнению $x^{p^n-1}-1=0$. Домножая на $x$ добавляем неприкаянный 0.
\endproof


\lm Пусть $L$ --- кольцо характеристики $p$, где $p$ -- простое. Тогда отображение $x\to x^{p}$ является эндоморфизмом $L$. Это отображение называется эндоморфизмом Фробениуса. Если $L$ --- конечное поле, то эндоморфизм Фробениуса является автоморфизмом. 
\elm
\proof Очевидно произведение переходит в произведение. Для суммы имеем $$(x+y)^p=\sum_{i+j=p} C_p^i x^iy^j= x^p+y^p,$$ т.к. все промежуточные биномиальные коэффициенты делятся на $p$. Пусть теперь $L$ -- поле. Тогда заметим, что отображение $\Frob$ инъективно, так как в поле не бывает нильпотентов и, следовательно, по принципу Дирихле, биективно.
\endproof

\rm Условие конечности поля в этой ситуации важно. Рассмотрим, например, поле $\Z/p(x)$. Тогда возведение в $p$-ую степень в качестве образа имеет $\Z/p(x^p)$.
\erm

\lm Пусть $L$ --- поле характеристики $p$. Тогда множество элементов из $L$ удовлетворяющих уравнению $x^{p^n}=x$ образует подполе в $L$.
\elm
\proof Обозначим рассматриваемое множество за $K$. Тогда $0,1\in K$. Очевидно, что $K$ замкнуто относительно умножения. Замкнутость относительно сложения следует из того, что $x^{p^n}$ есть композиция $n$ раз эндоморфизма Фробениуса. Значит $K$ --- подкольцо. Обратный к $x\neq 0$ имеет вид $x^{p^n-2}$, что следует из уравнения.
\endproof
 

\proof[{\color{red!80!black} Доказательство теоремы. Существование}]
Рассмотрим поле $\F_p=\Z/p$ и $x^{p^n}-x$ --- многочлен над ним. Тогда есть поле $L$ в котором   $x^{p^n}-x$ раскладывается на линейные множители. Рассмотрим $K$ --- подполе в $L$ состоящее из элементов удовлетворяющих уравнению $x^{p^n}=x$. В $K$ ровно $p^n$ элементов т.к. многочлен $x^{p^n}-x$ не имеет кратных корней.

\proof[{\color{red!80!black} Доказательство теоремы. Единственность}]
Пусть есть два поля $K$ и $L$ из $p^n$ элементов. Рассмотрим их мультипликативные группы. Они циклические порядка $p^n-1$. Пусть группа $K^*$ порождена элементом $\xi$. Тогда любой элемент заведомо является многочленом от $\xi$. Пусть $f$ -- минимальный многочлен $\xi$. Значит $K\cong\F_p[x]/f(x)$. Многочлен $f$ неприводим. $\xi$ --- его корень. Многочлен $x^{p^n}-x$ делится на $f$, так как $\xi$ есть корень $x^{p^n}-x$, а $f$ -- минимальный многочлен.

Тогда у $f$ есть корни в любом поле из $p^n$ элементов, в частности в $L$. Тогда у нас есть гомоморфизм  $K\cong\F_p[x]/f(x)\to L$ переводящий $\xi$ в некоторый корень $f$ в $L$. Этот гомоморфизм инъективен, так как $K$ -- поле и, по принципу Дирихле, является биекцией. 
\endproof
 
\rm В частности, мы увидели, что любое конечное поле $\F_{p^n}$ может быть построено как $\F_p[x]/f(x)$, где $f$ -- неприводимый многочлен степени $n$.
\erm

\noindent{\bf Пример:}\\
Построим поле из $4=2^2$ элементов. Для этого нам нужно найти неприводимый многочлен степени $2$ над $\F_2$. Это $x^2+x+1$. Тогда построим поле из четырёх элементов как
$$\F_4= \F_2[x]/x^2+x+1.$$

Когда одно конечное поле может быть подполем в другом и сколько может быть вариантов для выбора такого подполя?

\thrm Поле $\F_{p^n}$ подполе $\F_{p^m}$ тогда и только тогда, когда $m\di n$. Такое подполе единственно.
\ethrm 
\proof
Если $\F_{p^n}$ подполе $\F_{p^m}$, то сравнивая степени расширения получаем, что $m \di n$. Обратно, возьмём в $\F_{p^m}$ подполе $\{x \in \F_{p^m} \,|\, x^{p^n}-x=0\}$. Очевидно, что любое подполе из $p^n$ элементов там содержится. Это даёт единственность. 


Для того, чтобы доказать существование, покажем, что в указанном подполе $p^n$ элементов. Для этого заметим, что многочлен $x^{p^m}-x \di x^{p^n}-x$, если $m \di n$. Первый многочлен раскладывается на линейные множители над $\F_p$, откуда аналогичное свойство выполнено для второго многочлена. То есть у многочлена $x^{p^n}-x$ есть все $p^n$ корней в $\F_{p^m}$. Что и требовалось. 
\endproof
 



\crl Элемент $\alpha\in \F_{p^m}$ лежит в подполе из $p^n$ элементов тогда и только тогда, когда $\alpha^{p^n}-\alpha=0$.
\ecrl

Так же, если внимательно посмотреть на доказательство основной теоремы, то можно увидеть ещё несколько закономерностей.

\utv Пусть $f(x) \in \F_p[x]$ -- неприводимый многочлен степени $n$. Многочлен $f(x)$ раскладывается над $\F_{p^n}$ на линейные множители. Кроме того $x^{p^m}-x \di f(x)$ тогда и только тогда, когда $m\di n$. Кроме того, если $\alpha$ --  корень $f$, то все остальные его корни имеют вид $\alpha^{p^k}$ при $0\leq k<n$. 
\eutv
\proof Рассмотрим многочлен $f$ и поле $\F_p[x]/f$ из $p^n$ элементов. Корень $f$ в этом поле удовлетворяет уравнению $x^{p^n}-x=0$ и потому  $x^{p^m}-x \di f$. Тогда $f$ раскладывается в $\F_{p^n}$ на линейные множители. 

Если $x^{p^m}-x \di f$, то все корни $f$, которые порождают поле из $p^n$ элементов, лежат в $\F_{p^m}$. Но тогда $m \di n$. Обратно, если $m\di n$, то $x^{p^m}-x\di x^{p^n}-x\di f$.

Так как возведение в $p$-ую степень автоморфизмом поля $\F_{p^n}$ над $\F_p$, то если $\alpha$ корень $f$, то $\alpha^p$ тоже корень $f$. Почему таким образом получаются все корни? Для этого достаточно показать, что корни $\alpha$ и $\alpha^{p^k}$ не склеиваются при всех $k<n$. Но они не могут быть одинаковыми, так как тогда возведение в степень $p^k$ и тождественное отображение $\F_{p^n} \to \F_{p^n}$ совпадали бы (ведь $\F_{p^n}$ порождается $\alpha$), а это противоречит описанию всех автоморфизмов.
\endproof


Кроме того, посмотрим, как устроены все автоморфизмы конечных полей.

\utv Все автоморфизмы $\F_{p^n}$  имеют вид $\Frob_p^{\circ i}$, где $0\leq i \leq n-1$. Все указанные автоморфизмы различны.
\proof Обозначим $q=p^n$. Заметим, что поле $\F_q$ порождено одним элементом $\F_q =\F_p[\alpha]$. Минимальный многочлен $\alpha$ над $\F_p$ обозначим за $f$, его степень равна $n$. 

Теперь, гомоморфизмы $\F_p[\alpha]$ в себя определяются образами элемента $\alpha$, которые обязаны быть корнями того же многочлена $f$. Но таких корней в $\F_p[\alpha]$ не более $n$. Тогда и автоморфизмов не более $n$. Осталось показать, что мы нашли все $n$ возможных. Для этого надо показать, что автоморфизмы $\Frob_p^{\circ i}$ различны для всех $0\leq i\leq n-1$. Предположим, что для всех элементов из $\F_q$ выполнено, что $\Frob_p^{\circ l} - \Frob_p^{\circ k}=0$, для $k<l<n$. Но это уравнение степени меньше $p^n$ и оно не может иметь $p^n$ решений в поле $\F_q$. 
\endproof
 
\eutv


\subsection{Расширения конечных полей}

Говоря про конечные поля характеристики $p$, мы рассматриваем их как расширения поля $\F_p$. Но что происходит, с расширениями больших конечных полей $\F_q$, где $q=p^n$? Оказывается, что все те свойства расширений, которые мы сформулировали над $\F_p$, в  подходящем виде верны и над $\F_q$. Сформулируем их. 



\thrm Все конечные расширения поля $\F_q$, где $q=p^n$ имеют $q^m$ элементов. Два расширения $\F_q$ из $q^m$ элементов изоморфны между собой над $\F_q$. Внутри поля $\F_{q^m}$ есть подполе $\F_{q^l}$ только если $l|m$.
\proof
Если $L$ расширение $\F_q$, то оно имеет $q^{[L:\F_q]}$ элементов. Покажем существование. Возьмём поле из $q^m=p^{nm}$ элементов и рассмотрим в нём подполе $\F_q$ из $q=p^n$ элементов. Такое есть по предыдущей теореме. Так как все поля из $q$ элементов изоморфны, то мы можем отождествить  и даёт необходимое расширение. 

Покажем, что все расширения $\F_q$ из $q^m$ элементов изоморфны над $\F_q$. Применим ту же логику что и в ситуации над $\F_p$. А именно, рассмотрим некоторое расширение $K$ из $q^m$ элементов. Оно порождено одним элементом $\alpha$ над $\F_q$. Минимальный многочлен $f$ для $\alpha$ над $\F_q$ имеет общий корень с $x^{q^m}-x$ и, значит, делит его. Значит в любом другом расширении из $q^m$ элементов есть корень $f$. В него и надо отправить $\alpha$.

Теперь рассмотрим поле из $q^m$ элементов. Тогда в нём есть подполе из $q^l$ элементов только если $nm \di nl$. Но это происходит только если $m \di l$. Такое подполе единственно и автоматически снабжается структурой $\F_q$ расширения, так как содержит образ последнего при его вложении в $\F_{q^m}$.
\endproof
\ethrm

Теперь рассмотрим свойства неприводимых многочленов над конечным полем $\F_q$.

\crl Пусть $f(x)$ -- это неприводимый многочлен из $\F_q[x]$. Тогда $x^{q^m}-x \di f(x)$ тогда и только тогда, когда $\deg f(x) | m$. Если многочлен $f$ имеет корень $\alpha$ в $\F_q^m$, то он раскладывается над этим полем на линейные множители. Все корни $f$ в этой ситуации можно найти как $\alpha^{q^k}$. 
\proof Пусть $x^{q^m}-x$ делится на $f(x)$. Тогда в поле $\F_q^m$ многочлен $f(x)$ имеет корень $\alpha$ (на самом деле там лежат все его корни). Теперь $\F_q[\alpha]$ подполе $\F_{q^m}$. Но тогда $\deg f(x) = [\F_q[\alpha]: \F_q] \di m $. 

Обратно, пусть $k=\deg f(x) | m$. Тогда в $\F_q^m$ есть подполе $\F_{q^k}$. Но такое подполе изоморфно $\F_q[x]/f$ и имеет внутри корень $\alpha$ многочлена $f(x)$. Но тогда $f(x)$ и $x^{q^m}-x$ не взаимно просты, откуда следует, что $x^{q^m}-x \di f(x)$, благодаря неприводимости последнего. 
\endproof
\ecrl


\utv Все автоморфизмы $\F_{q^m}$ над $\F_q$ есть композиции отображения $x \to x^q$ с самим собой.
\eutv
\proof Для начала стоит заметить, что отображение $x\to x^q$ является $\F_q$-линейным и, значит, автоморфизмом над $\F_q$. Остальное повторяет рассуждения над $\F_p$.
\endproof

\section{Алгоритм Берлекэмпа}


Я опишу здесь некоторый набор соображений и алгоритмов касательно разложения многочленов на множители над конечным полем. 

Мы помним, что над полями характеристики 0 всегда легко выделить все  кратные множители многочлена просто взяв отношение $f$ и $\Nod(f,f')$. Однако, над конечными полями всё немного не так. Точнее

\lm Пусть $f= \prod g_i^{n_i}\in \F_q[x]$, где  $q=p^n$, $g_i$ неприводимы над $\F_q$. Тогда
$$\Nod(f,f')=\prod_{n_i \not\di p} g_i^{n_i-1} \prod_{n_i \di p}g_i^{n_i}.$$
\proof
Рассмотрим неприводимый множитель $g_i$. Пусть $f(x)=g_i(x)^{n_i}g(x)$. Продифференцируем. Имеем $f'(x)= n_ig_i'(x)g_i^{n_i-1}g(x)+ g_i^{n_i}g'(x)$. Если $n_i\di p$, то $f'(x)=g_i^{n_i}g'(x)$, что показывает, что степень вхождения $g_i$ в $f'(x)$ не менее $n_i$. Но в $f(x)$ многочлен $g_i$ входит с кратностью ровно $n_i$. А значит с такой кратностью он входит и в $\Nod(f,f')$.

Если же $n_i\ndi p$, то кратность вхождения $g_i(x)$ в $f'(x)$ равна $n_i-1$, что следует из следующей леммы.
\endproof
\elm

\lm Многочлен $h$ над конечным полем характеристики $p$ имеет нулевую производную тогда и только тогда, когда он является $p$-ой степенью. Извлечение степени можно провести эффективно.
\proof Как мы уже знаем с прошлого семестра, если $h'=0$, то $h=g(x^p)$. Посмотрим на коэффициенты $g$ -- $a_0, \dots, a_l\in \F_q$. Вспомним, что эндоморфизм Фробениуса $\Frob \colon \F_q \to  \F_q $ -- биекция. Иными словами из каждого элемента можно извлечь корень степени $p$. Пусть $b_i^p=a_i$. Тогда $f=b_0+b_1x+\dots+b_lx^l$ обладает свойством $f^p=g(x^p)=h$. Как вычислить корень степени $p$ из элемента? Для этого заметим, что обратное отображение к $\Frob \colon \F_q \to \F_q$ это $\Frob^{\circ n-1}$. 
\endproof
\elm 


Это позволяет свести задачу разложения произвольного многочлена над конечным полем к разложению на множители многочлена без кратных множителей. Действительно $\frac{f}{\Nod(f,f')}$ без кратных множителей. В свою очередь $\Nod(f,f')$ состоит из множителей двух типов -- чьи степени кратны $p$ и не кратны $p$. Первые встречаются как сомножители в  $\frac{f}{\Nod(f,f')}$ и легко находятся после получения его разложения. Из оставшихся множителей можно извлечь корень степени $p$ и перейти к разложению многочлена заведомо меньшей степени. 



Прежде чем мы перейдём к, собственно, к разложению на неприводимые множители. Для того, чтобы разложить многочлен на неприводимые множители достаточно научиться получать его нетривиальный делитель. Теперь вспомним факт про многочлены.

\fct[Китайская теорема об остатках] Пусть $K$ -- поле и в $K[x]$ выполнено равенство $f(x)=g(x)h(x)$, где $g,h$ взаимно простые многочлены. Тогда
$$K[x]/f \simeq K[x]/g \times K[x]/h.$$
\efct


\thrm[Алгоритмы Берлекэмпа] Пусть $f(x)\in \F_q$ без кратных множителей. Тогда существуют детерминированный полиномиальный по $n$ (но не по $\log q$)  алгоритм раскладывающий $f$ на множители. 


\proof Первое соображение, которое мы применим, будет состоять в том, что мы переформулируем  задачу факторизации многочлена $f$ как задачу про некоторое кольцо. Точнее, пусть $f=h_1\dots h_l$ разложение на неприводимые. Тогда по Китайской теореме об остатках 
$$R= \F_q[x]/f\cong \F_q[x]/h_1 \times \dots \times \F_q[x]/h_l.$$ 
Заметим, что нахождение нетривиального делителя нуля в $R$ равносильно нахождению делителя $f$. Заметим, что, в свою очередь, $\F_q[x]/h_i$ -- поле из $q^{\deg h_i}$ элементов. Делителем нуля является любой элемент с хотя бы одним нулём в компоненте.


В каждом таком поле есть единственное подполе из $q$ элементов, состоящее из решений уравнения $x^q-x=0$. Если рассмотреть множество решений этого уравнения в $R$, то оно будет состоять из $l$-ек покомпонентных решений. Иными словами множество решений уравнения $x^q-x$ в $R$ есть подалгебра $R'$, изоморфная $\F_q\times \dots \times \F_q$, взятое $l$ раз. Если мы найдём делитель нуля в этой подалгебре, то найдём и в исходной. Заметим, что удельно, делителей нуля в этой подалгебре больше чем в исходной. 

Как найти $R'$? Для этого надо найти все решения уравнения $x^q-x=0$ в $R$. Второе соображение состоит в том, что это уравнение линейно (над $\F_q$). Чтобы решить это линейное уравнение надо составить его матрицу. У отображения $x \to -x$ матрица $-E_n$, где $n=\deg f$. У оператора $x \to x^q$ матрица легко считается. Далее достаточно применить любой из методов для решения систем линейных уравнений. Заметим, что если алгебра $R'$ одномерна (она не менее чем одномерна, так как константа всегда решение), то многочлен $f$ неприводим.

Теперь мы нашли $R'$. Построим детерминированный алгоритм нахождения разложения. Напомню, что нам надо получить делитель нуля, то есть элемент, у которого хоть одна компонента равна 0, но сам он не ноль. Возьмём произвольный не константный элемент $h$ из $R'$.  Тогда $h$ соответствует $l$-ка $(a_1,\dots,a_{l})$.  Переберём все константы $c$ из $\F_q$. Их $q$ штук (это и даёт неполиномиальность алгоритма по $\log q$). Тогда $h-c$ для, например, $c=a_1$ есть нетривиальный делитель 0 (он не ноль, потому что $h$ не константа).

Делитель $f$ теперь можно найти как $\Nod(f,h-c)$.

\endproof
\ethrm

\rm Несмотря на то, что алгоритм Берлекэмпа является неполиномиальным по размеру входных данных он даёт полиномиальный способ проверки неприводимости многочлена. А именно, многочлен неприводим тогда и только тогда, когда размерность подалгебры $R'$ над $\F_q$ равна 1.

С другой стороны, во многих приложениях в качестве базового поля берётся $\F_2$ и проблема с неполиномиальностью по размеру поля отпадает.
\erm


\subsection{Дополнительно: алгоритм Кантора-Цассенхауза}

Рассмотрим полиномиальный, но вероятностный алгоритм нахождения делителей:

\thrm[Алгоритмы Кантора-Цассенхауза] Пусть $f(x)\in \F_q$ без кратных множителей, $q\neq 2^d$. Тогда существуют вероятностный полиномиальный по $n\log q$  алгоритм раскладывающий $f$ на множители.
\proof
Будем предполагать, что $f(x)$ имеет в качестве неприводимых делителей многочлены степени ровно $d$. Тогда алгебра $R$ имеет вид
$$R= \F_q[x]/f(x)\cong \F_{q^d}\times \dots \times \F_{q^d}.$$ 
Наша задача придумать вероятностный алгоритм находящий делитель нуля в таком произведении.

Для упрощения обозначений я заменю $q^d$ на $l$. Посмотрим отдельно на один сомножитель $\F_{l}$. Заметим, что любой элемент поля $\F_l$ удовлетворяет условию, что $x^{\frac{l-1}{2}}$ либо 0, либо 1, либо $-1$. Ноль реализуется только в случае $x=0$, а $1$ и $-1$, если $x$ квадрат и не квадрат соответственно. Из этого стоит пояснить, что, если $x$ не квадрат, то $x^{\frac{l-1}{2}}$ элемент порядка 2 (и следовательно равен $-1$). Действительно, любой элемент $\F_l^*$ есть степень первообразного корня $\alpha$. Тогда элемент квадрат  только если он есть $\alpha^{2d}$. В свою очередь, элемент $\alpha^{2d+1}$ не может быть тривиальным, потому что порядок $\alpha$ чётен. В частности, $(\alpha^{2k+1})^{\frac{l-1}{2}}$ с одной стороны имеет порядок либо 2, либо 1 и при этом не тривиален, то есть имеет порядок 2.


Возьмём теперь случайный элемент $a$ из $R$. Если $a$ делитель нуля всё и так хорошо. Это можно проверить взяв $\Nod(f,a)$, который заодно вычислит делитель $f$. Иначе с вероятностью больше чем  $\frac{1}{2}$ одна из компонент $a$ в одном из сомножителей $\F_l$ является квадратом, а ещё одна не является квадратом. Не умоляя общности пусть это первая и вторая компоненты. Тогда $a^{\frac{l-1}{2}}$ имеет вид $(1,-1,\dots)$ и $a-1$ является нетривиальным делителем нуля.  
\endproof
\ethrm

Этот алгоритм не работает для полей характеристики 2. Это не страшно, потому что над $\F_2$ итак хорошо работает алгоритм Берлекэмпа. А если вы хотите что-то над расширением $\F_2$ то есть подход, использующий не $x^{l-1/2}-1$, а другой многочлен (вычисляющий след). 

В алгоритме Кантора-Цассенхайза есть условие, что многочлен $f$ должен раскладываться на неприводимые множители одинаковой степени. Может показаться, что это обременительное условие, но на самом деле этого легко добиться, если заметить, что $(f,x^{q^d}-x)$ легко вычисляется и вспомнить, какие у этого многочлена могут быть неприводимые делители.


\section{Коды исправляющие ошибки}

Одно из применений конечные поля находят при решении задачи о кодах исправляющий ошибки. Постановка вопроса следующая: Алиса хочет передать Бобу сообщение, но канал связи может искажать часть передаваемого. Задача состоит в том, чтобы так закодировать сообщение, что, даже если оно было немного изменено по дороге, тем не менее, можно было бы восстановить исходный текст.

Дадим математическую постановку задачи. Пусть мы работаем с некоторым конечным алфавитом $\mc A$. Часто -- это двухэлементное множество $\{0,1\}$. Но если вы передаёте байты целиком и не лезете в их потроха, то ваш алфавит состоит из  $2^8$ символов.  Сообщение -- это строка фиксированной длины из элементов $\mc A$. Что означает, что при передаче сообщения возникает не более чем  сколько-то ошибок? Одна ошибка -- это изменение в одном символе. Не более $r$ ошибок -- изменение не более $r$ символов. Это удобно выражать на следующем языке:

\dfn[Расстояние Хемминга] Пусть дан некоторый алфавит $\mc A$ и два слова $x,y \in \mc A^n$. Тогда расстоянием Хемминга между ними называется число позиций, в которых эти слова различаются
$$d_H(x,y)=|\{i \in \ovl{1, n} \, |\, x_i \neq y_i \}|.$$
\edfn

Итак, у нас происходит не более чем $r$ ошибок, если сообщение до передачи $x$ находится от сообщения после передачи $y$ на расстоянии не более чем $e$.

Что такое кодирование? Пусть исходные сообщения, которые надо передать, были длины $k$. Никто не говорил, что передавать надо именно их. Вместо них мы можем передавать слова другой длины, или даже слова в другом алфавите. Впрочем, менять алфавит мы не собираемся, особенно, когда речь идёт о передаче электронных данных. 

А вот размер сообщения мы можем поменять. Зафиксируем $n\in \N$. Тогда кодированием (блочным) назовём инъективное  отображение $K\colon \mc A^k \to \mc A^n$.

Итак, мы кодируем сообщения длины $k$ в сообщения блины $n$. Что значит, что мы можем исправить $r$ ошибок? Пусть $x\in \mc A^k$. Если при передаче произошло не более $r$ ошибок, то переданное сообщение $y \in \mc A^n$ обладает тем свойством, что $d_H(K(x),y)\leq r$. Для того, чтобы мы могли однозначно восстановить по $y$ кодовое слово $K(x)$, должно выполняться свойство, что для всех других $z\in \mc A^k$ верно, что $d_H(K(z),y) > r$.

Если мы потребуем, чтобы восстановление было возможно всегда, то  это равносильно условию, что все шары радиуса $r$ с центрами в точках вида $K(x)$ не пересекаются. Заметим, что это условие за висит не столько от отображения $K$, сколько от его образа. Это приводит к определению

\dfn Пусть дан алфавит $\mc A$ из $q$ символов. Тогда $[n,k]_q$-кодом назовём  $C \subseteq A^n$ размера $|C|=q^k$.  Если $q=2$, то говорят, что код бинарный.
\edfn

\dfn Будем говорить, что такой код исправляет $r$ ошибок, если для любых двух слов $x\neq y \in C$ верно, что $B_r(x) \cap B_r(y)= \varnothing$. 
\edfn

Попробуем немного переформулировать это определение. Что значит, что два шара радиуса $r$ с центрами в двух точках не пересекаются? Это значит, что расстояние между этими точками больше $2r$. И наоборот, если расстояние между двумя точками больше $2r$, то два таких шара не пересекаются. Введём определение:

\dfn Пусть $C$ -- код. Тогда кодовым расстоянием называется величина
$$d(C)= \min_{\substack{x,y\in C \\ x \neq y}} d_H(x,y).$$
\edfn

Тогда, если $d=2t$ или $d=2t-1$, то код $C$ исправляет  $r=t-1$ ошибок. Но число $d$ не только определяет число исправляемых ошибок. Оно имеет свой смысл. А именно, если совершено меньше чем $d$ ошибок, то мы с уверенностью можем сказать, что ошибки в принципе были. Это полезно, если вы хотите не восстанавливать ошибки, а, например, запросить тот же кусок повторно в надежде, что он придёт без ошибок. В дальнейшем нас будет интересовать именно параметр $d$.

\dfn $[n,k,d]_q$-кодом называется $[n,k]_q$-код $C$ с кодовым расстоянием $d$.
\edfn

\section{Линейные коды}

Откуда брать такие коды $C$? Понятно, что можно просто брать и повторять сообщение несколько раз, но это крайне не эффективно. С другой стороны отображение $K \colon \mc A^k \to \mc A^n$ тоже играет свою роль, ведь от скорости его вычисления будет зависеть итоговая скорость передачи сообщения. Кроме того, процесс декодирования, особенно в ситуации когда известно, что допущена ошибка, тоже должен проходить быстро.

Для того, чтобы решить эти задачи нам нужно представить себе алфавит $\mc A$ и код $C$ не просто как множества. Нам нужно завести на них максимум возможных структур. А именно, мы будем считать, что  $q=p^n$ -- степень простого, $\mc A = \F_q$ -- конечное поле. В такой ситуации естественно потребовать, чтобы отображение $K$ было линейным над $\F_q$, а $C$, следовательно, было векторным подпространством в $\F_q^n$.

\dfn Линейным $[n,k]_q$-кодом называется $k$-мерное подпространство в $\F_q^n$.
\edfn

Если кодирование линейно, то оно задаётся матрицей размера $n\times k$ и ранга $k$, которая обозначается $G$ и называется {\it порождающей матрицей}. Одному коду могут соответствовать разные отображения кодирования и, следовательно, разные матрицы $G$. В столбцах матрицы $G$ записаны вектора, порождающие код $C$. 

Отметим, что в книжках по теории кодирования кодовые слова обычно являются строчками. В этом случае кодирующее отображение есть умножение на матрицу размера $k\times n$ справа. Именно эта матрица в такой интерпретации и будет называться порождающей. Перейти от одной конвенции к другой можно при помощи операции транспонирования.

\dfn Процедура кодирования называется систематической, если первая часть кодового слова состоит из исходного сообщения. Для порождающей матрицы это накладывает следующее ограничение на её вид: 
$$G= \pmat E_k\\ G' \epmat.$$
Такое кодирование заметно облегчает декодирование, если ошибок нет. В этой ситуации, оставшиеся $n-k$ символов кодового слова называются проверочными. Иногда сообщение помещают в конец, но это вопрос соглашения.
\edfn

Как узнать, есть ли ошибки и раскодировать обратно сообщение? Для этого заметим, что всякое подпространство $C$ размерности $k$ есть ядро матрицы $H$ размера $n-k\times n$ и ранга $n-k$. Такая матрица называется {\it проверочной матрицей кода} $C$. Часто коды удобно задавать при помощи проверочной матрицы. Любая матрица $H\in M_{n-k\times n}{\F_q}$ ранга $n-k$ задаёт линейный $[n,k]_q$ код. 

Посмотрим, как связаны порождающая $G$ и проверочная $H$ матрицы одного и того же кода $C$. Несложно заметить, что они удовлетворяют равенству $HG=0$. Обратно, если есть матрица $H$, такая что $HG=0$ и $\rk H=n-k$, то $H$ есть проверочная матрица для $C$.

Как можно построить проверочную матрицу? Если $G$ имеет вид
$$G= \pmat E_k\\ G' \epmat,$$
то $H$ всегда можно взять 
$$H=(-G'\,|\,E_{n-k}).$$
Заметим, что матрица $H$ является по совместительству порождающей матрицей $[n,n-k]_q$ кода. Это приводит к понятию двойственного кода к данному. Мы не будем говорить про это подробно.

Линейность кода позволяет упростить многие вычисления, например, нахождение кодового расстояния.



\lm Пусть $C$ -- линейный $[n,k]_q$ код. Тогда кодовое расстояние $d(C)$ вычисляется по правилу 
$$d(C)=\min_{\substack{x \in C \\ x\neq 0} } d_H(0,x).$$
Так же минимальное расстояние можно вычислить как наименьшее число линейно зависимых столбцов проверочной матрицы $H$.
\proof
Действительно, пусть минимальное расстояние достигается на паре $(x,y)$. Но тогда оно же достигается на паре $(0,y-x)$. Далее, кодовые слова и только они являются элементами ядра $H$, то есть коэффициентами в линейной комбинации столбцов, дающей в результате ноль. Но тогда, если бы было меньше чем $d$ линейно зависимых столбцов, то было бы  ненулевое кодовое слово с менее чем $d$ ненулевыми символами и наоборот. 
\endproof
\elm

\rm Если матрица $H$ не является проверочной, но выполнено, что код $C$ лежит в ядре $H$, то изложенное доказательство может дать оценку на кодовое расстояние. А именно, если любые $d-1$ столбцов матрицы $H$ линейно независимы, то кодовое расстояние больше или равно $d$. 
\erm 

\rm Расстояние Хэмминга от $x$ до $0$ есть просто количество ненулевых компонент в $x$. Эту величину часто называют весом вектора и обозначают $w(x)$ или $\|x\|$.
\erm 

Общий алгоритм декодирования систематических линейных кодов следующий: полученное сообщение $v= u+e$ ($u\in C$ -- исходное сообщение, а $e$ -- ошибка) подставляем в проверочную матрицу. Имеем 
$$Hv= He=s.$$
Вектор $s\in \F_q^{n-k}$ обычно называется синдромом. Если $s=0$, то ошибок нет и мы всего лишь выбрасываем проверочные символы. Если ошибки есть, но их можно исправить (то есть $w(e)\leq r$), то вектор $e$ однозначно определяются синдромом $s$. Действительно, пусть для двух векторов $e$ и $e'$ веса меньше или равно $r$ два синдрома совпадают $He=He'$. Тогда $e-e'\in \Ker H$ и $w(e-e')\leq 2r$ чего не может быть.

Далее, есть два способа исправить ошибки. Либо мы заранее посчитали для всех возможных $e$ значения $He$ и тогда мы просто находим $e$ по этим предварительным вычислениям. Это не так долго если кодовое расстояние маленькое. 

Либо, можно рассмотреть общее решение системы $He=s$. Оно имеет вид $v-Gx$, где $x$ -- произвольный вектор из $\F_q^k$. Тогда переберём все значения $x$ и найдём ближайший к нулю вектор вида $v-Gx$. Тогда $v-Gx$ есть вектор ошибки, а $x$ есть раскодированное сообщение.

У нас пока не было ни одного примера кодов. Как можно строить линейные коды? Предложим один вариант построения линейных кодов при помощи многочленов. 

Пусть дан произвольный многочлен $g(x)$ над $\F_q$, степени $s=\deg g < n$, то по нему можно построить код при помощи отображения $\F_q[x]_{\leq n-s-1} \to \F_q[x]_{\leq n-1}$, заданное правилом:
$$m(x) \to g(x)m(x).$$
Такое отображение линейно и инъективно. В его образе лежат все многочлены степени меньше $n$ делящиеся на $g(x)$ и они образуют $[n,n-s]_q$-код. Такие коды я буду называть полиномиальными. 

Почему такие коды удобно использовать? Прежде всего их использование позволяет ускорить процесс кодирования, так как домножить один многочлен на другой можно быстрее чем умножить матрицу на столбец. Кроме того, можно подобрать многочлен в котором много нулевых коэффициентов.

Правда указанный способ кодирования не является систематическим. Несложно построить пример систематического кодирования:
$$m(x) \to m(x)x^s-r(x), \text{ где } r(x)=x^s m(x) \mod g(x).$$
Здесь, строго говоря, $m(x)$ записан в последние биты.

\upr Как выглядит порождающая матрица для естественного кодирования? А как для систематического?
\eupr

Так же, мы получаем некие бонусы в вопросах проверки наличия ошибок. А именно, в качестве проверки того, лежит $v(x)$ в коде или нет, достаточно найти остаток от деления $v(x)$ на $g(x)$.   

Если же нам известны все корни $\alpha_i$ многочлена $g(x)$ и они не кратные, то делимость $v(x)\di g(x)$ равносильна условию $v(\alpha_i)=0$. Более того, от одного неприводимого множителя $g(x)$ достаточно взять по одному корню.

Примерами применения таких кодов являются \href{https://ru.wikipedia.org/wiki/%D0%A6%D0%B8%D0%BA%D0%BB%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D0%B8%D0%B7%D0%B1%D1%8B%D1%82%D0%BE%D1%87%D0%BD%D1%8B%D0%B9_%D0%BA%D0%BE%D0%B4}{CRC}.
Название CRC расшифровывается как {\it Cyclic Redundancy Check}. Слово {\it Check} подчёркивает то, что эти коды используются, прежде всего, не для восстановления ошибок, а для их обнаружения. А что же в этих кодах циклического?

\subsection{Дополнительно:  циклические коды}

\dfn Код $C\leq \F_q^n$ называется циклическим, если для всякого слова $(a_0,\dots,a_{n-1})\in C$ слово $(a_{n-1},a_0,\dots,a_{n-2})$ тоже из $C$. 
\edfn

\rm Забавно, но не все CRC-коды на самом деле циклические. Но все они полиномиальные.
\erm


Встаёт вопрос: а как цикличность кода связана с полиномиальностью. Оказывается, что напрямую -- любой циклический код обязан быть полиномиальным. Для этого мы посмотрим на следующее соображение: удобно рассматривать пространство $\F_q^n \cong \F_q[x]_{\leq n-1}$ не просто как векторное пространство над $\F_q$, а как алгебру. Например, отождествив это пространство с $\F_q[x]/x^n-1$. А в качестве кодов будем смотреть только идеалы $C \subseteq \F_q[x]/x^n-1$. Если расписать условие, что $C$ идеал, то получим следующее утверждение

\thrm Пусть $C$ подпространство в $\F_q[x]/x^n-1$. Тогда $C$ идеал в том и только том случае, когда $C$ является циклическим кодом.
\ethrm
\proof Домножение на $x$ в этой алгебре соответствует сдвигу, а вся алгебра порождаетcя элементом $x$.
\endproof

Многочлены уже появились на арене. На вопрос, откуда вязть многочлен $g(x)$ отвечает следующий факт, дающий описание всех идеалов в $\F_q[x]/x^n-1$.

\fct Пусть $K$ -- поле. Делителю $g(x)$ многочлена $f(x)$ можно сопоставить главный идеал в $K[x]/f(x)$. Идеалы в кольце $K[x]/f(x)$ однозначно соответствуют делителям $f$ со старшим коэффициентом $1$.
\efct

Итого, все идеалы в $\F_q[x]/x^n-1$ главные и соответствуют делителям многочлена $x^n-1$. Чтобы задать идеал необходимо задать многочлен $g(x)$, такой что $ x^n-1 \di g(x)$. Это даёт линейный $[n, n-\deg g]_q$ циклический код.

Определение циклического кода появилось в самом начале прикладного использования кодов, исправляющих ошибки. Его важность была обусловлена простотой аппаратной реализации проверки на принадлежность коду. А именно, рассмотрим многочлен $\hat{g}=f/g$. Тогда элемент $v \in \F_q[x]/x^n-1$ делится на $g$ тогда и только тогда, когда $\hat g v \equiv 0 \mod x^n-1$. 
Произведение многочленов и взятие по модулю $x^n-1$ удачно реализовывались аппаратно при помощи {\it сдвиговых регистров с обратной связью}.



\section{Коды БЧХ}

Мы пока не научились двум вещам о полиномиальных кодах: оценивать кодовое расстояние у полиномиальных кодов и строить эффективный алгоритм исправления ошибок. Займёмся расстоянием.

Для оценки кодового расстояния мы воспользуемся некоторой информацией про порождающий полином $g(x)\in \F_q[x]$. Итак, пусть мы строим код над $\F_q$ длины $n$ и так получилось, что все корни многочлена $g(x)$ не кратны, лежат в $\F_{q^m}$ и являются степенями одного и того же элемента $\beta\in \F_q^*$ порядка $r$. Построим  множество степеней $T=\{i\in\ovl{0,r-1} \,|\, g(\beta^i)=0\}$, отвечающих корням многочлена $g(x)$. Немедленно замечаем, что $|T|=\deg g \leq r$.  

Чтобы оценить кодовое расстояние, нам необходима проверочная матрица или что-то вроде того. Можем ли мы предъявить проверочную матрицу для кода, заданного $g(x)$?

И да и нет. Нам будет удобнее думать не про матрицу, а про линейное отображение. Заметим, что для $f(x)\in \F_q[x]$ верно, что $f(x) \di g(x)$ тогда и только тогда, когда $f(\beta^i)=0$ для $i\in T$. Это приводит к следующей <<проверочной матрице>> $H$:
$$H=\pmat 1& \beta^{i_1}& \dots& \beta^{i_1(n-1)}\\
\vdots &&\dots&\vdots\\
1& \beta^{i_k}& \dots & \beta^{i_k(n-1)}
\epmat.$$
Слова <<проверочная матрица>> стоят в кавычках, потому что $H$ матрица не над $\F_q$, а над $\F_{q^m}$. Впрочем, легко сделать из неё матрицу над $\F_q$ переписав $\beta^i$ через базис $\F_{q^m}$. Докажем теперь основную теорему:

\thrm Пусть корни порождающего многочлена $g(x)$ кода длины $n$ есть степени элемента $0\neq \beta \in \F_{q^m}$ и $g(x)$ не имеет кратных корней. Пусть $\ord \beta \geq n$. Пусть в множестве $T$ есть $d-1$ идущие подряд индексов. Тогда минимальное расстояние кода длины $n$ заданного $g(x)$ больше или равно $d$.
\ethrm
\proof
Пусть подряд идущие $d-1$ элементов $T$ имеют следующий вид $l_0, l_0+1,\dots,l_0+d-2$. Нам надо показать, что у матрицы $H$ любые $d-1$ столбцов линейно независимы над $\F_q$. Нам удобно проверить независимость её столбцов не над $\F_q$, а над $\F_{q^m}$.

Возьмём столбцы с номерами $j_1,\dots,j_{d-1}$ и оставим только строки с номерами $l_0, l_0+1,\dots,l_0+d-2$. Получим матрицу 
$$ \pmat 
\beta^{l_0 j_1}& \dots &\beta^{l_0 j_{d-1}}\\
\vdots & & \vdots\\
\beta^{(l_0+d-2)j_1} & \dots & \beta^{(l_0+d-2)j_{d-1}} 
\epmat.$$
Нам достаточно показать, что её определитель не ноль. Найдём его. Из $s$-го столбца вынесем $\beta^{l_0j_s}$. Имеем
$$ \det \pmat 
\beta^{l_0 j_1}& \dots &\beta^{l_0 j_{d-1}}\\
\vdots & & \vdots\\
\beta^{(l_0+d-2)j_1} & \dots & \beta^{(l_0+d-2)j_{d-1}} 
\epmat = \beta^{l_0(j_1+\dots+j_{d-1})} \det  \pmat 
1& \dots &1\\
\vdots & & \vdots\\
\beta^{(d-2)j_1} & \dots & \beta^{(d-2)j_{d-1}} 
\epmat.$$
Но определитель последней матрицы есть определитель Вандермонда для элементов $\beta^{j_1}, \dots, \beta^{j_{d-1}}$. Этот определитель не обращается в ноль так как $\beta^{j_k}\neq \beta^{j_l}$, при $0\leq k< l\leq n-1< \ord \beta -1$.
\endproof

Перейдём к основной конструкции кодов, для которых применима эта лемма --  кодам Боуза -- Чоудхури -- Хоквингема.

Зафиксируем числа $q=p^s$ степень простого, натуральные числа $n$  и $m$, такие, что $(q^m-1)\di n$, а так же число $2\leq d\leq n$ и $l_0 \leq n$. $m$ обычно минимально возможное при данном $n$. Из условия делимости $q^m-1\di n$ следует, что в поле $\F_q^m$ есть элемент порядка $n$. Его несложно найти зная первообразный корень $\alpha$ в $\F_{q^m}$. Действительно, можно взять 
$$\beta=\alpha^{(q^m-1)/n}.$$ 

Рассмотрим элементы $\beta^{l_0},\beta^{l_0+1},\dots, \beta^{l_0+d-2}$ и рассмотрим $g(x)$ -- многочлен наименьшей степени, корнями которого являются эти элементы. Тогда кодом БЧХ длины $n$ c конструктивным расстоянием $d$ назовём циклический код длины $n$, порождённый полиномом $g(x)$.

\rm
Заметим, что $g(x)$ делит $x^n-1$ и поэтому код действительно циклический.
\erm

\noindent {\bf Пример:}\\
Пусть $q=2$. Возьмём $n=2^4-1=15$ (то есть $m=4$). Для того, чтобы построить поле из 16 элементов рассмотрим многочлен $x^4+x^3+1$. Он неприводим над $\F_2$. Его корень $\alpha$ -- первообразный корень степени $15$ из единицы. Возьмём $l_0=1$ и $d=5$. Тогда необходимо найти минимальные многочлены для элементов $\alpha,\alpha^2,\alpha^3,\alpha^4$. Заметим, что минимальные многочлены для $\alpha,\alpha^2,\alpha^4$ одинаковы и равны $x^4+x^3+1$ так как два последних элемента есть образы предыдущих при автоморфизме Фробениуса. Минимальный многочлен для $\alpha^3$ равен $x^4+x^3+x^2+x+1$. 

В качестве многочлена, задающего соответствующий код БЧХ с расстоянием $5$, берём $$g(x)=(x^4+x^3+1)(x^4+x^3+x^2+x+1).$$
Итого, получили $[15,7,d\geq 5]_2$-код.

\upr Постройте проверочную матрицу для этого кода.
\eupr

Частным случаем кодов БЧХ являются коды Рида-Соломона. Обобщение, использующее многочлены от нескольких переменных называется кодами Рида-Мюллера.

\subsection{Декодирование БЧХ-кодов}

Поговорим о декодировании кодов БЧХ. Опишем простейший алгоритм декодирования -- алгоритм Питерсона-Горенштейна-Цирлера.

Пусть нам дан БЧХ-код длины $n$ с конструктивным расстоянием $d$. Корни порождающего многочлена этого кода есть $\beta^{l_0},\dots, \beta^{l_0+d-2}$.
Пусть на вход мы получили многочлен $v(x)=u(x)+e(x)$, где $u(x)$ -- это пересылаемое закодированное сообщение, а $e(x)$ -- ошибка. Предположим, что $e(x)=e_{i_1}x^{i_1}+\dots +e_{i_t}x^{i_t}$ состоит из $t$ мономов, где $t\leq [(d-1)/2]$, то есть мы теоретически можем раскодировать сообщение. Для того чтобы узнать, что ошибки есть, мы вычисляем $v(\beta^{i})$, где $i\in \ovl{l_0, l_0+d-2}$. Но так как $u(x) \di g(x)$, то
$$v(\beta^{i})=u(\beta^i)+e(\beta^i)=e(\beta^i).$$
Итого мы знаем значения $e(\beta^i)$. Обозначим за
$$S_k= e(\beta^{l_0+k-1}), \text{ при } k\in \ovl{1,d-1}, \,\, X_s=\beta^{i_s} \text{ и } \eps_s=e_{i_s} \text{ при } s\in \ovl{1,t}.$$
Заметим, что эти определения эти элементы связаны следующим соотношением 
$$S_k=\sum_{s=1}^t \eps_s X_s^{l_0+k-1}.$$

Если известны $X_s$, то из указанных выше уравнений легко найти $\eps_s$. Определитель матрицы этой системы при $k\in\ovl{1,t}$ есть,  определитель Вандермонда, что гарантирует однозначность решения.

Итого, необходимо найти элементы $X_k$. Уравнения на $X_k$ не линейны. Наша задача ввести новые величины, однозначно определяющие $X_k$, на которые уже можно написать линейные уравнения. Для этого рассмотрим многочлен 
$$\chi(x)=(x-X_1)\dots(x-X_t)= x^t+\sigma_1x^{t-1}+\dots+\sigma_t.$$
Корни этого уравнения -- это величины $X_1,\dots,X_t$. Если мы найдём $\sigma_i$, то сможем найти $X_i$.

Заметим, что последовательность $S_k$ есть сумма геометрических прогрессий с частными $X_1,\dots,x_t$. Каждая из таких прогрессий является решением линейного рекуррентного соотношения с постоянными коэффициентами у которого характеристический многочлен в точности $\chi(x)$. Это означает, что у нас есть система уравнений 
$$\left\{ \begin{array}{rcl}
-S_{t+1}&=& \sigma_t S_1+\dots + \sigma_1 S_t\\
&\vdots&\\
-S_{2t}&=& \sigma_t S_t+\dots + \sigma_1 S_{2t-1}\\
\end{array} \right.$$
Теоретически, можно написать выражения для всех элементов вплоть до $S_{d-1}$, но нам это не понадобится. Разрешимость этой системы зависит от её матрицы, которая имеет вид 
$$ \Sigma_t =\pmat
S_1 & \dots & S_t\\
\vdots & & \vdots\\
S_t & \dots & S_{2t-1}
\epmat.
$$
 

Заметим, однако, что кроме того, что нам надо решить систему нам надо найти её размер, то есть число ошибок $t$. Для того, чтобы ответить на оба эти вопроса, напишем аналогичные матрицы для всех $\tau$  от $1$ др $[(d-1)/2]$ 
$$ \Sigma_\tau=\pmat
S_1 & \dots & S_\tau\\
\vdots & & \vdots\\
S_\tau & \dots & S_{2\tau-1}
\epmat =  \pmat
1 & \dots & 1\\
\vdots & & \vdots\\
X_1^{\tau-1} & \dots & X_t^{\tau-1}
\epmat
\pmat
\eps_1X_1^{l_0} & \dots & 0\\
\vdots & \ddots& \vdots\\
0 & \dots & \eps_t X_t^{l_0}
\epmat
\pmat
1& \dots & X_1^{\tau-1}\\
\vdots & & \vdots\\
1 & \dots & X_t^{\tau-1}\\
\epmat .
$$
Теперь, если $\tau=t$, то определитель этой матрицы не равен 0 и значит система однозначно разрешима. Если же $\tau>t$, то ранг первой матрицы меньше $\tau$. Значит и $\rk \Sigma_{\tau}<\tau$. В этом случае ясно, что определитель матрицы $\Sigma_{\tau}$ равен 0. Итого мы получаем следующий алгоритм исправления ошибок:
\enm 
\item Запускаем цикл по $\tau$ от 1 до $[(d-1)/2]$. Вычисляем $S_1,\dots,S_{2\tau}$ и вычисляем определители матриц $\Sigma_{\tau}$. Находим последний $\tau$ для которого этот определитель не ноль. Это и есть $t$. Решаем соответствующую систему и находим $\sigma_i$.
\item По $\sigma_i$ находим $X_l$ после чего находим позиции $i_l$. Это можно сделать подставив все возможные $\beta^i$ в $\chi(x)$.  
\item Далее, решив систему линейных уравнений можно найти $e_{i_l}$. Осталось  найти ответ по формуле $u(x)=v(x)-e(x)$.
\eenm

\subsection{Дополнительно: алгоритм Берлекэмпа-Мэсси}
Однако указанная матрица системы имеет очень регулярную структуру. Это наводит на мысль, что есть и более эффективные алгоритмы декодирования. Опишем алгоритм декодирования Берлекэмпа-Мэсси. Для этого нам надо проинтерпретировать указанную систему. Заметим, что элементы $S_1,\dots,S_d$ образуют кусок последовательности, заданной линейным реккурентным соотношением с характеристическим многочленом $x^t+\sigma_1x^{t-1}+\dots + \sigma_t= x^t \sigma(x^{-1})$.

При этом, данный многочлен является минимальным возможным для этой последовательности. Действительно, так как при $\tau =t$ определитель системы не ноль, то получаем, что есть единственный такой многочлен степени $\leq t$, который годится для элементов $S_1,\dots,S_{2t}$, а значит и для последовательности $S_1,\dots,S_{d-1}$. 

В общем виде, алгоритм Берлекэмпа-Мэсси по данной конечной последовательности $S_0,\dots, S_{N-1}$ ищет наименьшее линейное рекуррентное соотношение, которому удовлетворяет данная последовательность. 

Прежде всего перепишем условие, что последовательность $S_0,\dots, S_{N-1}$ удовлетворяет линейному рекуррентному соотношению  с коэффициентами $c_0,\dots, c_{k-1}, 1=c_k$. Рассмотрим многочлены $S(x)=S_0+\dots+S_{N-1}x^{N-1}$ и многочлен $\sigma(x)=c_0x^k+\dots + c_{k-1}x+1$. Тогда получаем, что 
$$S(x)\sigma(x)\equiv D(x) \mod x^N, \text{ где } \deg D(x) < k.$$
нам придётся отдельно говорить про степень $k$ так как её нельзя восстановить по $\sigma(x)$. Кроме того, при данном $k$ подходящий  $\sigma(x)$ не всегда единственный.


Будем искать $\sigma(x)$ и $k$ индуктивно. На $n$-ом шаге найдём наименьшее число $l_n$ и многочлен $\sigma_n(x)$ степени меньше $l_n$, которые решают задачу для $S_0,\dots, S_{n-1}$, где $n\leq N$. Иными словами, будет выполнено 
$$ S(x)\sigma_n(x) \equiv D_n(x) \mod x^n, \text{ где } \deg D_n(x) < l_n.$$
Заметим, что $l_n$ определено однозначно и всегда $l_n\leq n$ (если $l_n=n$, то это значит, что никаких соотношений в принципе нет). А  вот $\sigma_n(x)$ определено не всегда однозначно.



Посмотрим, как работает алгоритм. Если в последовательности $S_0,\dots,S_{n-1}$ одни нули, то будем считать степень $l_n$ соответствующей рекурренты равной $0$, а $\sigma_n=1$. Если же $S_{n-1}\neq 0$, но все предыдущие элементы нулевые, то очевидно на роль минимума годятся $l_n=n$ и $\sigma_n(x)=1= 1+0x+\dots+0x^n$.


Рассмотрим общую ситуацию. Пусть уже построено минимальное $l_n$ и соответствующее $\sigma_n$. Если $\sigma_n$ подходит и при добавлении $S_n$, то можно взять $l_n=l_{n+1}$ и $\sigma_{n+1}=\sigma_n$. 

Если же $\sigma_n$ не подходит, то
$$ S(x)\sigma_n(x) \equiv D_n(x)+d_nx^n \mod x^{n+1}, \text{ где } \deg D_n(x) < l_n$$
и $d_n\neq 0$. Если расписать явно, то $d_n=S_n+\sum_{k=0}^{l_n-1} c_k S_{n-l_n+k} \neq 0$, отвечает за то, что рекуррентное соотношение не выполнено для $S_n$. Посмотрим на наибольшее $m<n$, что $\sigma_m(x)\neq \sigma_n(x)$, то есть на предыдущее место изменения при построении $\sigma(x)$. Это даёт нам элемент $d_m\neq 0$, кроме того случая, когда было выполнено $S_0=\dots=S_{m-1}=0$, но $S_m\neq 0$. В этой ситуации $l_m=0$ и $d_m$ просто не вычислить. Будем считать, тогда, что $d_m=1$. Теперь, как только мы нашли $m,\sigma_m,d_m$ утверждается, что надо взять 
$$\sigma_{n+1}(x)= \sigma_n(x) - d_n d_m^{-1} x^{n-m} \sigma_m(x).$$
$$l_{n+1}=\max(l_n,n+1-l_n).$$

Для того, чтобы показать корректность этого алгоритма докажем вспомогательную лемму.

\lm[Ключевая] Пусть понадобилось поменять рекуррентное соотношение на шаге $n+1$. Тогда имеет место неравенство $l_{n+1}\geq n+1-l_n$. 
\elm
\proof  Предположим противное. Пусть $l_{n+1}\leq n-l_n$, то есть $l_{n+1}+l_n\leq n$. Распишем условие на $\sigma_n$ и $\sigma_{n+1}$ по модулю $x^{n+1}$.

$$\sigma_n S(x) \equiv D_n+d_nx^n \mod x^{n+1}$$
$$\sigma_{n+1}S(x)\equiv D_{n+1} \mod x^{n+1}$$
Так как $\sigma_n$ не подходит на роль $\sigma_{n+1}$, то $d_n\neq 0$. Домножая верхнее равенство на $\sigma_{n+1}$ получаем
$$ \sigma_{n+1}\sigma_n S(x) \equiv \sigma_{n+1}(D_n+d_nx^n) \equiv \sigma_n D_{n+1}\mod x^{n+1}$$
Заметим, что из неравенства, выражение, содержащее $d_nx^n$ имеет степень ровно $n$ после взятия остатка по модулю $x^{n+1}$, что не так для $\sigma_{n+1}D_n$ по тому же самому неравенству.
\endproof


\crl Пусть нам понадобилось поменять рекуррентное соотношение. Тогда $l_{n+1}\geq \max (l_n, n+1-l_n)$.
\ecrl

\proof[Корректность алгоритма]
Будем доказывать корректность алгоритма вместе со следующим утверждением: в предыдущем следствии при каждой смене $\sigma_i$ в алгоритме достигается равенство. 

Итак, пусть построены $\sigma_1(x),\dots, \sigma_n(x)$ и мы хотим построить $\sigma_{n+1}$.
В случае $d_n=0$ всё ясно. Пусть $d_n \neq 0$ и $m$ такое, что $l_m<l_n$, но $l_{m+1}=l_n$. Тогда по индукционному предположению имеем 
$$l_m<l_n = l_{m+1}=\max (l_m,m+1-l_m) $$
Отсюда получаем, что $l_n=m+1-l_m$. Возьмём
$$l_{n+1}=\max (l_n,n+1-l_n)=\max (l_n,n-m+l_m)$$
Условия следствия выполнены автоматически. Взятый нами $\sigma_{n+1}$ имеет степень меньшую или равную этого числа. Осталось проверить, что выполнено сравнение. 

$$\sigma_{n+1}S(x)\equiv d_n x^n+ 
D_n(x)+ d_nd_m^{-1}x^{n-m}(d_mx^m+ D_m(x)) \equiv D_n(x)-d_nd_m^{-1}x^{n-m}D_m(x) \mod x^{n+1}
$$
Оставшийся многочлен нужной степени. Отдельно стоит разобраться с базой индукции и началом работы алгоритма.
 

\endproof

\bibliographystyle{alpha}
\bibliography{lectures}

\end{document}
