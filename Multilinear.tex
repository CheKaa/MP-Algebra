\chapter{Полилинейные формы}

\section{Билинейные формы}

Одним из основных примеров нормы на векторном пространстве над $\mb R$ является корень из суммы квадратов координат $\|x\|=\sqrt{x_1^2+\dots +x_n^2}$. Однако вместе с такой нормой <<в комплекте>> идёт отображение от двух переменных $(x,y) \to \sum x_iy_i$, которое называется скалярным произведением. Оно обладает свойством билинейности, то есть линейности по каждой координате. Наша задача разработать единый подход к таким объектам над произвольным полем и разобраться с самыми важными примерами.

\dfn Пусть $V$ -- векторное пространство над $K$. Отображение $h\colon V\times V \to K$ называется билинейной формой, если\\
1) $\forall \lambda \in K$ $\forall u,v,w \in V$ верно, что $h(u+\lambda v, w) = h(u,w)+\lambda h(v,w)$,\\
2) и по второй координате: $h( w, u+\lambda v) = h(w,u)+\lambda h(w,v)$.
\edfn

\exm\\
1) Рассмотрим пространство $K^n$ и определим на нём билинейную форму $h(u,v)=\sum_{i=1}^n x_iy_i$.\\
2) Пусть $A\in M_{n}(K)$. Тогда на $K^n$  можно задать билинейную форму $(x,y)\to x^{\top}Ay$.\\
3) Рассмотрим пространство многочленов $\mb R[x]$ или пространство непрерывных функций на промежутке $[a,b]$, а так же какую-нибудь непрерывную функцию $w(x)$ и зададим билинейную форму $h(f,g)=\int_a^b f(x)g(x)w(x)dx$. Часто  в качестве промежутка выступает вся вещественная прямая, а в качестве веса выбирается $w(x)=\frac{1}{\sqrt{\pi}}e^{-x^2/2}$. Обычно, такой вес возникает, когда речь идёт о нормально распределённых случайных величинах.\\
4) Рассмотрим пространство один раз непрерывно дифференцируемых функций на отрезке $C^1[a,b]$ и введём на нём билинейную форму по правилу $h(f,g)=\int_a^b f'(x)g(x)dx$.\\
5) Рассмотрим пространство матриц $M_n(K)$ и введём на нём билинейную форму $h(A,B)= \Tr(AB)$.\\
6) Рассмотрим конечномерную алгебру $A$ над полем $K$. Тогда любой элемент $a\in A$ задаёт линейное отображение $L_a \colon A \to A$, переводящее $x\to ax$. У этого линейного отображения есть след. Для простоты обозначим его как $\tr a$.
Тогда отображение $A\times A \to K$ заданное по правилу $\tr_{A/K}(u,v)=\tr(uv)$ является билинейной формой. Замечу, что конструкция из предыдущего пункта не является частным случаем этой, а отличается на константу.\\

Как обычно, поведение объекта линейной алгебры определяется его взаимодействием с каким-либо базисом.

\dfn Пусть $e_1, \dots, e_n$ базис $V$, а $h$ -- билинейная форма на $V$. Тогда матрица $A$ составленная из элементов $h(e_i,e_j)$ называется матрицей билинейной формы.
\edfn

\lm Пусть $V$ -- пространство с выбранным базисом $e_1,\dots,e_n$. Тогда имеет место взаимооднозначное соответствие между билинейными формами $h$ на $V$ и матрицами  $A\in M_n(K)$. В частности, если вектор $v$ имеет столбец координат $x$, а вектор $u$ -- столбец $y$, то значение $h(u,v)$ можно найти по формуле $y^{\top}Ax$.
\elm
\proof Пусть $u=\sum x_i e_i$  и $v=\sum y_ie_i$. Тогда $h(u,v)=\sum x_i h(e_i,v)=\sum x_iy_j h(e_i,e_j)=x^{\top}Ay$.
\endproof

\lm Пусть $V$ -- пространство с билинейной формой $h$ и базисом $e_1,\dots,e_n$. Пусть матрица $h$ в этом базисе -- это $A$. Если выбрать другой базис $f$ с матрицей перехода $C$, то в новом базисе матрица $A$ будет иметь вид 
$A'=C^{\top}AC.$
\elm
\proof Распишем: $x^{\top}Ay=(Cx')^\top A Cy'= {x'}^{\top}C^{\top}AC y'= {x'}^\top A' y'$. Тогда матрицы $C^{\top}AC$ и $A'$ равны.
\endproof



\dfn Ранг билинейной формы -- это ранг её матрицы. 
\edfn

\dfn Будем говорить, что элемент $u$ ортогонален (слева)  элементу $v$, если $h(u,v)=0$, и записывать это как $u\bot v$. 
\edfn

\dfn Билинейная форма $h$ называется невырожденной, если $\forall v \neq 0$ существует $u \in V$, что $h(u,v)\neq 0$.
\edfn

\utv Билинейная форма невырождена тогда и только тогда, когда её матрица в некотором базисе невырождена.
\eutv
\proof Пусть $A$ -- матрица билинейной формы $h$ в некотором выбранном базисе. Пусть $A$ -- невырождена. Проверим условие для формы $h$ в координатах. Если столбец $x\neq 0$, то столбец $Ax$ ненулевой. Значит, есть столбец $y$ (с одной единицей на подходящей позиции), что $y^\top Ax\neq 0$. Но это и есть свойство невырожденности для $h$ в координатах. Обратно, если $h$ невырождена, что для любого столбца $x\neq 0$ можно подобрать столбец $y$, что $y^{\top}Ax\neq 0$. Значит $Ax\neq 0$, для любого $x\neq 0$. Значит $\Ker A=0$ и $A$ -- невырождена.
\endproof

\dfn[Ортогональное дополнение справа] Пусть $h$ -- билинейная форма на $V$. Если $U$ -- подпространство $V$, то определим множество $$U^{\bot}=\{v\in V\,|\, \forall u \in U \, \, u\bot v\}.$$ 
Это множество называется правым ортогональным дополнение к $U$ (внутри $v$ относительно $h$). Аналогично можно говорить про ортогональное дополнение слева. Оно обозначается как ${}^\bot U$
\edfn

\rm Если $e_1,\dots,e_k$ базис $U$, то условие $v\in U^{\bot}$ равносильно $e_i\bot v$ по всем $i$.  
\erm

Что можно ожидать от ортогонального дополнения относительно билинейной формы? Можем ли мы что-то сказать про его размерность?

\utv Пусть $U$ -- подпространство $V$, $h$ -- билинейная форма на $V$. Тогда $\dim U^{\bot}\geq \dim V - \dim U$.
Если форма $h$ невырождена, то имеет место равенство $\dim U^{\bot}= \dim V- \dim U$ и верно, что ${{}}^\bot(U^{\bot})=U$.
\proof Если $e_1,\dots,e_k$ базис $U$, то принадлежность $v$ ортогональному дополнению задаётся  $k$ уравнениями $h(e_i,v)=0$. Отсюда немедленно вытекает неравенство. Равенство выполнено, если матрица для указанной системы (в каком-нибудь базисе) имеет ранг ровно $k$. Давайте проверим, что для невырожденной формы это имеет место. Дополним набор векторов $e_1,\dots,e_k$ по базиса $V$ и распишем систему уравнений на координаты $x$ вектора $v$ в этом базисе. Она имеет вид:
$$\pmat E_k & 0 \epmat \cdot A x=0$$
Форма невырождена, поэтому матрица $A$ обратима. Ранг правой матрицы равен $k$. При домножении на обратимую матрицу ранг не меняется. Итого ранг равен $k$. 

Для того, чтобы доказать последнее утверждение заметим, что $U\leq {U^{\bot}}^{\bot}$. Но размерность ${}^{\bot}(U^{\bot})=n-(n-k)=k=\dim U$. Откуда получаем равенство.
\endproof 
\eutv

Можно поставить вопрос: при каком условии на подпространство $U\leq V$ пространство $V$ раскладывается в виде $U\oplus U^{\bot}$?


\utv Пусть $U\leq V$  и $h$ --  билинейная форма на $V$ тогда $V=U\oplus U^{\bot}$ тогда и только тогда, когда $h|_{U}$ невырождена.
\proof Проверим, что $U \cap U^{\bot}= \{0\}$. Пусть это не так и есть такой $0\neq x \in U$, что $x \in U^{\bot}$. Но тогда $\forall y \in U$ $h(y,x)=0$ так как $x\in U^{\bot}$, что противоречит невырожденности ограничения. Значит вместе они порождают подпространство размерности по крайней мере $\dim U+\dim U^{\bot}$. Но, как мы знаем $\dim U +\dim U^{\bot} \geq \dim V$. Значит имеет место равенство и, следовательно, пространство раскладывается в прямую сумму. 

Обратно, если $V=U \oplus U^{\bot}$, то $x\in \Ker h|_{U}$ лежит одновременно в $U$ и в $U^{\bot}$, что противоречит определению прямой суммы.
\endproof
\eutv

\rm Даже в случае невырожденных форм условие, что $h|_U$ невырождена выполнено далеко не всегда. Рассмотрим билинейную форму на $\mb R^2$, заданную  матрицей 
$$ \pmat 0& 1\\ 1& 0\epmat.$$
Её ограничения на первую ось $\lan e_1\ran$ имеет нулевую матрицу. Несложно вычислить и ортогональное дополнение к $\lan e_1 \ran $ -- это само это пространство. Разложения в прямую сумму точно нет.
\erm


\dfn В случае, если пространство $V$ разложилось в виде прямой суммы подпространств $V=U\oplus U'$, таких, что $ U'\leq U^{\bot}$, то будем говорить, что имеет место разложение в ортогональную сумму подпространств  $V=U\oplus^{\bot} U'$.
\edfn

\rm[Дополнительно] Если форма $h$ невырождена, то для данного подпространства $U$ может найтись не более одного пространства $U'$, что $V=U\oplus^{\bot} U'$ -- ортогональная прямая сумма. А именно, $U'=U^{\bot}$.
\proof Если сумма ортогональная, то $U' \leq U^{\bot}$. Осталось заметить, что их размерности должны быть равны.
\endproof
\erm

 


















\section{Симметричные билинейные формы и квадратичные формы}
Наибольший интерес среди билинейных форм вызывают формы со специальными свойствами. В этом разделе речь пойдёт про симметрические формы, хотя мы вскользь обсудим и кососимметричные формы. 



\dfn Билинейная форма $h$ называется симметричной, если $h(u,v)=h(v,u)$. Форма $h$ называется кососимметричной, если $h(u,v)=-h(v,u)$(это неправильное определение в случае $\chr k=2$).
\edfn

Для симметричных и кососимметрических билинейных форм условие, что $x\bot y$ и $y\bot x$ совпадают. Это позволяет говорить про ортогональное дополнение, не упоминая, с какой стороны мы его берём. Кроме того изучение билинейной формы можно свести (хотя и не самым удобным образом) к изучению симметричных и антисимметричных форм благодаря замечанию:

\rm Любая билинейная форма $h$  над полем, характеристика которого отлична от $2$ может быть единственным образом представлена в виде суммы $h^+$ и $h^-$, где $h^+$ -- симметрическая форма, а $h^-$ -- кососимметрическая. ($h^+(u,v)=\frac{h(u,v)+h(v,u)}{2}$, $h^-(u,v)=\frac{h(u,v)-h(v,u)}{2}$)
\erm

\lm  Билинейная форма $h$ симметрична тогда и только тогда, когда её матрица в некотором базисе симметрична, то есть $A^{\top}=A$ и кососимметрична, если $A^{\top}=-A$.
\elm
\proof Разберём случай, когда $h$ симметрична. Пусть $e_1,\dots,e_n$ -- некоторый базис. Тогда $$A_{ij}=h(e_i,e_j)=h(e_j,e_i)=A_{ji},$$
Что и означает симметричность. Обратно, пусть $x=[u]_e$, а $y=[v]_e$. Тогда $$h(u,v)=x^{\top}Ay=(x^\top A y)^{\top}=y^{\top}A^{\top} x=y^\top Ax=h(v,u).$$
\endproof

\exm\\
1) Если матрица $A$ -- симметричная, то билинейная форма $x^{\top}Ay$ на $K^n$ -- симметричная. В частности, обычное скалярное произведение -- симметричная билинейная форма.\\
2) Отображение $(f,g) \to \int_a^b f(x)g(x)\omega(x)dx$ является симметричной билинейной формой на $\mb R[x]$.\\
3) Если посмотреть на пространство $V$, состоящее из функций $f\in C^1([a,b])$, что $f(a)=f(b)=0$, то форма $f,g \to \int_a^b f'g dx$ является кососимметричной. Причина -- интегрирование по частям.\\


У нас больше не будет идти речи про билинейные формы общего вида, а только про симметричные формы. Предложим теперь альтернативный взгляд на симметричные билинейные формы. {\color{red} Внимание!} С технической точки зрения удобно предполагать, что характеристика поля, над которым мы работаем отлична от $2$. Сохраним это предположение до конца раздела про билинейные формы.

\dfn Квадратичная форма -- это отображение $q\colon V \to K$, такое, что в некоторой линейной системе координат это отображение есть однородный многочлен степени 2, то есть имеет вид $\sum_{i\leq j} b_{ij} x_i x_j$. Матрицей квадратичной формы в указанной системе координат называется матрица $$a_{ij}=\begin{cases} b_{ii}, \text{ если $i=j$},\\
\frac{b_{ij}}{2}, \text{ если $i\neq j$}.
\end{cases}.$$
Если вектор $v$ имеет столбец координат $x$, то $q(v)=x^{\top}Ax$.
\edfn

Матрица $A$ квадратичной формы -- это единственная симметричная матрица, что $q(v)=x^{\top} A x$. Действительно, единственный способ решить уравнения $b_{ij}=a_{ij}+a_{ji}$ и $a_{ij}=a_{ji}$  есть $a_{ij}=a_{ji}=\frac{b_{ij}}{2}$.
Таким образом, при выборе базиса возникает взаимооднозначное соответствие 
$$\text{ Симметричные билинейные формы } \leftrightarrow \text{ Симметричные матрицы } \leftrightarrow \text{ Квадратичные формы } $$
Покажем, что соответствие симметричных билинейных и квадратичных форм не зависит от выбора системы координат. Для этого достаточно предъявить бескоординатные формулы для этого соответствия.


\utv Пусть $h$ -- билинейная симметричная форма на $V$. Тогда $q(v)=h(v,v)$ -- это квадратичная форма. При этом, в любой системе координат матрица $q$ есть $A$ -- матрица $h$.
\proof $q(v)=h(v,v)= x^{\top}Ax$. Матрица $A$ -- симметричная и, следовательно, она и есть матрица для квадратичной формы $q$.
\eutv




\rm
Пусть $q$ -- квадратичная форма. Тогда форма $h(u,v)=\frac{q(u+v)-q(u)-q(v)}{2}$ -- симметричная билинейная. Эта конструкция обратна к конструкции из предыдущего факта. В этом случае, форма $h$ называется поляризацией квадратичной формы $q$.
\erm 


\dfn Квадратичная форма невырождена, если соответствующая ей симметричная билинейная форма невырождена.
\edfn

\subsection{Ортогонализация и метод Лагранжа}

\dfn Пусть $h$ -- симметричная билинейная форма на $V$, тогда система векторов $e_1,\dots,e_k$ называется ортогональной, если $h(e_i,e_j)=0$, при $i\neq j$. Если указанная система векторов является базисом, то такой базис называют ортогональным.
\edfn

Например, стандартный базис в $\mb R^n$ ортогонален относительно билинейной формы $x,y \to x^\top y$.

\rm Матрица симметричной билинейной формы в ортогональном базисе имеет диагональный вид, а выражение для квадратичной формы есть сумма квадратов координат вектора с коэффициентами $\sum \lambda_i x_i^2$.
\erm

\dfn Будем говорить, что симметрические билинейные (или квадратичные) формы эквивалентны, если в некоторых базисах они имеют одинаковые матрицы.
\edfn

Вопросы: всегда ли можно найти ортогональный базис и насколько форма матрицы зависит от выбора ортогонального базиса? На первый вопрос ответ положительный.

\thrm Пусть $V$ -- пространство с симметричной билинейной формой $h$. Тогда в $V$ существует ортогональный относительно $h$ базис. 
\ethrm
\proof
Если пространство $V$ одномерно или $h=0$, то подойдёт просто любой базис. Пусть $h$ -- не ноль. Тогда существует вектор $e_1$, что $h(e_1,e_1)=q(e_1)\neq 0$, потому что форма $q$ не нулевая. Теперь $h|_{\lan e_1\ran}$ невырождена и следовательно $V=\lan e_1 \ran \oplus \lan e_1 \ran^{\bot}$. По индукционному предположению на пространстве $\lan e_1 \ran^{\bot}$ есть ортогональный базис $e_2,\dots,e_n$. Тогда подходящий базис -- это $e_1,\dots,e_n$. 
\endproof







Обсудим алгоритм который стоит за этим доказательством. Для это удобнее будет работать с формой $q$ и представлять её в виде однородного многочлена второй степени. После такого отождествления описанный алгоритм можно условно назвать выделением полного квадрата. Пусть форма $q(x)$ в координатах имеет вид
$$q(x)= a_{11}x_1^2+ 2a_{12}x_1x_2 + \dots + 2a_{1n}x_1x_n  + q'(x_2, \dots, x_n).$$

\noindent{\bf Первый случай} Предположим, что $a_{11}\neq 0$. Тогда представим $q(x)$ в виде, выделив полный квадрат 
$$q(x)= a_{11}\left(x_1+\frac{a_{12}}{a_{11}}x_2 + \dots +\frac{a_{1n}}{a_{11}}x_n\right)^2 - \frac{a_{12}^2}{a_{11}}x_2^2 - 2\frac{a_{12}a_{13}}{a_{11}}x_2x_3 - \cdots - \frac{a_{1n}^2}{a_{11}}x_n^2 + q'(x_2,\dots,x_n).$$

Новые переменные выглядят следующим образом:
\begin{align*}
y_1&=x_1+\frac{a_{12}}{a_{11}}x_2 + \dots +\frac{a_{1n}}{a_{11}}x_n,\\
 y_2&=x_2, \\
&\vdots\\
 y_n&=x_n.
\end{align*}
Или в подходящую сторону
$$ \pmat x_1 \\ x_2 \\ \vdots \\ x_n \epmat = \pmat 1 & -\frac{a_{12}}{a_{11}} & \dots & -\frac{a_{1n}}{a_{11}} \\
& 1 && \\
& & \ddots & \\
&&& 1
\epmat \pmat y_1 \\ y_2 \\ \vdots \\ y_n \epmat.
$$

Видно, что кроме формы $q'$ возникает ещё поправка, которая содержит слагаемые $\lambda x_ix_j$ $i,j\geq 2$. Таким образом мы обнулили $a_{1j}$ и сделали первый вектор новой системы координат ортогональным остальным, как и в доказательстве. Заметим так же, что указанное преобразование над матрицей эквивалентно одновременному применению одинаковых элементарных преобразований строк и столбцов.\\




\noindent{\bf Второй случай.} $a_{11}=0$. Если $a_{ii}\neq 0$, то меняем первую и $i$-ую координаты местами  и продолжаем как раньше. \\


\noindent{\bf Третий случай.} Все $a_{ii}=0$. Если есть такое $i$, что $a_{1i}\neq 0$, то можно перенумеровать координаты, чтобы $i$ стало равно $2$. Пусть $a_{12}\neq 0$. Тогда сделаем замену $x_1=y_1+y_2$, $x_2=y_1-y_2$, $y_i=x_i$, $i\geq 3$. Получим $2a_{12}$ при $y_1^2$ и $-2a_{12}$ при $y_2^2$. Теперь находимся в ситуации первого случая. Если так получилось, что $a_{12}=0$, но $a_{1i}\neq 0$, то опять же, можно перенумеровать координаты и применить указанную конструкцию.\\


\noindent{\bf Четвёртый случай.} Все $a_{ii}=0$ и все $a_{1i}=0$. Тогда форма не зависит от первой переменной и можно смело переходить к следующей переменной.\\ 



\subsection{Канонический вид квадратичной формы и критерий Сильвестра}

\dfn Пусть $A$ -- матрица. Числа $d_i=\det A_i$, где $A_i$ -- подматрица $A$ составленная из элементов первых $i$ строк и столбцов  называются главными минорами матрицы $A$. Будем считать $d_0=1$.
\edfn


Оказывается, что если посмотреть на алгоритм приведения к диагональному виду, то для итоговой формы есть выражение через числа $d_i$.
\thrm[Теорема Якоби]
Пусть $V$ -- векторное пространство, $q$ -- квадратичная форма, $A$ -- её матрица в некотором базисе $e_1,\dots,e_n$. Пусть главные миноры $d_i$ не равны 0. 
Тогда матрица $A$ -- невырожденная и может быть приведена к диагональному виду с числами $\frac{d_{i}}{d_{i-1}}$ на диагонали.
\proof Заметим, что если мы всё время пользуемся первым случаем из алгоритма, то домножение слева на матрицу перехода $C^{\top}$ будет эквивалентно прибавлению строки ко всем остальным строкам матрицы $A$ c коэффициентами $-\frac{a_{1i}}{a_{11}}$ (аналогично при домножении на $C$ справа происходит преобразование столбцов). Заметим, что при таком преобразовании главные миноры матрицы вообще не меняются (внутри каждого минора происходят преобразования первого типа, которые не меняют определитель). Покажем, что только первый случай реализуется. 

Пусть мы доказали это для шага $i$. На шаге $i+1$ первые $i$ столбцов и строк содержат только ненулевые диагональные элементы $a_{11},\dots, a_{ii}$. Тогда $d_{i+1}=a_{11}\dots a_{i+1 i+1}$. Так как $d_{i+1}$ не  поменялось и, следовательно не равно нулю, то и $a_{i+1 i+1} \neq 0$. Следовательно реализуется первый случай. 

Теперь посмотрим, что происходит, после приведения матрицы к диагональному виду. Заметим, что для всех $i$ $d_i=a_{11}\dots a_{ii}$. Тогда $a_{ii}=\frac{d_i}{d_{i-1}}$, что и требовалось.
\endproof
\ethrm



Есть ситуации, где мы можем полностью описать канонический вид, к которому можно привести квадратичную форму. Проще всего дать описание над алгебраически замкнутым полем, скажем $\mb C$. Над $\mb C$ любая квадратичная форма определяется своим рангом, так как приводится к виду $$q(x)=x_1^2+\dots+x_r^2.$$


Рассмотрим поле вещественных чисел $\mb R$. К какому виду можно привести форму над $\mb R$? 

\utv Пусть $q$ -- квадратичная форма на вещественном векторном пространстве $V$. Тогда существует линейная система координат в которой форма имеет вид $$q(x)= x_1^2+\dots + x_k^2 - x_{k+1}^2-\dots-x_{k+l}^2.$$
Такой вид квадратичной формы будем называть каноническим. 
\eutv
\proof Мы уже знаем, что можно найти такие координаты, что $$q(x)= \lambda_1 x_1^2+ \dots + \lambda_k x_k^2+ \lambda_{k+1} x^{k+1} + \dots + \lambda_{k+l} x_{l+k}^2.$$
Здесь все нулевые слагаемые соответствуют последним переменным и выброшены. Нумерация  координат выбрана так, что первые $k$ коэффициентов положительные, а следующие $l$ отрицательные. Тогда выберем новые координаты $y_i=\sqrt{|\lambda_i|} x_i$ при $i\leq k+l$ и $y_i=x_i$ иначе. Это и есть нужная система координат.
\endproof

\dfn Сигнатурой формы над $\mb R$ называется пара чисел $(k, l)$ -- число плюсов и число минусов в каноническом виде. Заметим, что сумма $l+k= \rk q$.
\edfn

\dfn Квадратичная форма называется положительно определённой, если $\forall v\neq 0$ $q(v)>0$. Симметричная билинейная форма называется положительно определённой, если соответствующая форма $q(v)=h(v,v)$ положительно определена. Симметричная матрица называется положительно определённой, если соответствующая форма положительно определена. Аналогично вводится понятие отрицательно определённой формы.
\edfn



\thrm Сигнатура формы $q$ не зависит от способа приведения формы к каноническому виду. Точнее -- число $k$ равно размерности наибольшего подпространства, ограничение формы  на которое положительно определено.
\proof Рассмотрим базис $e_1,\dots,e_k,e_{k+1},\dots,e_{l+k}, \dots, e_n$, что матрица формы $q$ диагональна, и первые $k$ её диагональных компонент положительны, следующие $l$ отрицательны, а остальные 0. 
Пусть $U$ подпространство $\dim U \geq k+1$, что $q|_{U}>0$. Тогда исходя из подсчёта размерности $U\cap \lan e_{k+1},\dots,e_n\ran \neq \{0\}$. Но это приводит к противоречию, так как $q(v)$ для $0\neq v \in U\cap \lan e_{k+1},\dots,e_n\ran $ выполнено, что $q(v)>0$ и $q(v)\leq 0$ одновременно.
\endproof
\ethrm

\crl Пусть $q$ -- форма на вещественном пространстве  $V$ размерности $n$. Тогда канонический вид $q$ однозначно определяется $n$ и её  сигнатурой. 
\ecrl

Можно ли как-то ещё найти сигнатуру не приводя форму к диагональному виду, а воспользовавшись другими знаниями? Ответ: да, можно. А именно:

\crl[Критерий Сильвестра]
Пусть $V$ -- векторное пространство над $\mb R$, $q$ -- квадратичная форма, $A$ -- её матрица в некотором базисе $e_1,\dots,e_n$. Пусть главные миноры  $d_i$ матрицы $A$ все не равны $0$. Тогда число перемен знака в последовательности $1=d_0,d_1,\dots,d_n$ равно числу отрицательных квадратов в каноническом виде.
\proof По теореме Якоби существует система координат в которой форма имеет диагональную матрицу с числами $\lambda_i=\frac{d_i}{d_{i-1}}$ на диагонали. Тогда последовательность $d_i$ меняет знак тогда и только тогда, когда $\lambda_i<0$.
\endproof
\ecrl

Поговорим теперь про частный случай положительно определённых форм:


\lm Положительно определённая билинейная(квадратичная) форма всегда невырождена.
\proof $h(x,x)>0$ и поэтому не равно 0.
\endproof
\elm

\thrm
Пусть дана форма $q$ на вещественном пространстве $V$ и её матрица $A$ в некотором базисе. Следующие условия эквивалентны:\\
1) Форма $q$ положительно определена.\\
2) Главные миноры матрицы $A$ положительны.\\
3) Матрица $A$ представима в виде $A=C^{\top}C$ для некоторой невырожденной верхнетреугольной матрицы $C$.\\
4) Матрица $A$ представима в виде $A=C^{\top}C$ для некоторой невырожденной матрицы $C$.
\proof
1) в 2). Если $q>0$, то $q|_{\lan e_1,\dots,e_k\ran} >0$. Но определитель матрицы этой формы и есть главный минор порядка $k$. Значит все $d_k \neq 0$ и можно применить критерий Сильвестра. Так как форма положительно определена, то  в последовательности $1=d_0,d_1,\dots,d_n$ нет перемен знака. Отсюда $d_i>0$.

2) в 3). Следует из доказательства теоремы Якоби -- всегда реализуется первый случай и, значит, матрицы перехода верхнетреугольные и невырожденные.

3) в 4). Просто забудем, что $C$ -- верхнетреугольная.

4) в 1)  Пусть $A=C^{\top}C$, тогда если $x^{\top}Ax=(Cx)^{\top}Cx\geq 0$. Более того, это выражение равно нулю только если $Cx=0$. Но такое возможно, только если $x=0$ из-за невырожденности $C$.
\endproof
\ethrm

Представление положительно определённой матрицы $A$ в виде $C^{\top}C$ для верхнетреугольной $C$ называется разложением Холецкого. Критерий Сильвестра пригоден в случае, если имеется задача с параметром. Иначе, вычисление определителя практически эквивалентно приведению к диагональному виду. Покажем, как можно применять понятие положительной определённости. Точнее, того, что положительная определённость гарантирует невырожденность.




\zd Рассмотрим множество $\left\{ 1,\dots, n\right\}$. Сколько может быть различных подмножеств $C_1,\dots,C_m$, таких, что $|C_i \cap C_j|=t\geq 1$ одинаково для всех $i \neq j$?
\ezd

Мы покажем, что есть ограничение $m\leq n$. Прежде всего, это надо сделать в случае, если $|C_i|=t$ для некоторого $i$. В этом случае $C_i \subseteq C_j$ для всех $j$. Тогда $C_j'=C_j\setminus C_i$ лежат в множестве $\{1,\dots,n\}\setminus C_i$, которое состоит из $n-t$ элементов. Множества $C_j'$ не пусты, но имеют пустое пересечение друг с другом. Тогда их меньше чем $n-t$ штук, откуда -- каждое множество должно содержать уникальный элемент. Итого
$$m-1\leq n-t \leq n-1 \text{ или, по-другому, } m \leq n.$$

Теперь покажем, что в ситуации $d_i=|C_i|>t$ выполнено то же неравенство. Для этого составим матрицу инцидентности
$$B_{ij}= \begin{cases} 1, \text{ если } i\in C_j. \\
0, \text{ иначе}
\end{cases}. $$
Теперь матрица $B^{\top}B$ есть квадратная симметричная матрица размера $m$. Я утверждаю, что $B^{\top}B$ положительно определена. Для этого найдём её явно.
$$B^{\top}B= \pmat
d_1 & t &t\\ 
t & \ddots & t \\
t & t & d_m
\epmat = \pmat
d_1-t &  &0\\ 
 & \ddots &  \\
0 &  & d_m-t
\epmat + \pmat
t & \dots & t\\ 
\vdots & \ddots & \vdots \\
t & \dots & t
\epmat.$$
Тогда при $x\neq 0$ имеем 
$$x^{\top} B^{\top}B x = \sum (d_i-t) x_i^2 + t(\sum x_i)^2>0.$$
Значит матрица $B^\top B$ имеет ранг $m$. Но с другой стороны 
$$m=\rk B^{\top}B \leq \rk B \leq n.$$

Дальнейшие оценки и результаты существования для различных конфигураций множеств можно найти по ключевым словам Block Design (аналог в русском языке --  комбинаторные схемы)




\section{Евклидовы и унитарные пространства}

Напомню, что основной нашей мотивацией для изучения билинейных форм было, то, что вместе с понятием расстояния часто идёт вместе некоторая билинейная форма.


\dfn Векторное пространство $V$ над $\mb R$ вместе с заданной на нём положительно определённой симметричной билинейной формой $\lan\cdot \, , \cdot \ran$ называется евклидовым пространством. Форма $\lan\cdot \, , \cdot \ran$ называется скалярным произведением. 
\edfn

\dfn Определим  норму на евклидовом пространстве как $\|v\|=\sqrt{\lan v , v\ran }$. Норма задаёт расстояние по правилу $\rho(u,v)=\|u-v\|$. 
\edfn

\lm[Неравенство Коши-Буняковского] В евклидовом пространстве выполнено неравенство
$$ \lan u,v\ran \leq \|u\|\|v\|.$$
\proof  Квадратный трёхчлен $\lan u,u\ran +2\lambda\lan u,v\ran +\lambda^2\lan v,v\ran=\lan u+\lambda v, u+\lambda v\ran \geq 0$ всегда положителен. Значит у него нет корней, то есть дискриминант отрицателен. То есть $$4\lan u,v\ran^2 \leq 4 ||u||^2||v||^2.$$
\endproof
\elm

\lm Введённая норма действительно является нормой.
\proof Необходимо показать неравенство $||u+v||\leq ||u||+||v||$. Оно эквивалентно $$||u+v||^2 \leq ||u||^2+||v||^2+2||u||||v||$$
Расписывая левую часть получаем эквивалентное
$$ ||u||^2+||v||^2+2\lan u,v\ran \leq ||u||^2+||v||^2+2||u||||v||.$$
Сокращая справа и слева приходим к уже известному неравенству.
\endproof
\elm

 
\lm Пусть $V$ -- евклидово пространство. Тогда для всякого подпространства $U$ имеет место ортогональное разложение $V=U\oplus U^{\bot}$. Если есть такое разложение, то оператор проекции на $U$ называется ортогональной проекцией.
\elm
\proof Положительно определённая форма невырождена. Ограничение положительно определённой формы на любое подпространство положительно определено.
\endproof



Мы с вами помним, что в задачах алгебры  удобнее бывает работать над алгебраически замкнутым полем. Однако, если мы дословно перенесём все определения с $\mb R$ на $\mb C$, то нас постигнет неудача. Прежде всего в плане положительной определённости. А именно, любая комплексная квадратичная форма не будет принимать вещественные значения. Это делает невозможным аналогичное вещественному случаю определение расстояния. 

Это приводит нас к тому, что язык билинейных форм не совсем адекватен в комплексной ситуации. С другой стороны у нас есть пример удачного понятия расстояния на $\mb C$, которое задаётся формулой $\sqrt{\ovl{z}z}$. Такой выражение получается не из билинейной формы $xy$ подстановкой $x=y$, а из менее ожидаемого $\ovl{x}y$. Заметим, что и вообще на пространстве $\mb C^n$ можно ввести операцию $
(x,y) \to \sum_{i=1}^n \ovl{x_i}y_i$, которая даст по стандартной схеме норму $\sqrt{\sum_{i=1}^n |x_i|^2}$. Попробуем разобраться в общей ситуации.

\dfn Пусть $V$ -- комплексное пространство.  Отображение $h\colon V \times V \to \mb C$ называется полуторалинейным, если \\
1) $h(x,y+\lambda z)=h(x,y)+\lambda h(x,z)$. \\
2) $h(x+\lambda y,z)=h(x,z)+\ovl{\lambda} h(y,z)$.
\edfn

\exm\\
1) Основным примером полуторалинейной формы на $\mb C^n$ будет форма $(x,y)\to \sum \ovl{x_i}y_i$\\
2) В более общем виде, если взять матрицу $A\in M_n(\mb C)$  то выражение $\ovl{x}^{\top}Ay$ задаёт полуторалинейную форму на $\mb C^n$.\\
3) Рассмотрим пространство комплекснозначных непрерывных функций на отрезке $C([a,b])$. Определим полуторалинейную форму по следующему правилу:
$$h(f,g)=\int_a^b \ovl{f(x)}g(x)w(x)dx,$$
где $w(x)$ -- непрерывная на $[a,b]$ комплекснозначная функция, которая обычно называется весом.\\



\dfn Матрицей полуторалинейной формы $h$
в базисе $e$ называется матрица $a_{ij}=h(e_i,e_j)$. 
\edfn

\lm Если $x$ и $y$ координаты векторов $u$ и $v$, то $$h(u,v)=\ovl{x}^{\top}Ay$$
и обратно, если $$h(u,v)=\ovl{x}^{\top}Ay,$$
то $A$ -- это матрица $h$.
\elm



Заметим, что понятия симметричности  от таких форм не приходится ожидать. Действительно, если $\lambda h(u,v)=h(u,\lambda v) = h(\lambda v,u)=\ovl{\lambda}h(v,u)=\ovl{\lambda}h(u,v)$ для всех $\lambda$. Тогда, конечно, $h(u,v)=0$. Однако приведённые примеры подсказывают нам необходимое свойство.

\dfn Полуторалинейная форма $h$ называется эрмитовой, если $h(u,v)=\ovl{h(v,u)}$ и косоэрмитовой, если $h(u,v)=-\ovl{h(v,u)}$
\edfn

\lm Полуторалинейная форма эрмитова тогда и только тогда, когда её матрица $A$ удовлетворяет соотношению $\ovl{A^{\top}}=A$ и косоэрмитова, если $\ovl{A^{\top}}=-A$.
\elm

Если коэффициенты матрицы вещественны, то условие эрмитовости -- это условие симметричности. Вообще, эрмитовость формы $h$ означает, что все значения $h(v,v) \in \mb R$. Это позволяет дать опеределение аналог положительной определённости. 


\dfn Эрмитова форма называется положительно определённой, если для всех $v\in V\setminus\{0\}$ выполняется $h(v,v)>0$.
\edfn

\lm Матрица положительно определённой эрмитовой формы невырождена.
\proof От противного, если существует вектор $x\in \Ker A\setminus\{0\}$, то $0<\ovl{x}^{\top}Ax = 0$, противоречие.
\endproof
\elm

Вообще, для эрмитовых форм есть аналог теоремы об ортогонализации, теоремы Якоби, сигнатуры и критерия Сильвестра. То есть при желании можно при помощи координат проверить положительную определённость эрмитовой формы. Перейдём к основному определению, связанному с положительной определённостью в комплексном случае.


\dfn(Унитарное пространство) Пространство $V$ над $\mb C$ вместе с положительно определённой эрмитовой формой $\lan \cdot, \cdot \ran$ называется унитарным пространством. Форма $\lan \cdot, \cdot \ran$ называется скалярным произведением.
\edfn

Попробуем доказать аналог неравенства Коши-Буняковского для унитарного пространства $V$ и вывести из него, что отображение $\|\cdot\| \colon V \to \mb R$, заданное по правилу $v\to \sqrt{\lan v,v\ran}$, так же как и в вещественном случае задаёт  норму на $V$.

\lm Пусть $u,v \in V$ -- два вектора в унитарном пространстве. Тогда имеет место неравенство:
$$|\lan u,v\ran| \leq \|u\| \|v\|.$$
\proof Как и раньше  $\lan u+\lambda v, u+\lambda v\ran \geq 0$ для всех $\lambda \in \mb R$. Раскрывая скобки получаем однако $$\lan u,u\ran +2\lambda \Re\lan u,v\ran +\lambda^2\lan v,v\ran\geq 0.$$  Итого имеем  $$4(\Re\lan u,v\ran)^2 \leq 4 ||u||^2||v||^2.$$
Пусть $\lan u ,v \ran = r e^{i\ffi}$. Тогда $\Re \lan  e^{i\ffi}u , v \ran= e^{-i\ffi} r e^{i\ffi}=r=|\lan u,v\ran|$, а $||e^{i\ffi}u||=||u||$. Применение предыдущего неравенства  к паре $e^{i\ffi} u, v$ доказывает общее неравенство.
\endproof
\elm

\crl Отображение $\|\cdot\| \colon V \to \mb R$, заданное по правилу $v\to \sqrt{\lan v,v\ran}$ задаёт  норму на $V$.
\ecrl


Так же полезно будет проинтерпретировать геометрическое понятие угла между двумя векторами, чтобы отдать дань школьному определению скалярного произведения.

\dfn Пусть $x,y\neq 0$ два вектора в $V$. Если $V$ -- евклидово, то углом между ними называется такое число $0\leq\ffi\leq \pi$, что 
$$\cos\ffi = \frac{\lan x,y\ran}{\|x\| \|y\|}.$$
В случае унитарного пространства $V$ угол $\ffi$ может принимать значения $0\leq \ffi \leq \frac{\pi}{2}$ и определяется соотношением
$$\cos\ffi = \frac{|\lan x,y\ran|}{\|x\| \|y\|}.$$
\edfn

Оба определения корректны благодаря неравенству Коши-Буняковского. По каждому из этих определений угол между векторами равен $\frac{\pi}{2}$ тогда и только тогда, когда $\lan x,y 
\ran=0$, то есть когда векторы ортогональны. 

В дальнейшем мы будем обсуждать  евклидовы и унитарные пространства. Свойства евклидовых и унитарных пространств похожи, поэтому  мы будем стараться доказывать общие утверждения в обоих случаях.



\section{Ортогонализация Грама-Шмидта}

Если мы рассматриваем вектор $x$ на плоскости и некоторую прямую $l$, заданную направлением $v$, то проекция $x$ на прямую $l$ может быть найдена по формуле 

$$pr_l(x)=\cos \alpha  ||x|| \cdot v= \frac{\lan x,v\ran}{||v||^2} v.$$

В случае евклидовых пространств ограничение положительно определённой формы на любое подпространство невырождено и, таким образом, нахождение базиса, в котором форма имеет канонический вид несколько облегчается.  Это приводит к тому, что мы можем уточнить сам результат и немного изменить алгоритм нахождения подходящего базиса. Так же мы покажем, что ровно тот же алгоритм работает в унитарных пространствах и позволяет найти такой базис, что матрица скалярного произведения в этом базисе диагональна и даже единична.

Итак пусть дан набор векторов $e_1,\dots, e_n $ евклидового или унитарного пространства $V$. Ортогонализацией набора $e_1,\dots,e_n$ называется  новый набор векторов $f_1,\dots,f_n$ такой, что\\
1) $f_i \bot f_j$, если $i\neq j$\\
2) $\forall\,\, 1\leq k\leq n\,\,\lan e_1,\dots,e_k\ran=\lan f_1,\dots,f_k\ran$\\
3) $\|f_i\|=1$.

\dfn Набор векторов со свойством 3) называется нормированным. со свойствами 1),3) -- ортонормированным.
\edfn 

\rm Заметим, что если мы нашли набор ненулевых векторов со свойствами 1) и 2), в котором нет нулевых векторов, то несложно сделать из него нормированный набор, взяв вектора $\frac{f_i}{\|f_i\|}$. 
\erm

\thrm Пусть $V$ -- евклидово или унитарное пространство. Задача ортогонализации разрешима для линейно независимого набора векторов из $V$.
\proof

Перейдём к решению задачи добиваясь только условий 1) и 2). Будем последовательно искать вектора $f_i$ в виде $f_i=e_i+\lambda_1 f_1 +\dots + \lambda_{i-1} f_{i-1}$. Этот подход приводит к ответу
$$f_i=e_i-\sum_{j<i} \frac{\lan f_j,e_i\ran}{\lan f_j,f_j\ran}f_j.$$
Так как вектора линейно независимы, то $f_i\neq 0$. Это означает, что можно поделить на его норму и добиться нормированности.
\endproof
\ethrm




\crl В евклидовом и унитарном пространстве любой ортонормированный набор векторов можно дополнить до ортонормированного базиса.
\ecrl

Процесс ортогонализации позволяет строить ортогональный базис для различных подпространств. Кроме того, очень удобно находить координаты в ортогональном базисе.



\utv[Нахождение координат в ортогональном базисе] Пусть набор $e_1,\dots,e_n$ --- ортогональный базис $V$. Если $c_i$ -- это координаты вектора $x$ в базисе $e$, то 
$$c_i= \frac{\lan e_i,  x\ran}{\lan e_i, e_i\ran}.$$
В случае нормированного базиса формула упрощается -- исчезает знаменатель. Кроме того, в этой ситуации $\|x\|^2=\sum |c_i|^2$
\proof Пусть $x=\sum c_i e_i$. Тогда $\lan e_i, x\ran = c_i \lan e_i,e_i \ran$, что и требовалось.
Предположим, что $e$ -- ортонормированный базис. Тогда, раскрывая скобки в выражении $||x||^2=\lan x, x\ran$ приходим к $\sum |c_i|^2 $.
\endproof
\eutv


\rm Для любого подпространства в унитарном пространстве $U \leq V$ так же определено его ортогональное дополнение $U^{\bot}=\{ v\in V\,|\, \lan u,v\ran =0 \text{ для всех } u\in U\}$. $V$  раскладывается в прямую сумму $V=U\oplus U^{\bot}$ -- действительно их пересечение $0$, а уравнения на элементы из $U^\bot$ по прежнему задаются при помощи базисных элементов из $U$, что даёт нужное соотношение на размерность.
\erm


\crl Пусть $ e_1,\dots, e_n$ --- ортогональный базис $V$, а  подпространство $U$ порождено $ e_1,\dots,e_k$. Тогда 
\enm
\item $ pr_U x= \sum \frac{\lan x,e_i\ran}{\lan e_i,e_i\ran} e_i.$
\item $||pr_{U^{\bot}} x||^2 + ||pr_U x||^2=||x||^2.$
\eenm
\proof Рассмотрим базис $e_1,\dots, e_k$ и дополним его векторами $e_{k+1},\dots,e_n$ до ортогонального базиса всего $V$. Тогда $$x= \sum_{i=1}^k \frac{\lan e_i,  x\ran}{\lan e_i, e_i\ran}e_i + \sum_{i=k+1}^n \frac{\lan e_i,  x\ran}{\lan e_i, e_i\ran}e_i $$
Первая часть суммы лежит в $U$, а вторая в ортогональном дополнении. По единственности такого разложения получаем требуемое.
\endproof
\ecrl







Для чего может пригодиться понятие ортогональной проекции? Прежде всего для вычисления расстояния. Расстояния между подпространствами. Начнём с общего определения.

\dfn Пусть $A$ и $B$ подмножества метрического пространства. Тогда расстоянием $\rho(A,B)$ положим равным
$$\rho(A,B)=\inf_{\substack{x\in A\\ y \in B}} \rho(x,y).$$
\edfn

Наша задача --- научиться считать расстояние между аффинными подпространствами $A_1$ и $A_2$ в евклидовом пространстве, то есть подмножествами вида $L+x$, где $L$ -- это произвольное подпространство (линейное), а $x$ -- некоторый вектор из $V$. Попробуем это сделать. Представим $A_1=L_1+x$ и $A_2=L_2+y$. 

Разберём сначала случай расстояния от точки до линейного подпространства. 

\thrm Пусть $U \leq V$ подпространство, $x \in V$. Тогда расстояние $\rho(x,U)$ достигается на проекции  $pr_U(x)$ и равно $\|x-pr_U(x)\|=\|pr_{U^{\bot}}(x)\|$.
\proof Рассмотрим $u \in U$, и представим $x=y+z$, где $pr_Ux=y \in U$, $pr_{U^{\bot}}x =z \in U^{\bot}$. Тогда $$\rho(x,u)^2= ||x-u||^2= ||y - u ||^2+ ||z||^2 \geq ||z||^2=||pr_{U^{\bot}}x||^2 .$$
С другой стороны равенство достигается при $u=y=pr_{U} x$.
\endproof
\ethrm



\rm Если размерность $U^{\bot}$ мала, то может быть легче найти проекцию на $U^{\bot}$, найдя ортогональный базис $U^{\bot}$.
\erm

Всё это немедленно приводит к решению общей задачи.

\utv Пусть  $A_1=L_1+x$ и $A_2=L_2+y$ -- аффинные подпространства. Тогда $\rho(A_1,A_2)=\rho(y-x, L_1+L_2)$. То есть задача сводится к ранее разобранной.
\proof $$\inf_{\substack{u+x\in L_1+x\\ v+y \in L_2+y}} ||u+x-v-y||=\inf_{\substack{u-v\in L_1+ L_2}}||u-v - (y-x)||=\inf_{\substack{u\in L_1+ L_2}}||u- (y-x)||.$$
\endproof
\eutv


\dfn Пусть $e_1,\dots, e_k$ набор векторов $V$. Тогда матрицей Грама называется матрица 
$$G(e_1,\dots,e_k)_{ij}= \lan e_i, e_j\ran.$$
Матрица Грама отличается от матрицы скалярного произведения как би(полутора)линейной формы, только тем, что определяется она для произвольного набора векторов. 
\edfn

Как мы помним, процедура ортогонализации Грама-Шмидта аналогична той, которую мы обсуждали в общем контексте билинейных форм. Главные миноры матрицы Грама и, в частности, определитель матрицы Грама дают коэффициенты матрицы после ортогонализации. А что значат эти коэффициенты? Разберёмся в случае $\mb R^n$.

\utv
Пусть $v_1,\dots,v_n$ -- набор векторов в $\mb R^n$. Тогда для стандартного скалярного произведения имеем
$$ \det G(v_1,\dots,v_n)=(\Vol(v_1,\dots,v_n))^2$$
\proof Пусть матрица $A$ составлена из столбцов $v_i$. Тогда $G(v_1,\dots,v_n)=  A^{\top}A$ и
$$ \det G(v_1,\dots,v_n)= \det A^{\top}A=\det A^{\top} \det A= (\det A)^2= \Vol(v_1,\dots,v_n)^2.$$
\endproof
\eutv

\rm Таким образом, видно, что понятие расстояния точно определяет понятие объёма параллелепипеда. Хотя и не даёт возможности задать ориентацию пространства.
\erm

\rm Определитель матрицы Грама обнуляется тогда и только тогда, когда вектора $v_i$ линейно зависимы.
\erm


\upr Пусть $A$ -- это матрица Грама набора векторов $e_1,\dots,e_k \in \mb R^n$, а $B$ -- набора векторов $e_1,\dots,e_{k+1}$. Покажите, что $$\frac{\det B}{\det A}=\rho(e_{k+1},U)^2, \text{ где } U=\lan e_1,\dots,e_k \ran.$$
\eupr





\subsection{Метод наименьших квадратов}


Допустим, мы хотим узнать некоторый закон природы в виде $y=f(x)$. Мы провели много измерений $y_i=f(x_i)$. Если мы предполагаем, что функция $f$ есть, например, многочлен, то на коэффициенты этого многочлена возникает система линейных уравнений. 

К сожалению, нет никаких шансов, что эта система разрешима: количество измерений велико а наши измерения не точны. Иными словами, на самом деле выполнены равенства $y_i= f(x_i) + \eps_i$ для маленьких $\eps_i$. Что делать если мы не можем точно решить систему? Будем искать наиболее близкое её решение. В качестве меры близости разумно выбрать сумму
$$\sum_i |y_i-f(x_i)|^2.$$
Обобщая, мы приходим к следующей задаче: пусть есть матрица $A\in M_{m\times n}(\mb R)$, столбец $b\in R^m$ и мы хотим найти $x$ такой, что  
$$||Ax-b|| \text{ минимальна.}$$
Заметим, что в качестве $Ax$ может выступать произвольный элемент $\im A$. Это означает, что для решения нашей задачи мы должны найти ближайший к $b$ элемент $y \in \Im A$ и для этого $y$ найти соответствующий ему $x$. У этой задачи есть довольно простое решение. А именно, заметим, что элемент $y=Ax=pr_{\Im A} b$ должен иметь одинаковые с $b$ скалярные произведения со всеми столбцами матрицы $A$ (потому что они порождают $\Im A$). Это равносильно матричному соотношению
$$A^\top A x=A^\top b.$$
Как теперь найти сам вектор $x$? Рассмотрим ситуацию, когда $\Ker A=\{0\}$. В этом случае, матрица $A$ осуществляет взаимооднозначное соответствие между векторами из $\mb R^n$ и векторами из $\Im A \leq \mb R^m$.
Матрица $A^\top A$ -- это матрица Грама для столбцов матрицы $A$ и значит невырождена (столбцы $A$ линейно независимы). Значит у этой системы есть единственное решение 
$$x=(A^\top A)^{-1}A^\top b,$$
которое и даёт искомый единственный вектор $x$. Если же $\Ker A \neq 0$, то ситуация усложняется. 

\begin{comment}

Мы по прежнему ищем $y\in \Im A$ ближайший к $b$. Но для данного $y$ есть много $x\in \mb R^n$, что $Ax=y$. Какой из них выбрать? Выберем $x$ с наименьшей  длиной. Что это означает? Представим произвольное решение этого уравнения как $x_0+z$, где $z\in \Ker A$. Наименьшее длина такого вектора будет достигаться при $z= pr_{\Ker A} x_0$. В этом случае $x=x_0+z$ будет равен проекции $x_0$ на $(\Ker A)^{\bot}$. Способ нахождения такого элемента мы обсудим позже.

\end{comment}

\dfn Если $\Ker A =0$, то матрица $(A^{\top }A)^{-1}A^{\top}$ называется псевдообратной к $A$. В общей ситуации определение более сложное.
\edfn

На математической статистике вам объяснят, почему такой выбор является наилучшим при некотором предположении на распределений ошибок в каждом равенстве. Однако метод наименьших квадратов удобно использовать и для анализа данных и прогнозирования их поведения.


\section{Ортогональные и унитарные операторы}

Основным отличием структуры евклидового и унитарного пространства от просто векторного пространства является понятие расстояние и поэтому некоторое время мы посвятим преобразованиям, это расстояние сохраняющим.




\dfn Пусть $V$ --- евклидово (унитарное) пространство. Ортогональным (соответственно унитарным) оператором на $V$ называется такой линейный оператор $L \colon V \to V$, что $||Lx||=||x||$.
\edfn




\thrm Пусть $L \colon V \to V$ -- линейный оператор на евклидовом или унитарном пространстве $V$. Тогда следующие условия эквивалентны:\\
1) $L$ -- ортогональный (унитарный) оператор.\\
2) $\lan Lx,Ly\ran=\lan x,y \ran$ для всех $x,y \in V$.\\
3) $L$ переводит любой ортонормированный базис в ортонормированный базис.\\
4) В любом ортонормированном базисе $A$ -- матрица $L$ -- удовлетворяет условию $\ovl{A}^{\top} A = E_n$.\\
5) $L$ переводит некоторый ортонормированный базис в ортонормированный базис.\\
6) В некотором ортонормированном базисе $A$ --  матрица $L$ -- удовлетворяет условию $\ovl{A}^{\top} A = E_n$.
\proof Покажем $1\to 2$. Пусть $x$ и $y$ из $V$, тогда $$||x||^2+ ||y||^2+ 2\Re\lan x,y\ran= ||x+y||^2= ||L(x+y)||^2= ||x||^2+||y||^2+2\Re \lan Lx,Ly\ran.$$
Итого вещественные части скалярного произведения сохраняются. Теперь взяв вместо $x$ вектор $ix$ получаем $\Re \lan ix,y\ran = - \Re i\lan x,y\ran=\Im \lan x,y\ran$ и аналогично $\Re \lan L(ix),Ly\ran =\Im \lan Lx,Ly \ran$ откуда и мнимые части совпадают.
$2\to 3$ ясно.


Покажем, что $3 \leftrightarrow 4$ и $5 \leftrightarrow 6$. Для этого покажем, что если фиксировать ортонормированный базис $e_1,\dots, e_n$, то $L(e_i)$ ортонормирован, тогда и только тогда, когда $A$ -- матрица $L$ удовлетворяет условию $\ovl{A}^{\top}A=E_n$. Для этого необходимо и достаточно заметить, что $$\lan Le_i, Le_j \ran = (\ovl{A}^{\top}A)_{ij}.$$
Действительно $v_j$ -- $j$-ый столбец $A$ составлен из координат $Le_j$. $i$-ая строка $\ovl{A}^{\top}$ равна тогда $\ovl{v}_i^{\top}$. Но тогда $$\lan Le_i,Le_j\ran = \ovl{v}_i^{\top}E_n v_j= \ovl{v}_i^{\top}v_j= (\ovl{A}^{\top}A)_{ij}.$$
Во втором равенстве $E_n$ играет роль матрицы Грама ортонормированного базиса $e_1,\dots, e_n$.
Итак $3 \leftrightarrow 4$ и $5 \leftrightarrow 6$. Из 3) очевидно следует 5). $6\to 1$. $$||Lx||^2=\ovl{[x]_e}^{\top} \ovl{A}^{\top} A [x]_e= \ovl{[x]_e}^{\top} [x]_e= ||x||^2.$$
\endproof
\ethrm

\crl В частности, ортогональный оператор  $L$ сохраняет углы между векторами.
\ecrl

\crl Пусть $e_1,\dots,e_n$ --- ортонормированный базис $V$. Линейный оператор $L$, который в базисе $e_i$ имеет матрицу, составленную из столбцов $v_1,\dots,v_n$, является ортогональным (унитарным) тогда и только тогда, когда $v_1,\dots,v_n$ --- ортонормированный базис $\mb R^n$. 
\proof
Пусть $B$ -- матрица $L$ -- составлена из столбцов $v_i$ в ортогональном базисе $e$. Тогда соотношение $\ovl{B}^{\top}B=E$ эквивалентно ортогональности и нормированности $v_i$.
\endproof
\ecrl




\dfn Матрица $A\in M_n(\mb R)$ называется ортогональной, если $A^{\top}A=E_n$. Множество всех ортогональных матриц размера $n$ обозначается $\O_n(\mb R)$. Такие матрицы описывают все линейные изометрии $\mb R^n$  и поэтому образуют подгруппу в группе $\GL_n(\mb R)$.
\edfn

\rm Это определение можно применить и к комплексным матрицам, в результате чего получится группа $\SO_n(\mb C)$. Однако вместо неё популярнее другая группа матриц:
\erm

\dfn Определим группу унитарных матриц $\U_n(\mb C)$, как подгруппу в $\GL_n(\mb C)$, состоящую из матриц, удовлетворяющих равенству $\ovl{A}^{\top}A=E_n$.
\edfn

\rm Заметим, что определитель ортогональной матрицы либо плюс, либо минус единица. Определим подгруппу $\SO_n(\mb R) \leq \O_n(\mb R)$ -- специальную ортогональную группу состоящую из матриц $$\SO_n(\mb R)= \{ A \in \O_n(\mb R)\, | \, \det A=1 \}.$$
сохраняющих ориентацию. Это подгруппа индекса 2. Её называют группой вращений $\mb R^n$.
Аналогично определяется группа $SU_n$
\erm 


\subsection{QR разложение}

Пусть $A \in M_{m\times n}(\mb R)$ -- матрица ранга $n$. Посмотрим на неё как на набор столбцов $e_1,\dots,e_n$. Применим к этим столбцам процедуру ортогонализации. При применении процедуры ортогонализации мы используем элементарные преобразования столбцов. Более того, мы всегда столбцы с меньшим номером прибавляем к столбцам с большим номером. Таким образом, мы всегда домножаем матрицу $A$ на верхнетреугольную матрицу. Итого существует верхнетреугольная матрица $R$, что 
$$AR=Q,$$ 
где столбцы $Q$ образуют ортонормированный базис $\mb R^n$. Тогда  $Q$ -- ортогональная матрица. Домножив на $R^{-1}$ получаем, что 
$$A=QR^{-1},$$
$R^{-1}$ -- верхнетреугольная невырожденная, $Q$ состоит из ортонормированного набора столбцов. Таким образом мы доказали:

\thrm Для любой матрицы  $A\in M_{m \times n}$ существуют матрицы $R\in UT_n(\mb R)$ и $Q\in M_{m\times n}(\mb R)$,  что столбцы $Q$ ортонормированны, $R$ -- невырождена и  
$$A=QR.$$
Такое разложение называется $QR$  разложением. Если $A$ -- квадратная, то матрица $Q$ -- ортогональна. 
\ethrm


Если вам известно $QR$ разложение матрицы, то это позволяет вам легко вычислить псевдообратную (в частности, обратную) матрицу. Действительно: $A^\top A= R^{\top} Q^\top Q R= R^\top R$. Значит
$$(A^\top A)^{-1}A^\top= R^{-1} (R^\top)^{-1}R^\top Q^\top= R^{-1}Q^\top.$$
Процесс ортогонализации считается более устойчивым к ошибкам округления, чем метод Гаусса. Поэтому, такое обращение для вещественных матриц предпочтительней.


Вы можете спросить: есть ли тут что-то общее с QR-кодом? Ответ: ничего кроме букв.
 





\section{Сопряжённые линейные отображения}

Геометрия евклидовых и унитарных пространств позволяет сводить определённые вопросы про билинейные формы к вопросам про операторы и наоборот. А именно, всякому оператору $L$ соответствует билинейная (полуторалинейная) форма $\lan x,Ly\ran$. Несложно понять, что любая билинейная форма имеет такой вид (из соображений, что и оператор и форма однозначно  задаются квадратными матрицами) и по этой форме можно обратно восстановить $L$.

Кроме того, $L$ задаёт другую билинейную (полуторалинейную) форму $\lan Lx,y\ran$. Восстановив по этой форме обратно оператор, используя уже обсуждённое ранее соответствие мы получим вообще говоря отличный от $L$ оператор. Его обозначают обычно как $L^*$. 

Разберёмся с этими соответствиями поближе. Нам удобно будет расширить контекст с операторов на одном евклидовом (унитарном) пространстве, до произвольных линейных отображений между такими пространствами.

\dfn Пусть $L$ -- линейное отображение $L\colon U \to V$ между евклидовыми или унитарными пространствами. Тогда сопряжённым отображением к $L$, называется такое линейное отображение $L^*$, что $\lan L^*x,y\ran = \lan x,Ly\ran$ для всех $x\in V$ и $y \in U$.
\edfn

\thrm Сопряжённое линейное отображение единственно. Более того, если в $U$ и $V$ выбрать ортонормированные базисы $u$ и $v$, \ матрица $L$ в этих базисах есть $A$, то матрица сопряжённого отображения будет равна $\ovl{A}^{\top}$.
\proof Достаточно доказать необходимость и достаточность последнего соотношения, чтобы показать единственность и существование. Выберем ортонормированные базисы в $U$ и $V$ -- $u_i$ и $v_j$. Обозначим матрицу кандидата на $L^*$ за $B$. Тогда для равенства из определения сопряжённости необходимо и достаточно, его выполнения на базисных. Иными словами необходимо и достаточно, чтобы $\lan L^*e_i,e_j\ran=\lan e_i,Le_j\ran$. Но первая часть даёт $\ovl{B_{ji}}$, а вторая -- $A_{ij}$. Итого необходимо и достаточно, чтобы $\ovl{B}^\top=A$, то есть $B=\ovl{A}^\top$.   
\endproof
\ethrm



\crl Сопряжённый оператор к оператору $L$ существует и единственен. Более того, если задан ортонормированный базис $e_1,\dots,e_n$ и $A$ -- матрица $L$, то матрица $L^*$ есть $\ovl{A}^{\top}$.
\ecrl









\lm[Общие свойства]
$(L+T)^*=L^*+T^*$\\
$(LT)^*=T^*L^*$\\
$(\lambda L)^*=\ovl{\lambda}L^*$.\\
$(L^{-1})^*=(L^*)^{-1}$.\\
$L^{**}=L$.
\proof Фиксируем ортонормированный базис. Тогда все свойства следуют из свойств транспонирования и сопряжения ($\ovl{AB} = \ovl{ A} \,\ovl{B}$).
Однако их можно показать и из определения сопряжённого оператора. Например,
$$\lan x, ABy\ran = \lan A^*x, By\ran = \lan B^*A^*x,y\ran,$$
откуда видно, что $(AB)^*=B^* A^*$. 
\endproof
\elm




\dfn Пусть $L$ оператор на евклидовом или унитарном пространстве $V$ называется самосопряжённым, если $L^*=L$.
\edfn


\rm Пусть $e_1,\dots e_n$ ортонормированный базис, тогда оператор $L$ cамосопряжён тогда и только тогда, когда его матрица в этом базисе удовлетворяет условию $\ovl{L}^{\top}=L$.
\proof Равенство операторов равносильно равенству их матриц в ортонормированном базисе.
\endproof
\erm



\exm\\
0) Любая симметричная матрица $A=A^{\top}$ задаёт самосопряжённый оператор на $\mb R^n$ относительно стандартного скалярного произведения.\\
1) Условие ортогональности оператора можно переписать в виде $L^*L=1$ или, что эквивалентно, $L^*=L^{-1}$. Таким образом, сопряжённый оператор к ортогональному -- это обратный оператор.\\
2) Пусть $\mb R[x]$ пространство многочленов и $g(x,y) \in \mb R[x,y]$ -- многочлен. Тогда сопряжённый к оператору $f \to \int_{a}^b f(y)g(x,y)dy$ это оператор $f\to\int_{a}^b f(x)g(x,y)dx$.\\
3) Сопряжённый оператор к ортогональному -- это обратный к нему.




\section{Спектральные теоремы}

Как всегда при обсуждении линейных операторов разумно задать вопрос про их собственные числа. Для того, чтобы облегчить нашу задачу и не повторять несколько раз одни и те же рассуждения, заметим, что унитарные, самосопряжённые и косоэрмитовы операторы обладают следующим свойством.

\dfn Пусть $L$ оператор на евклидовом или унитарном пространстве $V$. Оператор $L$ называется нормальным, если $LL^*=L^*L$.
\edfn



Мы хотим связать инвариантные пространства относительно $L$ и инвариантные пространства относительно $L^*$. Какой бы ни был оператор верен факт.

\lm \label{normal} Если подпространство $U$ инвариантно относительно $L$, то $U^{\bot}$ инвариантно относительно $L^{*}$.
\proof Пусть $v\in U^{\bot}$. Тогда для всех $u\in U$ верно $\lan L^* v, u\ran = \lan v, Lu\ran =0 $ так как $Lu\in U$, что и требовалось.
\endproof
\elm

\lm Пусть $L$ и $T$ два оператора на комплексном векторном пространстве $V$, которые коммутируют между собой, то есть $LT=TL$. Тогда у $L$ и $T$ есть общий собственный вектор. 
\proof Пусть $\lambda$ -- собственное число $L$. Покажем, что $\Ker L -\lambda E$ -- инвариантное пространство, относительно $T$. Действительно, пусть $v\in \Ker L-\lambda E$. Тогда $(L-\lambda E)Tv=T(L-\lambda E)v=0$.

Теперь, так как $\Ker L-\lambda E$ инвариантно относительно $T$, то у $T$ есть собственный вектор в $\Ker L-\lambda E$. Он же собственный для $L$.
\endproof
\elm

\thrm 
Оператор $L$ на  унитарном пространстве $V$ нормален тогда и только тогда, когда существует ортонормированный базис $e_1,\dots,e_n$ в котором матрица $L$ диагональна.
\proof
Доказательство идёт индукцией по размерности. Если оператор нормален, то у $L$ и $L^*$ есть общий собственный вектор $v_1$, так как они коммутируют.

Возьмём к нему ортогональное дополнение $U=\lan v_1\ran^{\bot}$. Это будет инвариантное подпространство для $L$ и $L^*$. При этом ограничение $L^*$ на $U$ -- это сопряжённый к $L|_U$. По индукции это даёт ортонормированный базис для $L$ на $U$, а вместе с $v_1$ базис из собственных векторов на всём $V$.
\endproof
\ethrm

\crl Если $L$ -- нормальный оператор на унитарном пространстве $V$, то $$V=\bigoplus_{\lambda - \text{ с.ч. } L} \Ker L-\lambda E.$$
При этом $\Ker L-\lambda_i E$ попарно ортогональны при различных $\lambda_i$.
\ecrl


Прежде чем перейти к вещественному случаю, докажем лемму про общее строение вещественных матриц и соответствующих им операторов.



\lm Пусть $A$ вещественная квадратная матрица размера $n$. Тогда если $v \in \mb C^n$ собственный вектор $A$ с собственным числом $\lambda$, то $\ovl{v}$ собственный вектор $A$ с собственным числом $\ovl{\lambda}$. В частности, комплексное сопряжение осуществляет биекцию между $\Ker A-\lambda E$ и $\Ker A-\ovl{\lambda} E$.
\proof Действительно
$$\ovl{\lambda}\ovl{v}=\ovl{\lambda v}=\ovl{Av}=\ovl{A}\ovl{v}=A\ovl{v}.$$
\endproof
\elm 




\thrm
Оператор $L$ на евклидовом пространстве $V$ нормален  тогда и только тогда, когда существует ортонормированный базис в котором его матрица $A$ блочно-диагональная, при этом блоки имеют  или размер $1\times 1$ или $2\times 2$ вида
$$\begin{pmatrix}
a  & b\\
-b & a
\end{pmatrix}.$$
Такое представление единственно с точностью до порядка блоков.
\ethrm
\proof
Прежде всего покажем единственность блочной структуры. Пусть $e_i$ -- ортонормированный базис в котором $L$ имеет матрицу в указанном виде. Тогда блочное представление матрицы даёт возможность посчитать характеристический многочлен. Клетка $1\times 1$ даёт множитель $t-\lambda$, то есть соответствует вещественному собственному числу. Клетке $2\times 2$ соответствует множитель в характеристическом многочлене $t^2-2at+a^2+b^2$. Этот многочлен раскладывается на множители $$t^2-2at+a^2+b^2= (t-a-bi)(t-a+bi).$$
Если $b\neq 0$, то корни этого многочлена не вещественны, а если равно 0, то мы попадаем в уже разобранную диагональную ситуацию. Тогда количество клеток вида 
$$\begin{pmatrix}
a  & b\\
-b & a
\end{pmatrix}$$
с $b\neq 0$ равно кратности корня $a+bi$ у характеристического многочлена.



Перейдём к доказательству существования. Достаточно доказать существование такого базиса для матриц $A\in M_n(\mb R)$, что $A^\top A=AA^\top$. Такая матрица задаёт нормальный оператор на только на $\mb R^n$, но и на $\mb C^n$. Этим и воспользуемся.

Рассмотрим все собственные числа $\lambda$ матрицы $A$. Если $\lambda\in \mb R$, то $\Ker A-\lambda E$ имеет ортонормированный базис из векторов с вещественными компонентами. Пусть  $\lambda = a+bi$ с $b\neq 0$. Пусть  $v=e_1 + i e_2 $ -- собственный вектор для $\lambda$. Здесь $e_1=\Re v$, а $e_2=\Im v$ -- вещественные вектора. Тогда $\ovl{v}=e_1-ie_2$ собственный вектор для $\ovl{\lambda}=a-bi$. Тогда
$$\lan e_1, e_2\ran = \frac{1}{4} \lan v+\ovl{v}, -i( v-\ovl{v})\ran= \frac{-i}{4}(||v||^2-||\ovl{v}||^2)=0.$$
Покажем, что норма $e_i$ равна $\frac{1}{\sqrt{2}}$, например, для случая $e_1$. 
$$||e_1||^2=\lan e_1, e_1\ran = \frac{1}{4} \lan v+\ovl{v},  v+\ovl{v}\ran= \frac{1}{4}(||v||^2+||\ovl{v}||^2)=\frac{1}{2}.$$

Итого, вектора $\sqrt{2}e_1,\sqrt{2}e_2$ вещественны, а так же независимы ортогональны и нормированы над $\mb C$ и, следовательно, над $\mb R$. Подпространство, порождённое ими инвариантно, относительно $A$. На нём в базисе $e_1, e_2$ матрица $A$ действует как 
$$e_1 \to \frac{1}{2}(\lambda v + \ovl{\lambda}\ovl{v})=ae_1 - be_2 $$
$$e_2 \to \frac{1}{2i}(\lambda v - \ovl{\lambda}\ovl{v})=\frac{1}{2i}( 2i b e_1 + 2i  a e_2) $$
Разумеется, она действует так же и в базисе $\sqrt{2}e_1,\sqrt{2}e_2$. Откуда получаем, что пространство $\lan e_1,e_2\ran $ инвариантно относительно $A$ над $\mb R$, и в базисе $\sqrt{2}e_1,\sqrt{2}e_2$ матрица $A$  действует как  
$$\begin{pmatrix}
a & b\\
-b & a
\end{pmatrix}$$
Из этих соображений построим ортонормированный базис всего $\mb R^n$. Для собственных чисел $\lambda \in R$ возьмём вещественный ортонормированный базис $\Ker A-\lambda E$, а для пары сопряжённых комплексных собственных чисел $\lambda,\ovl{\lambda}$ возьмём $v_1,\dots,v_k$ -- ортонормированный базис $\Ker A-\lambda E$ и по каждому вектору построим вещественные вектора $e_{i,1}=\Re v_i$ и $e_{i,2}=\Im v_i$. Тогда $\sqrt{2}e_{1,1}, \sqrt{2}e_{1,2},\dots, \sqrt{2}e_{k,1},\sqrt{2}e_{k,2}$ -- это ортонормированный набор векторов. При этом, подпространства, порождённые каждой парой векторов с одинаковым первым индексом инвариантны относительно $A$ и линейый оператор, заданный $A$ на этом подпространстве имеет матрицу нужного вида.

Вместе, эти вектора дают искомый ортонормированный базис $R^n$.

\endproof






Теперь можно легко получить характеризацию самосопряжённых, унитарных и вещественных ортогональных.

\thrm Пусть $L$ -- оператор в евклидовом (унитарном) пространстве $V$. Тогда $L$ -- самосопряжённый тогда и только тогда, когда существует ортонормированный базис $V$ состоящий из собственных векторов оператора $L$ и все собственные числа $L$ -- вещественны.
\proof Пусть $L$ оператор на унитарном пространстве. Возьмём ортонормированный базис из его собственных векторов и распишем условие самосопряжённости. Оно означает, что $\ovl{A}^{\top}=A$. Но $A$ диагональна и на диагонали стоят собственные числа. Итого на них получается уравнение $\ovl{\lambda}=\lambda$, что гарантирует их вещественность.

Пусть теперь  $L$ оператор на евклидовом пространстве. Тогда есть ортонормированный базис, в котором матрица $L$ составлена из блоков $1\times 1$ или $2\times 2$ вида 
$$\pmat a& b \\ -b & a \epmat.$$
Но матрица $L$ в ортогональном базисе должна быть симметрична. Отсюда $b=-b$, то есть $b=0$, что и требовалось.
\endproof
\ethrm



\zd Докажите спектральную теорему в случае самосопряжённого оператора напрямую.
\ezd

\thrm Оператор $L$ -- унитарный тогда и только тогда, когда его собственные числа по модулю равны 1 и существует ортонормированный базис из собственных векторов. 
\proof Рассмотрим ортонормированный базис из собственных векторов $e$. Матрица $A$ оператора $L$ в этом базисе диагональна и удовлетворяет соотношению $\ovl{A}^{\top}A=E_n$. Для собственных чисел $A$ это означает, что $$|\lambda|^2=\ovl{\lambda}\lambda=1.$$ 
\endproof
\ethrm






\thrm Оператор $L$ на евклидовом пространстве $V$ ортогональный  тогда и только тогда, когда существует ортонормированный базис в котором матрица $A$ блочно-диагональная, при этом блоки имеют размер $1$ и состоят из $\pm 1$ или имеют размера $2$ и имеют вид
$$\begin{pmatrix}
\cos \varphi & \sin \varphi\\
-\sin \varphi &\cos \varphi
\end{pmatrix}.$$
\ethrm
\proof
По вещественной версии спектральной теоремы для самосопряжённых операторов есть ортонормированный базис в котором матрица $L$ состоит из блоков $1\times 1$ или $2\times 2$ вида 
$$\pmat a& b \\ -b & a \epmat.$$
Но матрица $L$ в ортогональном базисе должна быть ортогональной и, следовательно, вещественной и унитарной. Откуда получаем, что её собственные числа по модулю равны 1. Если эти числа вещественные, то они равны $\pm 1$. Если же они не вещественные, то имеют вид $a+bi=\cos \ffi + i \sin \ffi$, что даёт необходимый вид для $a$ и $b$.

Обратно, оператор $L$ ортогонален так как его матрица ортогональна в ортонормированном базисе по условию теоремы. 
\endproof













\section{Задачи на максимизацию}

Теперь обратимся к вопросам, связанным с вещественными самосопряжёнными операторами.

Рассмотрим один из вопросов, связанных с такой конструкцией, а именно, рассмотрим задачу о нахождении нормы линейного оператора $L \colon U \to V$ между двумя евклидовыми пространствами. Для того, чтобы найти  норму необходимо найти
$$\max_{x\neq 0}\sqrt{\frac{\lan Lx,Lx\ran}{\|x\|^2}}=\sqrt{\max_{x\neq 0}\frac{\lan L^*Lx,x\ran}{\|x\|^2}}=\sqrt{\max_{\|x\|=1} \lan L^*Lx,x\ran}.$$
Таким образом, нахождение нормы оператора свелось к задаче максимизации некоторой квадратичной формы на единичной сфере. Заметим, что максимум действительно достигается благодаря компактности сферы.

Оказывается, что довольно легко найти максимум или минимум произвольной квадратичной формы на сфере. Для этого мы заметим, что по любому самосопряжённому оператору $A$ на евклидовом пространстве $V$ строится квадратичная форма $q(x)=\lan x, Ax\ran$ и обратно, оператор можно восстановить по этой квадратичной форме. Точнее, если $v_1,\dots,v_n$ -- ортонормированный базис, то матрица $A$ в этом базисе совпадает с матрицей $q$. Теперь докажем теорему.



\thrm Пусть $V$ -- евклидово пространство, $A$ -- самосопряжённый оператор на $V$, а $q(x)=\lan x,Ax\ran$ -- соответствующая квадратичная форма. Тогда 
$$\max_{ x\in V } \frac{q(x)}{||x||^2}=\max_{\substack{ x\in V \\ ||x||=1}} q(x)=\lambda_1,$$
где $\lambda_1$ - наибольшее собственное число оператора $A$ и достигается на собственном векторе $v_1$, соответствующему $\lambda_1$. Аналогично минимум равен минимальному собственному числу $A$. 
\proof
Пусть $v=\sum c_i e_i$, причём $1=||v||^2=\sum c_i^2$. Тогда $\lan Av,v\ran = \sum c^2_i \lambda_i $, что меньше $\lan A e_1,e_1\ran= \lambda_1= \sum \lambda_1 c_i^2$.
\endproof
\ethrm

Эта теорема, кроме, собственно, решения задачи, даёт геометрическую характеризацию первого собственного числа для самосопряжённого оператора. Вопрос: можно ли аналогично охарактеризовать другие собственные числа? Ответ получается не таким простым, но, тем не менее, полезным.

\thrm[Куранта-Фишера] Пусть $q(x)=\lan x, Ax\ran$, где $A$ -- самосопряжённый оператор на евклидовом пространстве. Тогда $k$-ое по убыванию собственное число $\lambda_k$ для $A$ есть 
$$\lambda_k=\max_{\dim L=k} \min_{\substack{ x\in L \\ ||x||=1}} q(x) = \min_{\dim L=n-k+1} \max_{\substack{ x\in L \\ ||x||=1}} q(x).$$
Причем максимум достигается на инвариантном подпространстве, содержащем собственные вектора для $\lambda_1,\dots,\lambda_k$.
\ethrm
\proof Будем доказывать только первое описание. Прежде всего заметим, $\lambda_k$ достигается, если взять $U=\lan v_1,\dots,v_k\ran$. Дейстительно, в этом случае, так как $U$ инвариантно относительно $A$ форме $q|_U$ просто соответствует ограничение $A|_U$, а его максимум уже легко посчитать так как нам известны его собственные числа.

Пусть $U$ --- подпространство на котором достигается максимум, причём допустим, что максимум больше $\lambda_k$. Тогда рассмотрим подпространство $W=\lan v_k,\dots,v_n\ran$, где $v_i$ --- собственный вектор соответствующий $i$-ому по убыванию собственному числу. Заметим, что $\dim U\cap W\geq 1$, из формулы Грассмана. С другой стороны, если мы возьмём вектор $x\in U \cap W$ с нормой $1$, то значение $q(x)$ с одной стороны меньше или равные $\lambda_k$, так как $x\in W$, а с другой стороны строго больше, так как $x\in U$. Полученное противоречие и доказывает теорему.  
\endproof




\crl Пусть $U$ некоторое подпространство размерности $m$ евклидового пространства $V$, а $q(x)=\lan x, Ax\ran$, где $A$ -- самосопряжённый оператор. Пусть собственные числа $A$ --- это $\lambda_i$, а собственные числа оператора, соответствующего $q(x)|_U$ -- это $\mu_i$, упорядоченные по убыванию. Тогда 
$$\lambda_{i+n-m}\leq \mu_i\leq \lambda_i.$$  
\proof Заметим, что
$$\mu_i=\max_{\substack{L\leq U\\ \dim L=i}} \min_{\substack{ x\in L \\ ||x||=1}} q(x),$$
что очевидно меньше, чем 
$$\max_{\substack{L\leq V\\ \dim L=i}} \min_{\substack{ x\in L \\ ||x||=1}} q(x)=\lambda_i.$$
Неравенство в другую сторону получается из второго описания в теореме Куранта-Фишера.
\endproof
\ecrl

В основном это удобно применять для оценки собственных чисел у симметричной подматрицы.

\crl Пусть $A\in M_n(K)$ -- симметричная матрица. Пусть $\Gamma \subseteq \{1,\dots, n\}$ размера $m$. Обозначим за $A^{\Gamma}_{\Gamma}$ матрицу $A$, в которой оставили только элементы из строк и столбцов из $\Gamma$. Пусть $\lambda_i$ -- это собственные числа $A$, а $\mu_i$ -- собственные числа $A^{\Gamma}_{\Gamma}$. Тогда
$$\lambda_{i+n-m}\leq \mu_i\leq \lambda_i.$$
\ecrl 


Введём не совсем стандартное определение:
\dfn
Пусть $q$ -- квадратичная форма на евклидовом пространстве. Рассмотрим ортонормированный базис $u_i$ пространства $V$. Тогда положим 
$$\Tr q= \sum q(u_i).$$ Если в базисе $u_i$ форме $q(x)=x^{\top} Ax $ соответствует симметричная матрица $A$, то $\Tr q(x)=\Tr(A)$.
\edfn

\rm Определение не зависит от выбора ортонормированного базиса. Действительно, если замена координат ортогональна, то матрица $q$ в новой системе координат имеет вид $C^{\top}AC=C^{-1}AC$. Осталось заметить, что след последней матрицы очевидно равен следу $A$.
\erm

\rm Если форме $q$ соответствует самосопряжённый оператор $A$, то $\Tr q=\sum \lambda_i$, где $\lambda_i$ -- собственные числа $A$.
\erm




\crl Пусть $q(x)=x^{\top} Ax$, $U$ --- некоторое подпространство, $\dim U=k$. Пусть собственные числа $A$ --- это $\lambda_i$. Тогда $$\Tr q|_U\leq \sum_{i=1}^k \lambda_i= \Tr q|_{V_k},$$
где $V_k$ подпространство натянутое на первые $k$ собственных векторов $q$.
\proof Нам известны неравенства на собственные числа $\mu_i$, ограничения $q|_U$. А именно, $\mu_i\leq \lambda_i$. Но тогда и $$\sum_{i=1}^k \mu_i \leq \sum_{i=1}^k \lambda_i.$$
\endproof
\ecrl 






\section{Метод главных компонент}

Рассмотрим следующую задачу: имеется массив данных --- набор векторов $x_1,\dots,x_s \in \R^n$. Подразумевается, что точек очень много. Предположим, что при идеальных измерениях между координатами этих векторов есть линейные зависимости. Все отклонения от этой погрешности вызваны небольшими погрешностями. Задача состоит в том, чтобы восстановить линейную зависимость. 


Переформулируем задачу геометрически. Пусть наши вектора удовлетворяют линейным условиям $Ax_i=b$ для некоторой матрицы $A$ ранга $n-k$. Это значит, что они лежат в аффинном подпространстве $\Ker A + a_0$ размерности $k$. Если же нам даны вектора с погрешностями, то задачу таким образом можно поставить в виде: найти аффинное подпространство размерности $k$ наиболее близкое к данным точкам $x_1,\dots,x_s$. Понятие <<наиболее близкое>> требует конкретизации. Вообще говоря, тут есть выбор. Мы будем считать, что подходящее пространство $W=U+a_0$ должно давать минимум следующего выражения:
$$\sqrt{\sum_{i=1}^s \rho(x_i,W)^2} \to \min.$$
Совершенно ясно, что корень квадратный тут для красоты, и минимизировать нужно $\sum_{i=1}^s \rho(x_i,W)^2$.

Прежде всего установим, что какой бы <<минимайзер>> $W=U+a_0$ мы не нашли, в $W$ всегда будет лежать среднее $\frac{1}{s}\sum_{i=1}^s x_i$ и, следовательно, в качестве $a_0$ всегда можно взять среднее. Действительно, распишем условие, что сумма 
$$\sum_{i=1}^s \rho(x_i-a_0, U)^2=\sum \|pr_{U^{\bot}} (x_i-a_0)\|^2$$
минимальна.  Продифференцируем по координатам $a_0$. Получим $$\sum -2pr_{U^{\bot}} x_i + 2s \,pr_{U^{\bot}} a_0=0$$
 Это условие означает, что проекции $a_0$ и среднего $\frac{1}{s}\sum x_i$ на подпространство $U^{\bot}$ совпадают. То есть эти две величины отличаются на элемент $U$. Тогда среднее лежит в $W$. 

Итак, вычтя из всех $x_i$ их среднее можно считать $a_0=0$, а все точки $x_i$ удовлетворяют равенству $\sum_{i=1}^s x_i=0$. Замечу, что это равенство нигде в дальнейшем не будет использовано. Благодаря такой замене мы свели задачу к поиску подпространства $U$ размерности $k$, которое минимизирует 
$$\sum_{i=1}^s \rho(x_i, U)^2=\sum_{i=1}^s \|pr_{U^{\bot}} (x_i)\|^2.$$

Воспользуемся тем, что $\|x\|^2=\|pr_U x\|^2+\|pr_{U^{\bot}} x\|^2$ или в другом виде $\|x\|^2-\|pr_U x\|^2=\|pr_{U^{\bot}} x\|^2$. Получаем, что выражение
$$\sum_{i=1}^s \|pr_{U^{\bot}}(x_i)\|^2=\sum_{i=1}^s \|x_i\|^2-\|pr_U x_i\|^2$$
должно быть минимально. Сумма $\|x_i\|^2$ постоянна. Значит  необходимо и достаточно, чтобы выражение $\sum_i \|pr_U x_i\|^2$ было максимально.


Для того чтобы посчитать проекцию на $U$ в каждом слагаемом выберем в $U$ ортонормированный базис $u_1,\dots,u_k$. Перепишем
$$\sum_{i=1}^s \|pr_U x_i\|^2=\sum_{i=1}^s\sum_{j=1}^k \lan x_i,u_j\ran^2=\sum_{j=1}^k \sum_{i=1}^s \lan x_i,u_j\ran^2.$$

Внутренняя сумма теперь есть значение некоторой квадратичной формы на векторе $u_j$. Разберёмся, что это за форма. Рассмотрим матрицу $X$ размера $s\times n$, чьи строки это  вектора $x_i$ $i\in\ovl{1,s}$. Тогда вектор $d_j=Xu_j$ состоит из скалярных произведений  $\lan x_i, u_j\ran$. Значит 
$$\lan d_j,d_j\ran = (Xu_j)^{\top}Xu_j = \sum_{i=1}^s \lan x_i,u_j\ran^2,$$ 
что совпадает со слагаемым нашей суммы. Рассмотрим симметричную матрицу $A=X^{\top}X$ и обозначим соответствующую ей форму за $q$. Тогда нам надо максимизировать выражение

$$\sum_{j=1}^k q(u_j).$$

Таким образом мы ищем максимум $\Tr q|_{U}$ по всем подпространствам $U$ размерности $k$, где форма $q$ соответствует матрице $X^{\top} X$. А его мы искать умеем. Сформулируем  ответ. 


\thrm  Пусть есть набор векторов $x_1,\dots,x_s \in V$. Определим симметричную, положительно полуопределённую матрицу $A$, как $A=X^{\top}X$, где строчки матрицы $X$ -- это вектора $x_i$. Тогда минимум по всем аффинным подпространствам $V$ размерности $k$ выражения $\sum_{i=1}^s \rho(x_i,W)^2$ достигается при $a_0=\frac{1}{s}\sum x_i$  и $U=\lan v_1,\dots,v_k\ran$, где $v_i$ --- собственные вектора $A$, причём соответствующие собственные числа упорядочены по убыванию. 
\ethrm

\rm Стоит немного сказать про вероятностную интерпретацию полученного ответа. Предположим, что точки $x_i\in \R^n$ сгенерированы каким-то случайным процессом и подчиняются некоторому общему распределению. Тогда вектор $a_0$ -- это просто оценка на математическое ожидание этого распределения.

После того как мы нашли $a_0$ мы центрируем набор $x_i$ и минимизируем $\sqrt{\sum \rho (x_i, U)^2}$. Представим себе, что $U=\{0\}$. Что значит эта сумма? Это то, что мы назвали бы оценкой дисперсией величины (конечно, надо ещё усреднить). 
А что если $U \neq \{0\}$? Тогда $\sqrt{\sum \rho (x_i, U)^2}$ -- это заготовка для оценки дисперсии проекций случайных величин на $U^\bot$. То есть мы ищем подпространство $U$, так что на ортогональное дополнение приходится минимально возможная дисперсия (а при проекции на само $U$ должен достигаться максимум дисперсии среди всех подпространств той же размерности).

Какой же смысле имеет матрица $X^\top X$, её собственные вектора и собственные числа. Матрица $X^\top X$ -- это с точностью до константы ($1/s-1$) эмпирическая матрица ковариации. Если мы хотим посчитать квадрат дисперсии в направлении $v$, то мы считаем $v\top X^\top X v$. Значит собственные вектора этой матрицы -- это такие вектора, для которых достигается экстремум квадрата дисперсии. А собственные числа -- это собственно квадраты дисперсии.
\erm

\rm Отдельно отметим, что в приложениях принято с самого начала центрировать данные и нормировать каждую компоненту $x_i$ так, чтобы дисперсия вдоль каждого направления была единичной.
\erm



\section{SVD-разложение}

При вычислении с помощью метода главных компонент часто используют несколько другие конструкции. Для того, чтобы в этом разобраться посмотрим на матрицу  $X^{\top}X$  и её собственные числа.  

\dfn Пусть $A$ -- линейное отображение между евклидовыми пространствами $U \to V$. Тогда сингулярными значениями $A$ называются корни из положительных собственных чисел оператора $A^*A$.
\edfn



Теперь обсудим важную конструкцию, проясняющую геометрический смысл сингулярных значений.


\thrm[SVD разложение] Пусть $L$ -- линейное отображение $L\colon U \to V$ между евклидовыми пространствами. Тогда существуют такие ортонормированный базисы $U$ и $V$, что матрица $L$ имеет вид 
$$\Sigma=\pmat \sigma_1 &\dots& 0 & 0\\
 \vdots & \ddots &\vdots & \vdots\\
 0 & \dots & \sigma_r & 0\\
 0 &  \dots & 0 & 0 \epmat,$$
 где $r$ -- ранг $L$. Числа $\sigma_1, \dots, \sigma_r$ на диагонали обязаны быть равными сингулярным значениям $L$.
На языке матриц это означает, что для любой матрицы $A \in M_{m\times n}$ существуют ортогональные матрицы  $P$ -- размера $m$ и $Q$ -- размера $n$,  что
$$A= P \Sigma Q.$$
 
\proof Рассмотрим оператор $B = L^{*}L$. Тогда существуют ортонормированный базис $e_1,\dots,e_n$ в котором оператор $B$ диагонален, с неотрицательными числами на диагонали $d_1\geq\dots\geq d_n\geq 0$. Имеем  $d_i=\sigma_i^2$ для единственного положительного $\sigma_i$. 
Посмотрим на вектора $Le_i \in U$. Они ортогональны. Действительно
$$\lan Le_i, Le_j\ran = \lan L^{*}Le_i,e_j \ran = \lan d_i e_i,e_j\ran,$$
что равно нулю, если $i\neq j$. В случае $i=j$ получаем $\|e_i\|^2=d_i$. Возьмём 
$$f_i=\frac{Le_i}{\sqrt{d_i}}$$
и дополним этот набор до ортонормированного базиса пространства $U$. Итого имеем $e_1,\dots,e_n$ ортонормированный базис $U$ и $f_1,\dots,f_m$ -- ортонормированный базис $V$.
Посмотрим матрицу $A$ в этих базисах. По определению $Le_i=\sqrt{d_i}f_i$. Это и даёт требуемый вид матрице оператора $L$.


Напоследок осталось решить вопрос, как выглядит матрица $Q$. В нашей конструкции матрица $Q$ есть матрица замены координат из стандартного базиса в базис из собственных векторов $e_i$ матрицы $A^{\top}A$. Если за $C$ обозначить матрицу из столбцов $e_i$, то $Q=C^{-1}$, но $C$ ортогональна и поэтому можно написать $Q=C^{T}$, то есть строки $Q$ -- собственные вектора $A^{\top}A$.
\endproof
\ethrm

\upr Получите аналогичное описание для $P$.
\eupr

\dfn Базисные вектора такой системы координат в $U$ называются левыми сингулярными векторами $A$, а базис в $V$ -- правыми.  
\edfn

Наличие SVD-разложения означает, что для всякого линейного отображения можно так выбрать декартову систему координат, что в этой системе координат это отображение будет выглядеть как растяжение вдоль каких-то осей.





SVD-разложение используется в практическом решении задачи из метода главных компонент и позволяет сразу найти не только пространство, но и проекцию начальных точек на него. Формализуется это так: рассмотрим матрицу $X$, чьи строки равны $x_i^{\top}$. Тогда если все её строки заменить на их проекции на оптимальное подпространство $V_k$, то получится матрица ранга $k$ или меньше.

Если составить из этих проекцией матрицу $X^{(k)}$, то $\rho(\{x_i\}, V_k)^2= \|X-X^{(k)}\|_F^2 $, где 
$$\|X\|_F=\sqrt{\sum_{i,j} x_{ij}^2}=\sqrt{\Tr X^{\top}X}$$
есть нормы на пространстве матриц. Эта норма, называется нормой Фробениуса.

Оказывается, что так построенная матрица $X^{(k)}$, является ближайшей матрицей ранга $\leq k$ к матрице $X$
Действительно, рассмотрим матрицу $Y^{(k)}$, которая приближает $X$ не хуже $X^{(k)}$. То есть 
$$\|X -Y^{(k)}\|_F^2 \leq \| X-X^{(k)}\|_F^2= \sum \rho(x_i, V_k)^2.$$
Возьмём $L=\Im Y$ -- подпространство размерности $k$. Тогда матрица ${Y^{(k)}}'$ чьи столбцы есть проекции столбцов $X$ на $L$ ближе к $X$, чем $Y^{(k)}$ и равенство достигается, только если $Y^{(k)}={Y^{(k)}}'$. В этой ситуации оптимальная матрица как раз и приходит из пространства $L$ что нам бы и хотелось. Покажем это равенство.

Заметим по построению $\| X - {Y^{(k)}}'\|_F = \sum \rho (x_i, L)^2$, что обязано быть больше или равно чем $\sum \rho(x_i, V_k)^2$. Значит имеет место равенство. Но тогда и матрицы $Y^{(k)}$ и ${Y^{(k)}}'$ одинаково хорошо приближают $X$. Значит $Y^{(k)}={Y^{(k)}}'$ приходят из оптимального подпространства в методе главных компонент. Почему мы не говорим, что эти матрицы обязательно равны $X^{(k)}$? Потому что оптимум не единственен (если есть кратные собственные числа у $X^\top X$).


Таким образом, нахождение проекций точек на оптимальное, с точки зрения метода главных компонент, подпространство можно переформулировать как нахождение ближайшей к $X$ матрицы ранга меньше или равного $k$. Оказывается, такую матрицу легко найти, зная SVD-разложение.

Прежде чем использовать SVD-разложение заметим, что в такой формулировке задача может быть поставлена в бескоординатном виде: для данного линейного отображения $L$ между евклидовыми пространствами найти наиболее близкое к нему линейное отображение ранга $k$ относительно нормы $\|L\|=\sqrt{\Tr L^*L}$.

\thrm Пусть $L$ -- линейное отображение между евклидовыми пространствами. Пусть $\Sigma$ -- матрица с сингулярными значениями $L$ на диагонали. Тогда ближайшее к $L$ отображения ранга $k$ имеет матрицу $\Sigma^{(k)}$ в базисах из правых и левых сингулярных векторов. Здесь  на диагонали $\Sigma^{(k)}$ стоят вначале $\sigma_1,\dots,\sigma_{k}$, а остальное -- нули.
\proof Перейдём в базис из правых и левых сингулярных векторов. В этих базисах матрица $A$ -- это $\Sigma$.
Значит теперь мы решаем задачу нахождения ближайшей матрицы к матрице $\Sigma$. Для этого нам надо взять проекции строк $\Sigma$ на подпространство порождённое первыми $k$ собственными векторами $\Sigma^\top \Sigma$. После этого из этих проекций надо составить матрицу. Но первые $k$ собственных векторов $\Sigma^\top \Sigma$ это просто первые $k$ координатных векторов. Значит составленная из проекций матрица -- это $\Sigma^{(k)}$.
\endproof
\ethrm 

\crl Пусть $A$ имеет сингулярное разложение $P\Sigma Q$. Тогда ближайшая к $A$ матрица ранга $k$ (в смысле нормы Фробениуса) имеет вид $A^{(k)}=P\Sigma^{(k)}Q$, где на диагонали $\Sigma^{(k)}$ стоят вначале $\sigma_1,\dots,\sigma_{k}$, а остальные элементы матрицы -- нули.
\ecrl

\upr На самом деле матрица $A^{(k)}$ ближайшая к $A$ среди матриц ранга $k$ и в смысле обычной матричной $l_2$-нормы. 
\eupr



\section{Немного вычислительной линейной алгебры}


Перед нами встаёт вопрос: как аккуратно вычислить те объекты линейной алгебры, которые мы определили. Какие для этого есть методы и чем они отличаются. Совершенно понятно, что это огромный круг вопросов и ответить на них на все невозможно. Поэтому я постараюсь обрисовать основные примеры и основные подходы к ответам.

Рассмотрим прежде всего задачу о решении системы линейных уравнений $Ax=b$ с вещественными коэффициентами. 

Во всех реальных приложениях вектор $b$, а часто и матрица $A$ даны не точно, а с некоторой погрешностью. Кроме того, при вычислениях с плавающей точкой мы создаём ещё больше погрешностей. Посмотрим  пример
$$ \pmat 1 & 1 \\ 1 & 1.0001 \epmat x = \pmat 2\\ 2.0001 \epmat$$
У этой системы есть точное решение $x=(1,1)$. Предположим, однако, что из-за ошибок округления вектор $b$ стал равен 
$$b_{new}=\pmat 2 \\ 2 \epmat$$
Решение для $b_{new}$ равно $x=(2,0)^\top$. Видно, что малые изменения коэффициентов системы могут значительно изменить её решение.

Попробуем разобраться, что происходит с общей системой при малых возмущениях её коэффициентов. Для начала разберёмся с погрешностями в $b$.

Обозначим погрешность в значениях свободного члена за $\Delta b$. То есть на самом деле нам дана система $Ay=b+\Delta b$. Самое лучшее, что мы можем сделать -- это точно решить эту новую систему. Насколько решение $y$ этой приближённой системы может отличаться от решения исходной? Пусть $y=x+\Delta x$. Тогда вычитая одно уравнение из другого получаем
$$A \Delta x= \Delta b.$$
Нас будет интересовать прежде всего не абсолютная, а относительная погрешность $\frac{\|\Delta x\|}{\|x\|}$. Оценим её
$$\frac{\|\Delta x\|}{\|x\|}=\frac{\|A^{-1} \Delta b \|}{\|x\|}=\frac{\|A^{-1} \Delta b \|}{\|b\|} \frac{\|b\|}{\|x\|} \leq \|A\| \|A^{-1}\| \frac{\|\Delta b\|}{\|b\|}.$$
Заметим, что здесь мы пользовались определением нормы матрицы при помощи нормы на исходном пространстве, но сам вид нормы на исходном пространстве нам не важен.

Получается, что чтобы мы не делали, просто из-за изначальных погрешностей в данных мы не можем рассчитывать  на погрешность меньшую $\kappa(A)\frac{\|\Delta b\|}{\|b\|}$
(чуть позже мы увидим, что эта оценка точная). 

\dfn Число $\kappa(A)$ называется числом обусловленности матрицы $A$.
\edfn

Оказывается, что число обусловленности отвечает и за влияние погрешностей в коэффициентах матрицы $A$.

Действительно, если посмотреть на решение возмущённой системы $(A+\Delta A)(x+\Delta x)=b$, то вычитая точную систему получаем
$$\Delta x= -A^{-1}\Delta A (x+\Delta x)$$
Переходя к нормам получаем
$$\frac{\|\Delta x\|}{\|x+\Delta x\|}\leq \|A^{-1}\| \|A\| \frac{\|\Delta A\|}{\|A\|}$$

Как найти $\kappa(A)$? Ответ зависит от выбранной нормы. Нам проще всего понять, как найти $\kappa(A)$ в случае евклидовой нормы на $\R^n$. В этой ситуации $$\kappa_2(A)=\frac{\sigma_1}{\sigma_n}.$$
Двойка снизу тут в честь $l_2$-нормы.
Покажем теперь, что оценка погрешности при помощи $\kappa_2(A)$ точная. Для этого перейдём в базисы из сингулярных векторов. Тогда можно считать, что $A=\Sigma$ -- диагональна. Возьмём вектора $x,b,\Delta x$ и $\Delta b$ следующим образом:
$$x=\pmat 1 \\ 0\\ \vdots \\ 0 \epmat,\, b=Ax=\pmat \sigma_1 \\ 0\\ \vdots \\ 0 \epmat, \, \Delta b = \pmat 0 \\ \vdots \\ 0 \\ \eps \epmat, \, \Delta x=  \pmat 0 \\  \vdots \\ 0 \\ \eps/\sigma_n\epmat.$$
Для таких векторов оценка достигается.

Можно ли заранее оценить $\kappa(A)$? Можно. Но это не так просто и мы это обсуждать не будем. Замечу только, что главная сложность состоит в оценке $\|A^{-1}\|$.

\dfn
Говорят, что система $Ax=b$ хорошо обусловлена, если $\kappa(A)$ -- мало.
\edfn

Однако мы пока ничего не сказали про особенности методов, могут ли они существенно добавить к ошибкам? Оказывается, что могут. Рассмотрим систему 
$$\pmat 0.0001 & 1 \\ 1 & 1 \epmat x= \pmat 1 \\ 2 \epmat $$
Допустим мы храним только три значащих цифры. Применяя напрямую метод Гаусса последнее уравнение преобразуется к виду $-9999 x_2=-9998 $. Округление его точного решения есть $x_2=0.9999$ (если округлить до 4-го знака), откуда получаем $x_1=1$ (что тоже будет верным до 4-го знака). Но если мы используем округление для коэффициентов матрицы на первом шаге, то предыдущее равенство принимает вид $10000x_2 = 10000$. Получаем, что $x_2'=1$ и $x_1'=0$. Что значительно отличается от точного решения.

\rm Отметьте, что $\kappa_2(A)$ в этом примере всего лишь $2.61...$.
\erm

\dfn Метод называется устойчивым, если ошибки в ходе вычисления имеют порядок, сравнимый с ошибками от возможных погрешностей в исходных данных.
\edfn

Метод Гаусса не является устойчивым методом решения систем линейных уравнений. Можно его доработать, на каждом шаге переставляя строки матрицы так, чтобы каждый раз выбирать в качестве главного элемента  наибольший элемент в столбце. Однако это всё равно решает не все проблемы (см. Wilkinson).

В качестве альтернативы можно посмотреть различные методы, которые находят $QR$ разложение. Однако и $QR$ разложение может давать побочные эффекты.





Есть другой класс методов, позволяющих довольно легко найти приближённое решение. Речь идёт про  итерационные методы. Разберём самый простой пример. Пусть нам дана система линейных уравнений вида $x=Ax+b$, где $A$ -- квадратная матрица. Заметим, что любая система может быть представлена в таком виде. 

Предположим в дальнейшем, что собственные числа $A$ по модулю меньше $1$. Возьмём начальный вектор $x_0$ и построим последовательность
$$x_i=Ax_{i-1}+b.$$
Покажем, что при указанных условиях, эта последовательность стремится к решению системы. Прежде всего заметим, что матрица $E-A$ обратима, так как $0$ не является её собственным числом, и, следовательно, система имеет единственное решение. Пусть это решение --- $x'$. Обозначим $h_i=x_i-x'$. Для $h_i$ получается соотношение:
$$x'+h_i=Ax'+Ah_{i-1}+b.$$
Откуда получаем, что $h_i=Ah_{i-1}$. Но тогда $h_n=A^n h_0$ стремятся к нулю при $n\to \infty$. Значит $x_n\to x'$.


Можно показать, что этот метод устойчив, если имеет место сходимость. Кроме того, заметим, что каждый шаг итерации вычисляется за $O(n^2)$ или даже меньше, если в матрице $A$ много нулей.

Есть более интересные вариации на тему метода итераций. Например, метод Гаусса-Зейделя. Оказывается, что совершенно не обязательно смотреть на представление системы именно в виде $x=Ax+b$. Пусть матрица $A$ есть сумма $A=L+D+U$ (нижнетреугольная -- диагональная -- верхнетреугольная). Представим систему $Ax=b$ в виде 
$$(L+D)x=-Ux+b$$
Если мы определим последовательность $x_i$ по правилу $(L+D)x_{i+1}=-Ux_i+b$, то за сходимость такой последовательности будут отвечать собственные числа матрицы $-(L+D)^{-1}U$. 

Метод Гаусса-Зейделя чаще сходится и скорость сходимости для него обычно более высокая, чем для простого метода итераций (но не всегда). Кроме того, так как матрица $(L+D)$ нижнетреугольная, то решение системы с такой матрицей требует $O(n^2)$ операций. То есть в итоге, каждый шаг стоит $O(n^2)$ операций как и в методе простых итераций.







\subsection{Нахождение собственных чисел и собственных векторов}

Хотелось бы уметь оценивать размер собственных чисел матрицы $A$. Можно ли это сделать? Оказывается, что можно. 


Пусть $A$ -- вещественная или комплексная квадратная матрица. Рассмотрим строки $A$. Зададим элементы $r_i=\sum_{j\neq i} |a_{ij}|$. Рассмотрим множество замкнутых кругов радиуса $r_i$ с центром в $a_{ii}$ на комплексной плоскости. Эти круги называются кругами Гершгорина. 

\thrm Все собственные числа матрицы $A$ лежат в объединении кругов Гершгорина. 
\ethrm
\proof Пусть $\lambda$ -- собственное число $A$, а $v$ -- соответствующий собственный вектор. Тогда $Av=\lambda v$. Посмотрим на максимальный по модулю элемент $v$ -- $v_i$ и соответсвующую ему строчку. Покажем, что $\lambda$ лежит в круге Гершгорина с номером $i$. Имеем $\sum a_{ij}v_j =\lambda v_i$  и, значит, $\sum_{j\neq i} a_{ij}v_j= (\lambda - a_{ii})v_i $. Имеем неравенство $$|\lambda -a_{ii}||v_i| \leq |v_i|\sum_{j\neq i} |a_{ij}|$$
Сокращая на $v_i$ получаем требуемое. 
\endproof

\rm В частности, это рассуждение даёт критерий невырожденности матрицы -- матрица $A$ невырождена, если сумма модулей её внедиагональных элементов каждой строки меньше модуля диагонального элемента. Такие матрицы называются матрицами с диагональным преобладанием
\erm

\rm
Переходя от матрицы к транспонированной можно получить ещё одну оценку, которая получается из рассмотрения столбцов $A$.
\erm

А можно ли точно сказать, в каком из кругов находится собственное число? Оказывается, что иногда можно. Для того чтобы это показать нам нужна некоторая аналитическая подготовка.

\fct[Корни непрерывно зависят от коэффициентов] Пусть $f(x,t)$ -- комплексный многочлен от двух переменных со старшим коэффициентом $1$ при $x^n$, как многочлена над $\C[t]$. Пусть $\lambda_1,\dots,\lambda_n$ -- корни $f(x,0)$, а $\mu_1,\dots, \mu_n$ -- корни $f(x,1)$. Тогда cуществуют такие непрерывные на $[0,1]$ функции $\lambda_i(t)$, что $f(\lambda_i(t),t)=0$ и $\lambda_i(0)=\lambda_i$, а $\lambda_i(1)=\mu_{\sigma(i)}$ (для некоторой перестановки $\sigma \in S_n$).
\efct

\thrm Если $k$ кругов Гершгорина матрицы $A\in M_n(\C)$ не пересекаются с остальными, то в них лежат $k$ собственных чисел с учётом кратности.
\ethrm
\proof Рассмотрим матрицу $A_t= D+ tB$, где $D$ -- это матрица из диагональных элементов матрицы $A$, а $B$ -- из внедиагональных. Применим факт для характеристического многочлена матрицы $A_t$. Пусть  $\lambda_1(t),\dots,\lambda_k(t)$ -- непрерывные функции, соответствующие выбранным кругам Гершгорина $B_1,\dots,B_k$. Покажем, что каждая из них не вышла за пределы объединения этих кругов в момент $t=1$. 

Пусть скажем $\lambda_1$ при $t=1$ оказался в другом круге. Круги Гершгорина замкнуты. Пусть 
$$r=\rho(B,\cup_{i=1}^k B_i),$$
где $B$ -- это объединение оставшихся кругов. Рассмотрим функцию $f(t)=\rho(\lambda_1(t),B)$,  Это непрерывная по $t$ функция при этом $f(1)=0$. Значит есть такое $t$, что $0<f(t)<r$. С одной стороны, эта точка должна лежать в кругах Гершгорина, но с другой, из определения $r$ следует, что она должна лежать вне всех кругов.  
\endproof

Как это применить к оценки возмущения собственных чисел? Предположим, что $A$ диагонализуема:
$$A=C\pmat \lambda_1 && \\ & \ddots & \\ && \lambda_n \epmat C^{-1}=CDC^{-1}.$$
Рассмотрим сумму $A+\Delta A$. Как оценить собственные числа этой матрицы, через собственный числа $A$?

Пусть $\| \Delta A\|=\eps$. Рассмотрим матрицу
$$C^{-1}(A+\Delta A) C= D+C^{-1}\Delta A C $$
Надо оценить её собственные числа. Для этого заметим, что позиции  $i,j$ матрицы $C^{-1}\Delta A C$ стоит элемент размера не более $\| C^{-1}\| \|C\| \eps$. По теореме о кругах Гершгорина собственные числа $A+\Delta A$ находятся в кругах радиуса $n \| C^{-1}\| \|C\| \eps $ c центрами в $\lambda_i$. Кроме того, для достаточно маленького $\eps$ пары таких кругов не пересекаются или вложены друг в друга. Отсюда

\crl Для достаточно маленького возмущения $\Delta A$ в круге радиуса  $n \| C^{-1}\| \|C\| \|\Delta A\|$ с центром в с.ч.  $\lambda$ матрицы $A$ находится ровно $k$ собственных чисел $A+\Delta A$, где $k$ -- это кратность $\lambda$ у $A$.
\ecrl

Отсюда видно, что  роль, аналогичную числу обусловленности, в оценке на собственные числа играет число $\|C\|\|C^{-1}\|$. На самом деле для каждого собственного числа можно ввести своё число обусловленности. В любом случае, видно, что оценить это число заранее не находя матрицу $C$ не получится. Тем не менее что-то сделать можно. Для произвольной системы это число уже можно оценить, если мы приближённо вычислили $C$ и $C^{-1}$. Однако, есть матрицы для которых это число и так наилучшее возможное. Что это за матрицы?


Раньше, для того, чтобы найти спектр оператора мы считали характеристический многочлен. Однако, его вычисление довольно трудоёмко. Кроме того, после этого необходимо найти ещё и корни характеристического многочлена.


Однако, есть ряд других методов нахождения собственных чисел. С первой идеей мы уже знакомы -- это вариация метода итераций: рассмотрим случайный вектор $x$. Посмотрим на последовательность $A^kx/\|A^kx\|$ $k\to \infty$. Она стремится к максимальному собственному вектору для почти любого $x$. Легко оценить и собственное число.

Если предположить симметричность матрицы $A$. Заметим, что  для почти любого $x$ 
$$\frac {\lan A^{k+1}x, A^k x \ran}{\lan A^k x, A^k x \ran} \to \lambda, $$
где $\lambda$ -- максимальное собственное число.

Есть ещё множество методов для нахождения собственных чисел. 

Есть и метод нахождения всех одновременно собственных чисел: $QR$-алгоритм. Он заключается в следующем: вначале возьмём $A_0=A$. На $k$-ом шаге для матрицы $A_{k-1}$ строится $QR$-разложение $A_{k-1}=Q_kR_k$, а затем строится матрица $A_k=R_kQ_k$. Оказывается, что этот процесс в случае симметричной $A$ сойдётся к диагональной матрице, чьи диагональные элементы  и есть собственные числа $A$. 

Почему этот алгоритм вообще работает и как до него догадаться.

\lm Имеет место соотношение $A_k=Q_k^\top A_{k-1} Q_k$.
\elm

Определим $Q^{(k)}=Q_1\dots Q_k$ и $R^{(k)}= R_k \dots R_1$. В частности, из предыдущей леммы получаем, что $$A_k=Q_k^\top \dots Q_1^\top A Q_1 \dots Q_k= {Q^{(k)}}^\top A Q^{(k)}.$$

\lm  Имеет место соотношение. 
$$A^k=Q^{(k)}R^{(k)}.$$
\elm
\proof Докажем по индукции. 
$$A^k=A A^{k-1}= Q^{(k)}A_k{Q^{(k)}}^\top Q^{(k-1)}R^{(k-1)} = Q^{(k)}A_k Q_k^\top R^{(k-1)}=Q^{(k)} R^{(k)}$$
\endproof
Для того, чтобы просто сформулировать и доказать результат о сходимости QR-алгоритма мы ограничимся положительно определёнными симметричными матрицами.

\thrm Пусть $A$ -- симметричная матрица с различными положительными собственными числами. Тогда столбцы $Q^{(k)}$ стремятся к собственным векторам $A$, а матрицы $A_k$ -- к диагональной матрице, где на диагонали расположены собственные числа $A$.
\ethrm
\proof  Пусть $u_1,\dots,u_n \in \R^n$ -- ортонормированный базис из собственных векторов $A$ упорядоченный по убыванию собственных чисел.
Разложим стандартный базисный вектор $e_j$ по базису $u_i$
$$e_j = \sum_i c_{ij} u_i $$
Применим матрицу $A^k$. 
$$A^k e_j = \sum_i c_{ij} \lambda_i^k u_i.$$
Предположим, что главные миноры матрицы $C$ не равны нулю.
Как мы уже доказали, матрица $Q^{(k)}$ -- есть результат ортогонализации (с точностью до $\pm 1$) столбцов матрицы $A^k$, то есть векторов $A^k e_j$. Обозначим столбцы $Q^{(k)}$ за $q^{(k)}_j$ Посмотрим на первый столбец $A^k$. Исходя из нашего предположения, $c_{11}$ не ноль и
$$q^{(k)}_1=\frac1{\|A^ke_1\|} A^k e_1 = \pm u_1 + o(1),$$
при $k\to \infty$. Меняя при необходимости направление $u_1$ на противоположенное избавимся от $\pm$ в формуле. Посмотрим, что происходит при ортогонализации второго столбца. Вычислим скалярное произведение:
$$\frac{\lan A^k e_2, A^k e_1 \ran}{\lan A^k e_1, A^k e_1 \ran} = \frac{c_{12}}{c_{11}}+ O\left( \lambda_2^{2k}/\lambda_1^{2k}\right)
=\frac{c_{12}}{c_{11}}+ o \left(\left(\lambda_2/\lambda_1 \right)^k\right)  $$
Вычитая получаем
$$ A^k e_2 - \left(\frac{c_{12}}{c_{11}}+ o \left(\left(\lambda_2/\lambda_1\right)^k\right)\right) A^k e_1= o(\lambda_2^k)u_1+ c_{22}\lambda_2^ku_2 - \frac{c_{12}}{c_{11}}c_{21}\lambda_2^k u_2 +o(\lambda_2^k).$$
Коэффициент $c_{22}-\frac{c_{12}}{c_{11}}c_{21}$ по условию не равен $0$. После нормировки этот вектор стремится к $\pm u_2$. Продолжая так далее, получим, что столбцы $Q^{(k)}$ сходятся к собственным векторам $A$ в порядке убывания собственных чисел. Тогда в $i,j$ элементе матрицы $A_k={Q^{(k)}}^\top A Q^{(k)}$  стоит
$$\lan q^{(k)}_i, Aq^{(k)}_j\ran \to \lan u_i, \lambda u_j \ran = \lambda_i \delta_{ij}.$$
Значит, матрица $A_k$ действительно сходится к нужной диагональной матрице.
\endproof



\section{Спектры графов}
С графом $G$ связаны несколько разных матриц. Как свойства этих матриц, их собственные числа отражаются в комбинаторных свойствах графа $G$ и может ли это помочь?
Приведём пример, как знание спектра графа даёт некоторые оценки на сложно вычисляемые величины.

\rm Граф $G$ является $k$-регулярным тогда и только тогда, когда вектор $(1,\dots,1)^\top$ является собственным вектором с собственным числом $k$ для $A(G)$.
\erm

\thrm Пусть $G$ -- $k$-регулярный граф. Тогда размер максимального независимого множества в графе оценивается как
$$\alpha(G)\leq n\frac{-\lambda_n}{k-\lambda_n}.$$
\ethrm
\proof Рассмотрим характеристический вектор $v$ для независимого множества размера $\alpha$. Имеем $v^{\top}Av=0$ и при этом $v^{\top}v=\alpha$. Так как граф $k$-регулярный, то у него есть нормированный собственный вектор $u_1=\frac{1}{\sqrt{n}}(1,\dots,1)$. Тогда $\lan v,u_1\ran = \frac{\alpha}{\sqrt{n}}$. Разложим вектор $v=c_1u_1 + \dots + c_n u_n$ по ортонормированной системе собственных векторов. Тогда из условия независимости получаем
$$0=v^{\top}Av=\sum c_i^2 \lambda_i= \lambda_1\frac{\alpha^2}{n}+ \sum_{i\geq 2} \lambda_i c_i^2\geq \lambda_1\frac{\alpha^2}{n}+ \lambda_n \sum_{i\geq 2} c_i^2.$$
Мы знаем, что $\sum c_i^2=\alpha$, откуда $\sum_{i\geq 2} c_i^2=\alpha - \frac{\alpha^2}{n}$. Итого 
$$0\geq \lambda_1\frac{\alpha^2}{n}+\lambda_n(\alpha- \frac{\alpha^2}{n})$$
Сокращая на $\alpha$ получаем 
$$(\lambda_1-\lambda_n)\frac{\alpha}{n}\leq -\lambda_n,$$
что эквивалентно нужному неравенству.
\endproof




\exm\\
1) Полный граф $K_n$ имеет спектр $n-1,-1,\dots,-1$. В полном соответствии с тем, что размер максимального независимого множества равен $1$.\\
2) Граф Петерсена имеет спектр 3, 1, 1, 1, 1, 1, -2, -2, -2, -2. Размер независимого множества в точности равен $4$.\\
3) Оценка точна в общем виде для графов Кнезера $K(n,r)$.


Из оценки на размер независимого множества можно вывести оценку на хроматическое число.

\zd Покажите, что $\alpha(G) \chi(G) \geq n$. Выведите отсюда, что для регулярного графа $\chi(G)\geq 1+ \frac{\lambda_1}{-\lambda_n}$. Покажите, что для графа Петерсена оценка точная.
\ezd

\rm Удивительно, но ровно та же оценка верна и для нерегулярных графов тоже.
\erm

\zd Покажите, что $\chi(G) \leq [\lambda_1]+1$ для любого графа $G$. (Адаптируйте классическое доказательство с максимальной степенью).
\ezd


\subsection{Дополнительно: ещё трюки и применения}

\thrm Рассмотрим граф $K_{10}$. Тогда его невозможно покрыть тремя копиями графа Петерсена.
\ethrm
\proof Пусть граф $K_{10}$ покрыт тремя графами Петерсена. Тогда его матрица смежности удовлетворяет соотношению 
$$A(K_{10})=P_1+P_2+P_3.$$
Для нашего удобства перепишем это в виде 
$$-A(K_{10})=-P_1-P_2-P_3.$$
Заметим, что вектор $(1,\dots,1)^\top$ -- собственный для всех трёх матриц. Значит мы может ограничить указанное равенство на его ортогональное дополнение $U$.

Мы находимся в 9-тимерном пространстве. Воспользуемся неравенствами на собственные числа суммы (которые мы не доказывали). Тогда $i$-ое собственное число  суммы трёх матриц $A,B,C$ может быть оценено как $\lambda_t +\mu_s+ \nu_r$, где $r+s+t=i+2$. У операторов $-P_i|_U$ собственные числа равны $$2,2,2,2,-1,-1,-1,-1,-1.$$
Возьмём  $r=s=5$ и $t=1$. Получим оценку на $9$-ое собственное число $-A(K_{10})|_U$. Но мы его знаем: это $1$. С другой стороны получается, что оно должно быть меньше $0$. Противоречие! 
\endproof

Кроме оценок на хроматическое число и покрываемость при помощи спектров можно доказать негамильтоновость графа, оценивать его рёберное хроматическое число переходя к рёберному графу и т.д.

\zd Пусть $B$ -- матрица инцидентности графа $G$. Покажите, что $B^\top B - 2 E$ есть матрица смежности для рёберного графа $G$, а $B B^\top$ есть $A(G)+D$, где $D$ -- это матрица со степенями вершин графа $G$ на диагонали. 
\ezd

\zd Покажите, что в рёберном графе графа Петерсена нет индуцированного цикла длины 10 и выведите отсюда, что граф Петерсена негамильтонов.
\ezd

Есть, однако, две темы, связанные с собственными числами графов, которые приводят к непосредственным применениям. К сожалению, как часто бывает, у нас нет шансов внимательно посмотреть результаты этих разделов, поэтому мы ограничимся лишь очерчиванием их границ.




\dfn Для $A$ -- подмножества $V(G)$ определим $\partial A$ как множество всех рёбер ведущих из $A$ в вершины не из $A$. Константа Чигера или константа расширения графа  $G$ это 
$$h(G) := \min \left\{ \left. \frac{| \partial A |}{| A |} \right|   A \subseteq V(G), 0 < | A | \leq \tfrac{1}{2} | V(G)| \right\} .$$
\edfn

Вычислить константу Чигера очень сложно. Однако её можно оценить.

\begin{thmm}[Неравенство Чигера] Пусть $G$ -- $d$-регулярный граф и $\lambda_2$ -- его второе по максимальности собственное число. Тогда
$$\tfrac{1}{2}(d - \lambda_2) \le h(G) \le \sqrt{2d(d - \lambda_2)}.$$
\end{thmm}



\dfn Семейство графов $G_n$ называется $(n,d,\alpha)$ (алгебраическим) экспандером, если $G_n$ -- $d$-регулярный граф на $n$ вершинах с $\lambda=\max(|\lambda_2|,|\lambda_n|)\leq \alpha d$.
\edfn

Где могут применяться такие графы:\\
1) Коды, исправляющие ошибки\\
2) Псевдослучайные генераторы\\
3) Дерандомизация

Несмотря на то, что большая часть регулярных графов степени $d$ обладает указанными свойствами  до некоторого времени не было ни одного явного примера семейства экспандеров. Первый пример семейства экспандеров дал Григорий Маргулис. \\


\exm\\
1) Рассмотрим $\Z/n \times \Z/n$ и проведём для пары $(x,y)$ ребро в $(x \pm 2y,y), (x \pm (2y+1),y), (x,y \pm 2x), (x,y \pm (2x+1))$. Получается граф-экспандер с $\lambda \leq 5\sqrt{2}$\\
2) Можно подметить, что рёбра экспандеров из предыдущего примера связаны с некоторыми обратимыми преобразованиями, то есть с какой-то группой. Это общее место для конструкции большинства экспандеров.



\dfn Определим Лапласиан графа как $L(G)$ как $D-A(G)$.
\edfn

\zd $L(G)=B B^\top$, где $B$ -- матрица инцидентности произвольным образом ориентированного графа $G$.
\ezd

\zd $L$ -- неотрицательно определена и  $\rk L= \rk B=n-c$, где $c$ -- это количество компонент связности графа $G$.
\ezd



Кроме собственно графов можно рассматривать взвешенные графы. Понятно, как для них определить матрицы $D$, $A(G)=W(G)$ -- матрица весов, $L(G)$. Кроме этих матриц нам понадобится ещё и нормализованная матрица Лапласа.


\dfn Рассмотрим матрицу $D^{-1/2} L(G) D^{-1/2}$. Это нормализованный Лапласиан.
\edfn

На основе нормализованного Лапласиана можно построить алгоритм фрагментации изображений. Как и любая естественная задача она не имеет чёткой формулировки. В 2000 году Jianbo Shi и  Jitendra Malik \href{https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf}{предложили} следующую версию формулировки задачи: по картинке строится взвешенный граф $G$  -- его множество вершин $V$ -- это пиксели, рёбра проводятся, если вершины близки друг к другу и вес зависит от того, насколько пиксели похожи друг на друга.

Теперь рассматриваются все подмножества пикселей $A\subseteq V$ и минимизируется величина разреза, который отрезает $A$ от остального графа.
$$\min_{A} \frac{assoc(A,V\setminus A)}{assoc(A,V)}  + \frac{assoc(A, V\setminus A)}{assoc(V\setminus A,V)}, $$
$$\text{ где } \,\, assoc(A,B)=\sum_{i\in A, j\in B} w_{ij}$$.


Вопрос стоит в выборе оптимального $A$. Сопоставим множеству $A$ вектор $y$, такой что $$y_i = \begin{cases}-b=-\frac{\sum_{v\in A} d_v}{\sum_{v \not\in A} d_v}, \text{ если } i\not\in  A \\
1, i\in A 
\end{cases}$$

Оказывается, что в этом случае задача сводится к минимизации по всем таким $y$ выражения 
$$\frac {y^\top L(G) y} {y^\top D y } \text{ при этом выполнено } y D \bf 1 = 0.$$ 

Здесь $\bf 1$ -- это столбец из единиц. Решить эту дискретную задачу (допустимых $y$-ков конечное число) трудно, поэтому авторы переходят к непрерывной, отбрасывая ограничения на значения $y_i$. Минимум такого выражения достигается на $D^{-1/2}v$, где $v$ -- собственный вектор нормализованного Лапласиана для минимального положительного собственного числа ($D^{-1/2}\bf 1$ -- это собственный вектор для собственного числа $0$).

Теперь, найдя $v$ можно разделить точки по признаку положительности и отрицательности компонент $v$ (например).

\rm Похожий метод для матрицы Лапласа приводит к оценке связности графа и позволяет приближённо найти минимальный вершинный разрез графа дающий несколько компонент связности. (см. вектор Фидлера).
\erm



\section{Кватернионы}



Наша цель сейчас рассказать про геометрию трехмерного пространства используя при этом определённые алгебраические конструкции. А именно, ещё в XIX веке Уильям Роуэн Гамильтон стал искать аналогичную комплексным числам алгебраическую систему на трёхмерном пространстве.  
Однако, подходящий аналог удалось найти только в четырёхмерной ситуации.


Рассмотрим вещественное подпространство в алгебре матриц $M_2(\C)$ вида
$$\H = \left\{\pmat \alpha & \beta \\ -\ovl{\beta} & \ovl{\alpha} \epmat \right\}.$$
Базис этого пространства, как вещественного векторного пространства, состоит из матриц 
$$ 1=\pmat 1 & 0 \\ 0& 1 \epmat, i= \pmat i & 0 \\ 0& -i \epmat, j=\pmat 0& 1 \\ -1 & 0 \epmat, k=\pmat 0 & i \\ i & 0\epmat. $$ 
Покажем, что это вещественная подалгебра в $M_2(\C)$ и следовательно ассоциативное кольцо. 
Для этого достаточно показать, что произведение базисных снова лежит в $\H$. Имеем $$i^2=j^2=k^2=-1 \text{ и } ij=k=-ji,$$ откуда $$ik= iij=-j=jii=-ki \text{ и } jk=-jji=i=-kj.$$ Таким образом $\H$ образует ассоциативную алгебру размерности 4 над $\R$.
 
\dfn[Алгебра кватернионов] $\H$ называется алгеброй кватернионов. 
\edfn
Мы больше не будем думать (кроме одного нюанса) про кватернионы как про матрицы, а будем записывать их через $i,j,k$. Именно так обычно кватернионы и вводят -- как формальный суммы $a+bi+cj+dk$, для произведения которых выполнены тождества $i^2=j^2=k^2=-1$ и $ij=-ji=k$. Посмотрев на кватернионы как на матрицы мы сэкономили на доказательстве ассоциативности умножения.

\zd Алгебра кватернионов не снабжается структурой $\C$-алгебры.
\ezd






\dfn[Векторная и скалярная часть, сопряжённый кватернион] Пусть $x= a+bi+cj+dk$ кватернион. Определим вещественную или скалярную часть $\Re x=a$ и векторную часть $v= bi+cj+dk$ кватерниона. Сопряжённым кватернионом называется $\ovl{x}= a-bi-cj-dk= \Re x - \Im x =a-v$. 
\edfn



Посмотрим, как перемножаются кватернионы. Если оба кватерниона  $x$, $y$ разделить на скалярную и векторную части $x=a+v$, $y=b+u$ то $xy=ab+au+bv+ vu$. Нам осталось разобраться с умножением векторных частей. 

Рассмотрим произведение двух чисто мнимых кватернионов $uv=-\lan u,v\ran+[u,v]$. Его вещественная часть совпадает с минус скалярным произведением векторов. Про мнимую часть мы поговорим отдельно.

\dfn[Векторное произведение] Пусть $u,v \in \R^3$ два вектора. Тогда их векторным произведением называется вектор $[u,v]$.
\edfn

Если расписать в координатах $u=x_1i+x_2j+x_3k$ и  $v=y_1i+y_2j+y_3k$, то векторное произведение задаётся формулой

$$[u,v]= (x_2y_3-x_3y_2)i + (x_3y_1-x_1y_3)j + (x_1y_2- x_2y_1)k= \begin{vmatrix} i& j&k \\ x_1 & x_2 & x_3 \\ y_1 & y_2 & y_3 \end{vmatrix} $$

\rm Операция $(u,v) \to [u,v]$ является билинейной и антисимметричной, то есть $[u,u]=0$ и, следовательно, $[u,v]=-[v,u]$.
\erm

Последнее замечание позволяет нам легко вычислить $x \ovl{x}= a^2+ \lan v,v \ran$. Это приводит нас к определению:

\dfn[Норма кватерниона] Определим норму кватерниона как $$\|x\|=\sqrt{x\ovl{x}}=\sqrt{ a^2+b^2+c^2+d^2}=\sqrt{\ovl{x}x}.$$
\edfn 


Норма кватерниона, как и модуль комплексного числа всегда положительны для ненулевых элементов. Это позволяет заметить, что

\dfn[Обратный кватернион] Если $0\neq x \in \H$, то $x^{-1}=\frac{\ovl{x}}{\|x\|^2}$. 
\edfn

Таким образом мы получили первый (и для нас единственный) пример некоммутативного кольца с делением. Такие кольца называются телами. Напоминаю, что алгебра для нас ассоциативна и с единицей. Неассоциативные алгебры представляют интерес. Например, можно взять $\R^3$, где в качестве умножения взято векторное произведение. Это пример неассоциативной алгебры или, точнее, алгебры Ли. В этом курсе мы не  обсуждаем неассоциативные алгебры в связи с тем, что им обычно находится применение либо внутри физических дисциплин, либо внутри самой математики и редко где ещё. 

Какие ещё свойства есть у отображения нормы? Если следовать параллели с комплексными числами, то стоит посмотреть, что происходит с нормой произведения. Для того, чтобы не обременяться вычислениями сделаем небольшой трюк и на секунду вспомним матричное представление кватернионов. Заметим, что на матричном языке, норма -- это $\|x\|=\sqrt{\det x}$, откуда получаем

\lm[Норма мультипликативна] $\|xy\|=\|x\|\|y\|$. В частности, $\|x^{-1}\|=\|x\|^{-1}$.
\proof Вспомним в последний раз, что кватернионы задаются матрицами из $M_2(\C)$. Пусть 
$$x=\pmat \alpha & \beta \\ -\ovl{\beta} & \ovl{\alpha}\epmat.$$
Тогда $$\|x\|^2=|\alpha|^2+|\beta|^2=\det \pmat \alpha & \beta \\ -\ovl{\beta} & \ovl{\alpha}\epmat,$$
а определитель мультипликативен.
\endproof
\elm



Продолжим. Используя мультипликативность нормы легко доказать 
\lm Отображение $x \to \ovl{x}$ является антиизоморфизмом алгебр, то есть $\ovl{ab}=\ovl{b}\ovl{a}$.
\proof Линейной ясна. Пусть $x,y \neq 0$. Тогда $$\frac{\ovl{y}\,\ovl{x}}{\|y\|^2\|x\|^2}=y^{-1}x^{-1}=(xy)^{-1}=\frac{\ovl{xy}}{\|xy\|^2}.$$
\elm

На самом деле и здесь можно было воспользоваться матричным представлением. А именно, можно заметить, что операция сопряжения кватернионов совпадает на этом языке с транспонированием и сопряжением соответствующей комплексной матрицы. Вернёмся теперь к векторному произведению.





\lm[Свойства векторного произведения] Верны следующие свойства\\
1) Для любых $u,v \in \R^3$ верно $u\bot [u,v]$. Точнее $$u[u,v]= -\|u\|^2v+ \lan u,v\ran u$$
2) $\| [u,v]\|= \|u\|\|v\| \cdot |\sin \ffi |$, где $\ffi$ --  это угол между $u$ и $v$.
\elm
\proof Для того, чтобы посчитать скалярное произведение $\lan u, [u,v]\ran$ необходимо посчитать скалярную часть $u[u,v]$. 
$$u[u,v]= u (uv+ \lan u,v \ran)= u^2 v+ \lan u,v \ran u= -\|u\|^2 v+ \lan u,v \ran u$$
Последнее выражение, очевидно, чисто векторное.
Теперь
\begin{align*}
&\|[u,v]\|^2= -[u,v][u,v]= (uv + \lan u,v\ran)(vu + \lan u,v\ran)=\\
&=\|u\|^2\|v\|^2+ \lan u,v\ran^2 + \lan u,v\ran (uv+vu)=\|u\|^2\|v\|^2 - \lan u,v\ran^2= \|u\|^2\|v\|^2(1-\cos^2 \ffi)
\end{align*}
\endproof

\dfn Обозначим за $\H_{1}$ группу кватернионов, по норме равных единице.
\edfn

\thrm Отображение $\H_{1}\to \GL_3(\R)$ заданное по правилу $x\to (y \to xyx^{-1})$ корректно определено и даёт сюръективный  гомоморфизм из группы кватернионов единичной нормы в $\SO_3(\R)$. Ядро этого гомоморфизма состоит из $\{\pm 1\}$. Точнее, если единичный кватернион $x$  представим в виде $x=a+bv$, то соответствующее ему вращение есть вращение относительно  оси $\lan v \ran$ на угол $2\ffi$, где $\cos \ffi= a$, $\sin \ffi= b$ или тождественное преобразование в случае $v=\pm 1$.
\ethrm
\proof Рассмотрим преобразование $L_x \colon \H \to \H$ вида $y \to xyx^{-1}$. Прежде всего покажем, что мы получили ортогональное преобразование $\R^4$. Имеем
 $$\|xvx^{-1}\|=\|x\| \|v\| \|x^{-1}\| = \|v\|.$$
Теперь заметим, что преобразование $L_x$ сохраняет на месте вектор 1 и, следовательно, его ортогональное дополнение, то есть $\R^3$. Таким образом $L_x$ ограничивается на $\R^3$. Далее, очевидно, $L_xL_y= L_{xy}$. Осталось посчитать ядро гомоморфизма и явный вид отображения $L_x$. Заметим, что если $x=a+bv$, то $L_x$ оставляет $v$ на месте. Действительно, при $b\neq 0$ 
$$xbvx^{-1}=x(x-a)x^{-1}= x-a=bv.$$
Вычислим угол поворота. Для этого рассмотрим нормированный вектор  $u\bot v$ и $[u,v]$, которые образуют ортонормированный базис дополнения и посчитаем $xux^{-1}$ и $x[u,v]x^{-1}$. Из условия ортогональности следует, что $uv=[u,v]=-[v,u]=-vu$ и значит $v[u,v]=-[u,v]v=-uv^2=u$. Теперь
\begin{align*}
xux^{-1}&=(a+bv)u(a-bv)= (a+bv)(au-buv)=\\
&=a^2u -ab[u,v]+ab[v,u]- b^2vuv=a^2u-2ab[u,v]-b^2\|v\|^2u=\\ &=(a^2-b^2)u-2ab[u,v]=\cos2\ffi u+ \sin 2\ffi [u,v]
\\
\\
x[u,v]x^{-1}&=(a+bv)[u,v](a-bv)= (a[u,v]+bu)(a-bv)=\\
&=a^2[u,v]+abu-ab[u,v]v-b^2uv=\\
&=(a^2-b^2)[u,v]+2abu=\cos 2\ffi[u,v]+\sin 2\ffi u
\end{align*}

Осталось показать, что только $x=\pm 1$ лежит в ядре этого отображения. Это возможно только тогда, когда $2\ffi \equiv 0 \mod 2\pi$. Значит $\ffi=2 \pi$ или $\ffi= \pi$. Первое соответствует $x=1$. Втораое --  $a=\cos \ffi = -1$, а $b=\sin \ffi = 0$, то есть $x=-1$. 
\endproof


\zd
Покажите, что отображение $(x,y) \to (z \to xzy^{-1})$ задаёт сюръективный гомоморфизм из декартового квадрата группы единичных кватернионов в группу $\SO_4(\R)$ с ядром $\{(1,1),(-1,-1)\}$.
\ezd

Обсудим теперь для чего могут понадобится кватернионы. Каждый кватернион, как мы установили кодирует вращение трёхмерного пространства или, что тоже самое -- новую декартову систему координат в $\R^3$ с центром в нуле. Такой тип данных встречается в компьютерной графике, если вы хотите зафиксировать ракурс, в котором вы смотрите на 3d-сцену или положение какого-то конкретного объекта в такой сцене.

В такой задаче кватернионы сложно превзойти. Действительно, задание сцены при помощи кватернионов очень экономно -- 4 коэффициента с одним соотношением на них (3 коэффициента, если очень нужно) против 9 коэффициентов у ортогональной матрицы.

А что с эффективностью операций? Тут вопрос состоит в том, про какие операции идёт речь. Если вы хотите сказать, что тот или иной вектор, который был повёрнут на кватернион $q=a+v$ надо повернуть ещё на кватернион $p=b+u$, то вам надо всего лишь вычислить $pq=ab+au+bv+uv$. Такое произведение считается за 16 умножений и 12 сложений. Если брать произведение матриц $3\times 3$, то там получается 27 умножений и 18 сложений (можно обойтись и 23 умножениями, но сильно увеличив число сложений).

Можно конечно использовать углы Эйлера, но тогда придётся использовать для вычислений косинусы и синусы этих углов, которые сами даются не бесплатно.

Если вам даны два ракурса в виде кватернионов, то легко понять, на какой кватернион надо домножить, чтобы из первого получить второй.

Но что если вы хотите просто повернуть при помощи кватерниона какой-то вектор из $\R^3$. Тут ситуация несколько хуже. Для этого вам необходимо посчитать $qxq^{-1}$. Предположив, что $q$ нормирован можно заменить обращение на сопряжение. Тем не менее такой подход довольно дорог -- 32 умножения и 24 сложения. Конечно, такой способ не оптимален. Например, не обязательно считать скалярную часть -- она должна стать нулевой. На самом деле, если $q=a+v$, то 
$$qxq^{-1}=x+ 2[v, [v,x]+ ax]$$
что даёт 15 умножений и 15 сложений (если считать умножение на 2 сложением). Это конечно отличается от матриц, где необходимо 9 умножений и 6 сложений.

Впрочем, отличается не сильно. Кроме того у кватернионного представления есть плюс произведение нескольких кватернионов -- это кватернион несмотря на ошибки округления. Что не так для ортогональных матриц.

Наконец, представим себе задачу, что нам нужно плавно перейти от ракурса $q$ к ракурсу $p$. Желательно с "равномерной" скоростью. На языке кватернионов это становится понятно. Для этого заметим, что единичные кватернионы -- это всего лишь точки на трёхмерной сфере в четырёхмерном пространстве. Несложно понять, что их соединяет часть дуги сферы, равномерное движение по которой и приводит к желаемой смене ракурса. Теперь несложно параметризовать равномерное движение по этой дуге. 
$$ \frac{\sin(t\theta)p + \sin ((1-t)\theta) q}{\sin \theta},$$
где $\theta$ -- угол между $p$ и $q$ (острый, не забываем, что представление кватернионами вращений немного не однозначно).





\section{Двойственное пространство}

Начнём наше триумфальное возвращение из мира вещественных чисел в мир произвольных полей. В дальнейшем $K$ обозначает произвольное поле. 

\dfn Определим двойственное пространство к пространству $V$ как $V^*=\Hom (V, K)$.
\edfn

\dfn Пусть $e_1,\dots,e_n$ -- базис $V$. Определим $e^1,\dots,e^n$ как базис пространства $V^*$, заданный следующим соотношением: 
$$e^j(e_i)=\delta_{ij}.$$
Иными словами, $e^i$ -- это $i$-ая координатная функция в базисе $e$.
\edfn

\utv Пусть $V$ -- векторное пространство над полем $K$. Тогда имеет место естественный изоморфизм пространств $V\to {V^*}^*$.
\eutv
\proof Построим отображение по правилу $v \to (f \to f(v))$. Это отображение инъективно. Из равенства размерностей следует, что этого и достаточно.
\endproof

Если мы перешли от одного базиса к другому в пространстве $V$, то что произошло в $V^*$? 

\thrm Пусть $e_1,\dots,e_n$ старый базис $V$, а $\hat{e}_1,\dots,\hat{e}_n$ -- новый. Пусть $C$ -- матрица перехода из старого базиса в новый. Тогда матрица перехода из базиса $e^1,\dots,e^n$ в базис $\hat{e}^1,\dots,\hat{e}^n$ есть ${C^{\top}}^{-1}$.
\proof Вспомним нашу конвенцию: матрица перехода $C$ это, столбцы которой составлены из координат нового базиса в старом базисе. Если у нас есть вектор $v$, то это условие означает, что его новый столбец координат $y$ и старый столбец координат связаны уравнением $x=Cy$. Переписывая получаем соотношение 
$$C^{-1}x=y.$$
Но это есть соотношение между координатными функциями в новом базисе и в старом базисе, то есть, между векторами двойственных базисов. Почти то, что нужно. Осталось из этого соотношения найти матрицу перехода. Для этого запишем $$y_i=\sum_j (C^{-1})_{ij}x_j$$
Значит, элементы $(C^{-1})_{ij}$ должны стоять в $i$-ом столбце матрицы перехода из $\hat{e}$ в $\hat{e}^*$. Это значит, что эта матрица равна ${C^{-1}}^{\top}$.
\endproof
\ethrm

В особые отношения со своим двойственным пространством вступают евклидовы и унитарные пространства. 

\utv Пусть $V$ -- евклидово или унитарное пространство. Тогда любой линейный функционал имеет вид $\lan v, \_ \ran$ для единственного вектора $v$. 
\eutv
\proof Пусть $f$ -- линейный функционал. Рассмотрим $U=\Ker f^\bot$. Это одномерное пространство. Рассмотрим $v\in U$ отличный от нуля и подгоним при помощи константы так, чтобы  $f(v)=\lan v,v \ran$. Этого достаточно благодаря линейности каждого выражения.  
\endproof

\section{Немного про квантовую механику}

Вкратце напомню парадигму квантовой механики. Физическая система в квантовой механике задаётся при помощи некоторого унитарного пространства $V$. Каждая прямая этого унитарного пространства задаёт некоторое возможное физическое состояние системы. На каждой прямой есть вектор, по норме равный $1$. Итого, каждый вектор, равный по норме $1$ описывает некоторое физическое состояние. 

Квадрат модуля скалярного произведения двух таких нормированных векторов  $|\lan u,v \ran|^2$ интерпретируется как вероятность того, что  $v$ при измерении будет находится в состоянии $u$ (если вы можете померить, что что-то находится именно в состоянии $u$). Таким образом, каждый вектор $u\in V$ задаёт линейный функционал $\lan u, \_ \ran $ на $V$, который по состоянию $v$ выдаёт информацию о вероятности найти $v$ в состоянии $u$.

Каждой измеримой физической величине соответствует самосопряжённый оператор $T$ на $V$. Пусть $\psi_i$ -- это ортонормированный базис из собственных векторов $T$ с собственными числами $\lambda_i$. Тогда вероятность, что находясь в состоянии $v$ при измерении величины $T$ мы получим $\lambda_i$ есть вероятность найти $v$ в состоянии $\psi$, то есть $|\lan \psi_i, v \ran|^2$. Несложно заметить, что, как и должно быть, сумма по всем возможным $i$ этих вероятностей даёт $1$. Такая конструкция объясняет квантованность (дискретность) значений разных физических величин, которая возникает при измерении.

Типичным примером квантовой системы является нерелятивистская (без учёта теории относительности) модель одной частицы в $\R^3$. В этом случае роль унитарного пространства играет 
$$V= L_2(\R^3)=\left\{  f\colon \R^3 \to \C\, \Big| \, f - \text{ измерима и } \int_{\R^3} |f|^2<\infty \right\}.$$
Технически тут ещё понадобится отождествить функции отличающиеся друг от друга на множестве меры ноль, но я это опускаю. Скалярное произведение таких функций задаётся привычной формулой 
$$\lan f,g \ran = \int_{\R^3} \ovl{f}g\, dx.$$
Состояние квантовой системы, соответствующее нормированной функции $f\in V$ (т.е. $\|f\|^2=1$)  имеет следующую интерпретацию: $\int_{B} |f|^2$ есть вероятность для частицы оказаться в области $B$. 

 

В квантовых вычислениях основным объектом является квантовый бит или кубит (qubit) -- это двумерная квантовая система. Первый её базисный вектор обозначается $\left|0\ran$, а второй $ \left|1\ran$. Предполагается, что это собственные вектора оператора энергии т.е. два эти состояния различаются при измерении энергии. Произвольное состояние (нормированное) есть смесь 
$$c_1 \left|0\ran+ c_2 \left| 1\ran, \text{ где } |c_1|^2+|c_2|^2=1.$$
 

Допустим, у нас есть две независимые системы с пространствами $V_1$ и $V_2$. Какое пространство соответствует  объединению двух таких систем? Обозначим это пространство за $V_1\otimes V_2$. Посмотрим как должны выглядеть состояния этого пространства. Понятно что для каждой пары состояний $u_1\in V_1$ и $u_2\in V_2$ должно существовать своё состояние объединённой системы. Обозначим его за $u_1 \otimes u_2$. Другими словами должно быть задано отображение $i\colon V_1\times V_2\to V_1 \otimes V_2$.

Пусть есть одна пара $(u_1,u_2)$ и вторая пара $(v_1,v_2)$. Какова вероятность того, что при измерении система в состоянии $u_1\otimes u_2$ перейдёт  в состояние $v_1 \otimes v_2$? Логично, чтобы это было $|\lan u_1,v_1 \ran \lan u_2, v_2\ran|^2$.

Какому скалярному произведению на таких парах это соответствует? Должно быть 
$$\lan u_1\otimes u_2, v_1\otimes v_2\ran = \lan u_1,v_1 \ran \lan u_2, v_2\ran.$$
То есть, скалярное произведение с состоянием $u_1\otimes u_2$ есть не линейная функция от пары $(v_1, v_2)$, а билинейная!



Линейные отображения можно складывать между собой. Сумме линейных отображений, заданных при помощи $u_1\otimes u_2$ на $V_1\otimes V_2$ должна соответствовать сумма билинейных форм на $V_1\times V_2$. Заметим, что любую билинейную форму можно представить в виде суммы форм заданных при помощи пар $(u_1,u_2)$. Таким образом, логично считать, что все линейные функционалы на $V_1\otimes V_2$ должны соответствовать билинейным формам на $V_1\times V_2$.





\section{Тензорное произведение}

Сейчас мы немного поговорили про пространство линейных функционалов, прошлом семестре мы подробно остановились на билинейных операциях. А квантовая механика дала нам необходимость в следующем определении:

\dfn Пусть дана пара пространств $U,V$ над полем $K$. Тогда их тензорным произведением называется пространство 
$U\otimes V$ вместе с билинейным отображением отображением
$$i \colon U \times  V \to U \otimes V,$$
удовлетворяющее условию что для любого билинейного отображения из $h\colon U \times V \to W$ существует единственное линейное отображение 
$$\hat{h}\colon U\otimes V \to W,$$
что 
$$\hat{h}\circ i=h.$$
Иными словами, отображение $i$ -- это <<универсальное>> билинейное отображение.
\edfn 


\lm Пусть $U\leq V$, а $\pi \colon V \to V/U$ -- это каноническая проекция на $V/U$. Пусть $L\colon V \to W$ -- линейное отображение. Тогда, для того, чтобы существовало $\hat{L}\colon V/U \to W$, что $L=\hat{L}\circ \pi$ необходимо и достаточно, чтобы $L(U)=\{0\}$.
\elm





\thrm Пусть $U,V$ -- пара векторных пространств. Тогда имеет место следующая конструкция тензорного произведения:
$$U \otimes V \cong K\lan U \times V \ran / Rel,$$
где $Rel$ -- это подпространство порождённое формальными суммами
$$(\lambda u_1+u_2, v) - \lambda (u_1, v) - ( u_2,v) \text{ и } (u,\lambda v_1+v_2) - \lambda (u,v_1) - (u,v_2).$$ 
\proof Для удобства обозначим пространство
$$T=K\lan U \times V \ran / Rel.$$ Будем обозначать образы элементов $(u,v)$ в $T$ как  $u\otimes v$. Отображение $$i \colon U\times V \to T$$
заданное правилом  $(u,v) \to u \otimes v$
билинейно по самому определению соотношений из $Rel$. Пусть теперь дано пространство $W$ и билинейное отображение $$h \colon U \times V \to W.$$
Построим отображение $\hat{h}$ следующим образом: сначала определим $\hat{\hat{h}}\colon K\lan U \times V \ran \to W$, а затем покажем, что оно пропускается через $T$. По самому своему определению $K\lan U \times V\ran$ имеет базисом элементы $(u,v)$. Отображение $\hat{\hat{h}}$ достаточно задать на них. Положим $$\hat{\hat{h}}((u,v))=h(u,v).$$
Покажем, что оно однозначно пропускается через $T$. Как всегда единственность очевидна. Для того чтобы показать, что $\hat{\hat{h}}$ пропускается через $T$ необходимо показать, что все соотношения лежат в ядре $\hat{\hat{h}}$. Но это так, потому что $h$ билинейно! 
\endproof
\ethrm




Мы показали, что тензорное произведение существует. Неплохо бы понять, что тензорное произведение однозначно определено с точностью до изоморфизма. 


\utv Если тензорное произведение существует, то оно единственно с точностью до изоморфизма. Конкретно, пусть $U,V$ -- векторные пространства и есть два пространства $T_1,T_2$ и $i_1 \colon U\times V \to T_1$ и $i_2\colon U\times V \to T_2$, играющие роль тензорного произведения, то существует единственный изоморфизм $h\colon T_1\to T_2$, что $h\circ i_1=i_2$.
\proof Это типичное <<категорное>> доказательство. Пусть есть два пространства $T_1$ и $T_2$ вместе с отображениями $i_1 \colon U\times V \to T_1$ и $i_2 \colon  U\times V \to T_2$, для которых выполнены аксиомы тензорного произведения. Заметим, что так как $i_2$ полилинейное, то есть такое $\hat{i_2} \colon T_1 \to T_2$, что $$\hat{i_2}\circ i_1= i_2.$$ 
Аналогично существует $\hat{i_1} \colon T_2 \to T_1$, что
$$\hat{i_1}\circ i_2= i_1.$$ 
Покажем, что $\hat{i_1} $ и $\hat{i_2}$ взаимно обратны. Действительно, имеем
$$\hat{i_1}\circ \hat{i_2}\circ i_1= \hat{i_1}\circ i_2= i_1,$$
что означает, что отображение $\hat{i_1}\circ \hat{i_2}$ есть то самое единственное отображение $T_1\to T_1$, которое гарантируется благодаря полилинейности $i_1$ и того, что само $T_1$ -- тензорное произведение. Но есть другой кандидат на эту роль -- это $\id_{T_1}$. По единственности 
$$\hat{i_1}\circ \hat{i_2}=\id_{T_1}.$$
Аналогично проверяется равенство для второй композиции.
\endproof
\eutv





Теперь необходимо посчитать что-то про тензорное произведение. Например, научиться считать размерность тензорного произведения и находить его базис.
\thrm Пусть $e_1,\dots,e_n$ базис $U$, а $f_1,\dots,f_m$ -- базис $V$. Тогда $e_i\otimes f_j$ базис $U \otimes V$. В частности, 
$$\dim U \otimes V= \dim U \cdot \dim V.$$ 
\proof Прежде всего заметим, что набор $e_i \otimes f_j$ является порождающей системой для тензорного произведения. Значит оно на самом деле конечномерно. Далее, по определению тензорного произведения,
$$\Hom(U;V, K) \simeq \Hom(U\otimes  V,K).$$
Размерность последнего пространства совпадает с размерностью $U\otimes V$. С другой стороны, полилинейное отображение $h \in \Hom(U,V, K)$ однозначно задаётся $\dim U \cdot \dim V$  параметрами $h(e_i,f_j)$. Комбинируя эти два факта получаем, что размерность $\dim U \otimes V$ есть $\dim U \cdot \dim V$. Отсюда, любая порождающая система такого размера есть базис. В частности, набор $e_i \otimes f_j$.
\endproof
\ethrm


\rm Пространство состояний квантовой системы из $n$ кубит имеет размерность $2^n$. Это ключевой факт, который проясняет потенциальную эффективность квантовых компьютеров: вместо $n$ бит мы имеем дело с $2^n$ чисел -- координатами вектора из $2^n$-мерного пространства.
\erm


Определим теперь тензорное произведение линейных отображений.

\dfn Пусть дана пара линейных отображений $L \colon U_1 \to V_1$ и $S\colon U_2 \to V_2 $. Определим отображение $$L\otimes S \colon U_1\otimes U_2 \to V_1\otimes V_2$$ по  правилу $$(L \otimes S) (u_1\otimes  u_2) = L(u_1)\otimes S(u_2).$$
Отображение с таким свойством единственно.
\edfn

\rm Указанное отображение существует и единственно. Действительно,  отображение $L\otimes S$ должно соответствовать билинейному отображению $L\times S$ заданному правилом $L\times S ((u_1,u_2)) = L(u_1)\otimes S(u_2)$. 
\erm

\rm Пусть заданы наборы линейных отображений $L_1, L_2$ и $S_1, S_2$, так что определены композиции $L_i\circ S_i$. Тогда
$$(L_1\otimes S_1) \circ (L_2 \otimes S_2)=(L_1\circ L_2)\otimes (S_1\circ S_2).$$
\erm 

А как устроена матрица тензорного произведения линейных отображений?


\lm Пусть $L_1 \colon V_1 \to U_1$, а $L_2 \colon V_2 \to U_2$. Пусть $e_1,\dots, e_{n_1}$ базис $V_1$,  $e_1',\dots, e_{n_2}'$ базис $V_2$,  и $f_1,\dots, f_{m_1}$ -- базис $U_1$, а $f_1',\dots, f_{m_2}'$ -- базис $U_2$. 
Упорядочим базисы тензорных произведений -- удобно это сделать, например, в лексикографическом порядке (номер первой координаты важнее).
Тогда матрица  $L_1\otimes L_2$  разобьётся на $n_1m_1$ блоков в каждом из которых будет стоять $ A_{ij} B$, где $i,j$ -- номер блока, а $A$ и $B$ матрицы $L_1$ и $L_2$ соответственно.
\proof Действительно, отображение $A\otimes B$ отправляет $e_i\otimes e'_j$ в 
$$Ae_i \otimes Be'_j = \sum_{k,l} A_{ki}B_{lj} \, f_k\otimes f'_l.$$
\endproof
\elm

\dfn Такая матрица называется кронекеровым или тензорным произведением матриц $A$ и $B$ и обозначается как $A\otimes B$.
\edfn

А что если стартовать с операторов, а не с линейных отображений?

\rm Если есть операторы $A\colon V \to V$ и $B \colon W \to W$. Тогда задан оператор $A\otimes B$ на $V\otimes W$.
\erm

\lm Собственные числа оператора $A\otimes B$  -- это попарные произведения собственных чисел для $A$ и $B$. 
\proof Перейдём к матрицам и, если нужно, будем считать, что мы находимся над алгебраически замкнутым полем. Рассмотрим жорданов базис для $A$ и $B$ -- $v_1,\dots,v_n$ и $u_1,\dots, u_m$. Тогда, рассмотрев базис $v_i\otimes u_j$, заметим, что под диагональю будут стоять нули, а на диагонали -- попарные произведения собственных чисел $A$ и $B$.
\endproof
\elm


 

\dfn[Произведение графов] Пусть $G$ и $H$ -- два графа (возможно ориентированных). Тогда их категорным произведением называется граф чьи вершины есть пары вершин $G$ и $H$ и ребро между парами $(u_1,v_1)$ и $(u_2,v_2)$ проводится только если есть рёбра $u_1 \to u_2$ и $v_1 \to v_2$.

Декартовым произведением графов $G$ и $H$ называется граф на тех же вершинах с ребром между парами если $u_1=u_2$ и есть ребро $v_1\to v_2$ или, симметрично, $v_1=v_2$ и есть ребро $u_1 \to u_2$. Разумеется для неориентированных графов эта конструкция снова выдаёт неориентированный граф.
\edfn


\crl Спектр категорного произведения графов состоит из всех возможных попарных произведений собственных чисел графов.
\proof Заметим, что матрица смежности категорного произведения графов -- это тензорное произведение матриц смежности исходных графов.
\endproof
\ecrl



\zd Чему равен спектр декартового произведения графов?
\ezd

\zd Чему равен спектр графа-решётки
\ezd

А можно ли как-то охарактеризовать тензорное произведение нескольких пространств?

\rm Если рассмотреть тензорное произведение $V_1\otimes \dots \otimes V_n$, то линейные отображения из него в $W$ будут соответствовать полилинейным отображениям $\Hom(V_1;\dots ;V_k, W)$.
\erm

С понятием тензорного произведения связан ряд канонических отождествлений между разными на первый взгляд пространствами в духе изоморфизма $V \simeq V^{**}$.

\thrm Имеют место следующие естественные изоморфизмы: 
$$(U \otimes V) \otimes W \simeq U \otimes V \otimes W \simeq U \otimes (V \otimes W)$$
$$ U \otimes V \simeq V \otimes U $$
$$ \Hom (U,V) \simeq  V \otimes U^*$$
$$(U \otimes V)^{*} \simeq U^{*}\otimes V^{*} $$
$$\Hom (U\otimes V,  W) \simeq \Hom (U, \Hom (V,W))$$
\proof Наиболее интересная часть этой теоремы состоит в бескоординатном построении этих отображений.

Построим отображение $V\otimes U^{*} \to \Hom (U,V)$ по правилу $$v\otimes f \to (u \to f(u)v).$$
Это соответствие полилинейно по $v,f$ поэтому задаёт корректное линейное отображение. Полезно посмотреть, как оно действует на базисных векторах. Пусть $e_i$ базис $V$, $f_j$ -- базис $U$, а $f^j$ базис $U^{*}$. Тогда $e_i\otimes f^j$ соответствует линейное отображение с матрицей 
$$ e_{ij}= \bordermatrix{
 & &j&& \cr
 &0&\cdots&\cdots&0\cr
 &\vdots&\ddots && \vdots\cr
i&\vdots& 1 & \ddots& \vdots\cr
 &0&\cdots& \cdots&0
}$$
Такие линейные отображения образуют базис $\Hom(U,V)$. Значит указанное отображение -- изоморфизм.

\endproof 
\ethrm


\subsection{Тензоры на пространстве и их классическое определение}

Понятие тензора появилось в классической механике для ответа на вопрос, как меняются те или иные величины при замене координат. Для того, чтобы обсудить этот аспект тензоров дадим определение:

\dfn Тензором валентности $(p,q)$ на пространстве $V$ называется элемент пространства ${V^{*}}^{\otimes p} \otimes V^{\otimes q}$. Так же будем говорить, что такие элементы -- это $p$ раз ковариантные и $q$ раз контравариантные тензоры. Тензорами валентности $(0,0)$ называются элементы поля $K$ -- скаляры.
\edfn

Теперь я утверждаю, что более менее все встречавшиеся нам структуры на векторном пространстве $V$ являются тензорами.



\exm\\
1) Вектор $v\in V$ является 1 раз контравариантным тензором.\\
2) Элемент двойственного пространства $f \in V^{*}$ является 1 раз ковариантным тензором. Вообще ковариантными называют тензоры, которые соответствуют полилинейным формам на пространстве $V$. Это историческая традиция. Точнее:\\
3) Так как пространство ${V^{*}}^{\otimes p} \simeq \left(V^{\otimes p}\right)^*\simeq \Hom(V;\dots;V,K)$, то тензор валентности $(p,0)$ соответствует полилинейному отображению $V\times\dots \times V \to K$.\\
4) В частности, тензор валентности $(2,0)$ -- это билинейная форма.\\
5) Линейный оператор -- это элемент $\Hom(V,V)\simeq V^{*}\otimes V$, то есть тензор валентности $(1,1)$.\\
6) Структура алгебры на $V$ (без требования ассоциативности) задаётся билинейным отображением $V \times V \to V$, то есть линейным отображением $V\otimes V \to V$ или же элементом $V^{*}\otimes V^* \otimes V$, то есть тензором типа $(2,1)$.\\

Как записать тензор в координатах? Выберем базис $e_1,\dots,e_n$ пространства $V$ и возьмём в двойственном пространстве двойственный базис $e^1,\dots,e^n$. Теперь построим базис тензорного произведения ${V^{*}}^{\otimes p}\otimes V^{\otimes q}$. Он имеет вид $e^{j_1}\otimes\dots\otimes e^{j_p}\otimes e_{i_1}\otimes \dots \otimes e_{i_q}$. Тогда произвольный тензор $T$ валентности $(p,q)$ имеет вид 
$$ T= \sum_{\substack{i_1,\dots,i_q \in \ovl{1,n}\\ j_1,\dots,j_p \in \ovl{1,n}} } \,T_{j_1,\dots,j_p}^{i_1,\dots,i_q}\,\, e^{j_1}\otimes\dots\otimes e^{j_p}\otimes e_{i_1}\otimes \dots \otimes e_{i_q}.$$
Элементы $T_{j_1,\dots,j_p}^{i_1,\dots,i_q}$ называются координатами тензора $T$. Как меняются координаты тензора при замене базиса? 

\thrm Пусть $e_1,\dots,e_n$ старый базис $V$, а $\hat{e}_1,\dots,\hat{e}_n$ -- новый. Пусть $C$ -- матрица перехода из нового базиса в старый, а $D={C^{\top}}^{-1}$. Тогда координаты тензора $T$ в базисе $\hat{e}$ выражаются через старые координаты следующим образом:
$$\hat{T}_{j_1,\dots,j_p}^{i_1,\dots,i_q}=\sum_{\substack{i'_1,\dots,i'_q \in \ovl{1,n}\\ j'_1,\dots,j'_p \in \ovl{1,n}}} \,\,
\prod_{t\in \ovl{1,p}} D_{j_t,j'_t} \prod_{s\in \ovl{1,q}} C_{i_s,i'_s}  \,\,T_{j'_1,\dots,j'_p}^{i'_1,\dots,i'_q}.$$
\ethrm

Важность тензоров в теоретической физике обуславливается тем, что практически все физические объекты -- это тензоры. Точнее: с точки зрения теории относительности пространство-время это некоторое четырёхмерное многообразие $M$ (в двумерной ситуации подошла бы обычная сфера или тор). С каждой точкой $x$ этого многообразия связано касательное пространство в этой точке -- некоторое четырёхмерное пространство $T_x$. Представим себе, что в каждой точке пространства задана плотность вещества (на самом деле не так, но допустим) -- это даёт вам функцию $f \colon M \to \R$ -- скаляр в каждой точке, то есть тензор типа $(0,0)$. 

Направление движения материи можно задать взяв в каждой точке касательный вектор, то есть тензор валентности $(0,1)$ на $T_x$. Дальше, у каждого такого вектора можно считать его <<длину>> и углы между векторами. Для этого надо задать для каждой точки $x$ билинейную форму на касательном пространстве, то есть элемент $T_x^{(2,0)}$. И т.д. Чаще всего такие объекты называют тензорными полями, если хочется подчеркнуть, что в разных точках это тензор вообще говоря на разных пространствах.

Важно, что уравнения в физике не должны зависеть от выбора координат. Можно, конечно, писать какие-то уравнения при помощи координат тензоров и каждый раз проверять, что выбрав новые координаты уравнение будет того же вида. Однако, чем сложнее наука тем сложнее становятся проверки. Становится важно работать с тензорами не рассматривая их координаты. Поэтому на тензорах вводят стандартные операции, которые заведомо не зависят от выбора координат.


\section{Ранг тензора}

Нам хорошо знакомо понятие ранга билинейной формы и ранга линейного отображения. Но и билинейная форма и линейное отображение тесно связаны с понятием тензорного произведения. Можно ли обобщить понятие ранга на произвольные тензоры? Оказывается, что да. 

\dfn Пусть $U_1,\dots, U_n$ пространства и дан $f\in U_1 \otimes \dots \otimes U_n$. Рангом тензора $f$ называется наименьшее такое $r$, что $f=f_1+\dots+f_r$, где $f_i$ -- элементарный тензор.
\edfn

\zd Это определение совпадает с определением ранга для билинейных форм (как элементов $U^*\otimes V^*$) и для линейных отображений (как элементов $U^* \otimes V$).
\ezd

Наибольшую популярность понятие ранга тензора приобрело после работ Штрассена по умножению матриц. А именно, рассмотрим конкретный тензор. Пусть $V$ -- это пространство квадратных матриц размера $k$. Тогда матричное умножение задаёт на $V$ тензор валентности $(2,1)$, то есть элемент $M\in V^*\otimes V^* \otimes V$. Представим себе, что ранг этого тензора равен $r$.

Это означает, что мы можем выбрать такие элементы $f_1,\dots,f_r, g_1, \dots, g_r \in V^*$, и $v_1,\dots,v_r \in V$, что
$$M= \sum f_i\otimes g_i \otimes v_i.$$
Что это означает с точки зрения произведения матриц? Две матрицы -- это два элемента $A,B \in V$. Тогда 
$$AB=\sum f_i(A)g_i(B)v_i$$
$f_i$ и $g_i$ -- это линейные выражения от коэффициентов матриц и значит  $f_i(A)$ и $g_i(B)$ могут быть вычислены при помощи сложения чисел и умножения чисел на фиксированные скаляры. Далее, мы вычисляем произведения $P_i=f_i(A)g_i(B)$. Для этого нужны полноценные операции произведения. После этого, умножаем $P_i$ на $v_i$. Так как коэффициенты $v_i$ постоянны, то для этой операции тоже не нужно честное умножение. Итого, реально используется $r$ настоящих умножений. 

На самом деле, верно и обратное: если если мы найдём формулу для умножения матриц, которая использует только $r$ честных умножений, то можно будет построить такое разложение тензора матричного умножения. 

Пример такого умножения построил Штрассен \cite{StrassenGauss} для матриц $2\times 2$. Пусть 
$$A=\pmat A_{1,1} & A_{1,2} \\ A_{2,1} & A_{2,2} \epmat \text{ а } B=\pmat B_{1,1} & B_{1,2} \\ B_{2,1} & B_{2,2} \epmat.$$
Определим вслед за Штрассеном
$$
\begin{aligned}
&f_1=A_{1,1}+ A_{2,2} &\quad & g_1= B_{1,1}+B_{2,2} &\quad & P_1=f_1\cdot g_1\\
&f_2=A_{2,1}+A_{2,2} &\quad & g_2=B_{1,1}&\quad & P_2=f_2\cdot g_2\\
&f_3=A_{1,1}&\quad & g_3=B_{1,2}-B_{2,2}  &\quad & P_3=f_3\cdot g_3\\
&f_4=A_{2,2}&\quad & g_4=B_{2,1}-B_{1,1} &\quad & P_4=f_4\cdot g_4\\
&f_5=A_{1,1}+A_{1,2}&\quad & g_5=B_{2,2} &\quad & P_5=f_5\cdot g_5\\
&f_6=A_{2,1}-A_{1,1}&\quad & g_6=B_{1,1}+B_{1,2} &\quad & P_6=f_6\cdot g_6\\
&f_7=A_{1,2}-A_{2,2}&\quad & g_7=B_{2,1}+B_{2,2} &\quad & P_7=f_7\cdot g_7.
\end{aligned}
$$


$$
v_1= \pmat 1 & 0 \\ 0 & 1 \epmat,  \quad
v_2=\pmat 0& 0\\0 & -1\epmat,  \quad
v_3=\pmat 0 & 1 \\ 0 & -1 \epmat,  \quad
v_4=\pmat 1& 0\\1 & 0 \epmat,  
$$
$$
v_5=\pmat -1& 1\\ 0& 0\epmat,  \quad
v_6=\pmat 0&0 \\0 & 1\epmat,  \quad
v_7=\pmat 1& 0\\ 0& 0\epmat,  \quad
$$
Элементы $f_i,g_i,v_i$ задают разложение тензора матричного умножения. Если $C=AB$, то для элементов $C$ справедливы формулы
$$ 
\begin{aligned}
&C_{1,1}=P_1+P_4-P_5+P_7\\
&C_{1,2}=P_3+P_5\\
&C_{2,1}=P_2+P_4\\
&C_{2,2}=P_1-P_2+P_3+P_6
\end{aligned}
$$

Элементы $f_i,g_i,v_i$ показывают, что ранг тензора умножения матриц $2\times 2$ не больше $7$. На самом деле ранг умножения матриц $2\times 2$ действительно равен $7$.

Но как это поможет для умножения матриц больших размеров? Как обычно предположим, что $n=2^k$. Тогда матрицы $A$ и $B$ можно разбить на подматрицы размера $2^{k-1}$ которые мы обозначим как $A_{i,j}, B_{i,j}$ как и раньше. Заметим, что теперь мы снова перемножаем матрицы $2\times 2$, элементы которых на этот раз не числа, а матрицы. 

Если мы посмотрим процедуру умножения матриц, которая произошла из разложения тензора матричного умножения , то увидим, что в этой процедуре мы не использовали коммутативности умножения элементов поля. Действительно, элементы $A$ всегда находились слева в формуле по сравнению с элементами $B$, а элементы $A$ как и элементы $B$ между собой не перемножались.

Значит указанная формула справедлива и для умножения блочных матриц. Мы можем применить указанный способ рекуррентно. Тогда необходимое  число умножений в данном алгоритме будет удовлетворять соотношению:
$$T(n)=7\cdot T(n/2).$$
То есть всего мы потратим $7^{k}=n^{\log_2 7}$ умножений.

\zd Оцените количество сложений в алгоритме как $6\cdot 7^k$.
\ezd

На текущий момент при помощи оценки рангов (правда не совсем тензора матричного умножения) получены оценки $O(n^{2.373})$ операций для умножений матриц $n\times n$.

Почему мы не знаем окончательного ответа на то, какой ранг у умножения матриц размера $k\times k$? Потому что ранг тензора тяжело считать. Прежде всего тут стоит отметить, что ранг тензора, зависит от того поля, над которым мы этот ранг считаем. 

Например, ранг умножения комплексных чисел над $\R$ равен $3$ (а не $4$, как вы могли бы подумать). Но ранг тензора умножения комплексных чисел над $\C$ равен $2$ (коэффициенты тензора вещественные, но мы их  рассматриваем как элементы из $\C$).  

Так вот. Вычисление ранга тензора над $\C$ и над $\R$ является NP-трудной задачей. А вычисление ранга тензора над $\Z$, то есть когда мы хотим, чтобы все коэффициенты были целочисленными, и вовсе алгоритмически не разрешим. Это есть следствие негативного решения 10 проблемы Гильберта. Вопрос алгоритмической неразрешимости над $\Q$ остаётся открытым (как и аналог 10 проблемы Гильберта).




\section{Внешняя и симметрическая алгебры}

В этом разделе мы будем рассматривать векторное пространство $V$ над полем характеристики $0$. Есть общая теория не только для полей, но и для произвольных колец, однако, даже базовые конструкции в этой теории сложнее и некоторые её утверждения просто неверны над полем ненулевой характеристики.

\dfn Определим пространство $\Lambda^k V$ как подпространство $V^{\otimes k}$. Это подпространство выделяется следующими условиями -- для любой перестановки из $\sigma \in S_k$ и любого тензора $a\in \Lambda^k V$ верно, что $\sigma(a)=\sgn(\sigma)a$. Под $\sigma(a)$ подразумевается действие перестановки $\sigma$ на тензор $a$ перестановкой его компонент. Аналогично определяется подпространство $\Sym^k V \leq V^{\otimes^k}$, чьи элементы удовлетворяют свойству: $\sigma(a)=a$.
\edfn

\lm Имеет место проектор $Alt \colon V^{\otimes k} \to \Lambda^k V$ (называется альтернированием) 
$$a \to Alt(a)=\frac{1}{k!} \sum_{\sigma \in S_k} \sgn (\sigma) \sigma(a).$$
Аналогично отображение  симметризации
$$ a \to S(a)= \frac{1}{k!} \sum_{\sigma \in S_k} \sigma(a)$$
есть проектор на подпространство $\Sym^k V$.
\proof Докажем только первую часть. Прежде всего заметим, что $Alt$ принимает значение в подпространстве кососимметричных тензоров. Действительно 
$$\tau(Alt(a))=\frac{1}{k!}\sum_{\sigma \in S_k}\sgn(\sigma) \tau(\sigma(a))= \sgn{\tau} \frac{1}{k!}\sum_{\tau\sigma \in S_k} \sgn(\tau\sigma) \tau(\sigma(a))=\sgn(\tau) Alt(a).$$
Далее, покажем, что для любого кососимметричного тензора $a$ верно, что $Alt(a)=a$. Это покажет, что $Alt$ -- есть проектор и в его образе лежат все элементы из $\Lambda^k(V)$, то есть ровно то, что осталось показать. Итак пусть $a\in \Lambda^k(V)$. Тогда 
$$Alt(a)=\frac{1}{k!} \sum_{\sigma \in S_k} \sgn (\sigma) \sigma(a)=\frac{1}{k!} \sum_{\sigma \in S_k} \sgn^2(\sigma) a=a.$$
\endproof
\elm

\dfn Пусть $e_1,\dots, e_k$ набор элементов из $V$. Определим элементы $e_1\wedge \dots \wedge e_k \in \Lambda^k V$ как образы при проекции $e_1\otimes \dots \otimes e_k$.
\edfn

\thrm Пусть $e_1,\dots, e_n$ базис пространства $V$. Тогда элементы $e_{i_1}\wedge \dots \wedge e_{i_k}$, где $i_1<i_2< \dots < i_k$ образуют базис пространства $\Lambda^k V$. Элемент $e_{i_1}\wedge \dots \wedge e_{i_k}$, где $i_1<i_2< \dots < i_k$  обозначим за $e_{\Gamma}$, где $\Gamma =\{i_1,\dots,i_k\}\subseteq \ovl{1,n}$ подмножество размера $k$. Это взаимооднозначное соответствие. В частности, размерность $\dim \Lambda^k V = C^k_n$. 
\proof Прежде всего заметим, что это действительно порождающая система. Для этого вспомним, что $e_{i_1,\dots,i_k}$ по всем возможным наборам $i_1,\dots,i_k$ порождают тензорное произведение $V^{\otimes k}$. Но раз они порождают тензорное произведение, то они порождают и его образ при проекции $Alt$ на пространство кососимметричных тензоров. Далее заметим, что $$Alt(e_{i_1,\dots i_k})= \sgn(\sigma) Alt(e_{i_{\sigma(1)},\dots i_{\sigma(k)}}).$$
В частности, если в наборе есть два повторяющихся индекса, то элемент проектируется в 0. Далее, это же соотношение даёт, что,  с точностью до знака $Alt$ от тензора для набора $i_1,\dots,i_k$ совпадает с проекцией для упорядочевания этого набора. Таким образом, из набора образующих можно исключить неупорядоченные наборы и наборы с повторениями, что и требовалось.

\noindent Покажем линейную независимость. Пусть  
$$\sum_{\Gamma} \alpha_{\Gamma} e_{\Gamma}=0.$$
Тогда расписывая эту сумму через $e_{i_1}\otimes \dots \otimes e_{i_k}$ -- базисные элементы $V^{\otimes k}$ получаем
$$\frac{1}{k!}\sum_{\Gamma} \alpha_{\Gamma} \sum_{\sigma \in S_{\Gamma}} \sgn(\sigma) e_{\sigma(i_1),\dots,\sigma(i_k)} =0 $$
Заметим, что все слагаемые соответствуют разным базисным элементам. Тогда, все коэффициенты равны нулю. В частности, коэффициенты при $ e_{i_1, \dots, i_k}$, которые равны $\frac{1}{k!}\alpha_{\Gamma}$. 

\endproof
\ethrm

\thrm Аналогично, пусть $e_1,\dots, e_n$ базис пространства $V$. Тогда элементы образы тензоров $e_{i_1}\otimes \dots \otimes e_{i_k}$, где $i_1\leq \dots \leq i_k$ образуют базис пространства $\Sym^k V$.
\ethrm

\dfn Определим $k$-ую внешнюю степень линейного отображения $L\colon V \to W$ -- отображение $\Lambda^{k} L  \colon \Lambda^k V \to \Lambda^k W$ заданное на тензорах по правилу $v_1\wedge \dots \wedge v_k \to L v_1 \wedge \dots \wedge L v_k$. 
\edfn

Для того, чтобы показать корректность такого определения покажем следующую теорему:

\thrm Рассмотрим отображение $g=Alt \circ i \colon V^{\times k} \to \Lambda^k(V)$. Тогда для любого полилинейного кососимметричного $h \colon V^{\times k} \to U$ существует единственное отображение $\hat{h} \colon \Lambda^k(V) \to U$, что $\hat{h} \circ g = h$.
\proof Зафиксируем подходящее $h \colon V^{\times k} \to U$. Так как это полилинейное отображение, то есть линейное $\hat{\hat{h}}\colon V^{\otimes k} \to U$, что $\hat{\hat{h}} \circ i =h$. Покажем, что ограничение $\hat{\hat{h}}$ на $\Lambda^k(V)$ есть искомое отображение. Действительно $$\hat{h}(v_1\wedge\dots\wedge v_k)=\hat{\hat{h}}(v_1\wedge\dots\wedge v_k)=\hat{\hat{h}} \left(\frac{1}{k!} \sum_{\sigma \in S_k} \sgn(\sigma) v_{\sigma(1)}\otimes \dots \otimes v_{\sigma(k)} \right) =$$
$$=\frac{1}{k!} \sum_{\sigma \in S_k} \sgn(\sigma) h(v_{\sigma(1)},\dots,v_{\sigma(k)})=\frac{1}{k!}k!h(v_1,\dots,v_k)=h(v_1,\dots,v_k).$$

Осталось показать единственность. Для этого заметим, что из условия  $\hat{h}$ однозначно задано на базисе $e_{\Gamma}$.
\endproof
\ethrm


\fct Полезно смотреть не на пространства $\Lambda^k (V)$ и $\Sym^k V$, а на пространства $\Lambda^k(V^*)$ и $\Sym^k(V^*)$, потому что они допускают привычную и наглядную интерпретацию --- их элементы это полилинейные функции со специальными свойствами.
\efct


\exm \\
1) Элемент $\Lambda^2(V^*)$ --- это просто кососимметрическая билинейная форма.\\
2) А элемент $\Sym^2 V^*$ -- это симметрическая билинейная форма или просто квадратичная форма.\\
3) Элемент $\Lambda^{\dim V} V^*$ -- это просто форма объёма на $V$.\\

Мы хотим ввести умножение на кососимметричных тензорах. Есть два подхода: первый из них -- потребовать, чтобы пара $v_1 \wedge \dots \wedge v_p , u_1\wedge \dots \wedge u_q$ переходила в $v_1 \wedge \dots \wedge v_p \wedge u_1\wedge \dots \wedge u_q$. С таким подходом возникает вопрос о корректности. С другой стороны, такое умножение удобно вычислять. Второй подход -- связать это умножение с формальным умножением тензоров. Проблема возникает в следующем -- если $T_1$ и $T_2$ -- кососимметрические тензоры, то $T_1\otimes T_2$, вообще говоря, не кососимметричный. Мы доведём до конца второй путь.

\thrm[Внешняя алгебра] Рассмотрим пространство $\Lambda(V)=\oplus_{k=0}^{\dim V} \Lambda^k(V)$ и введём на нём структуру ассоциативной алгебры по правилу $ f\wedge g= Alt(f\otimes g)$. Если $f\in \Lambda^p(V)$, а $g \in \Lambda^q(V)$, то $f\wedge g=(-1)^{pq}g \wedge f$. Такое свойство называется градуированной коммутативностью. Более того, пара $v_1 \wedge \dots \wedge v_p , u_1\wedge \dots \wedge u_q$ переходит при этом умножении в $v_1 \wedge \dots \wedge v_p \wedge u_1\wedge \dots \wedge u_q$. Такое умножение называется внешним произведением тензоров.
\proof Для этого удобно проверить тождество $Alt(Alt(T_1)\otimes T_2)= Alt(T_1\otimes T_2)= Alt(T_1 \otimes Alt(T_2))$, которое говорит, что внутри альтернирования можно свободно альтернировать сомножители не боясь ничего поменять. Действительно
$$\frac{1}{k!}\sum_{\sigma \in S_{k}}\sgn(\sigma) Alt(T_1^{\sigma}\otimes T_2)=\frac{1}{k!}\sum_{\sigma \in S_{k}} \sgn^2(\sigma) Alt(T_1\otimes T_2)=Alt(T_1 \otimes T_2).$$
Аналогично получается второе равенство. Теперь видно, что 
$$Alt(v_1 \wedge \dots \wedge v_p \otimes u_1\wedge \dots \wedge u_q)=Alt(v_1 \otimes \dots \otimes v_p \otimes u_1\wedge \dots \wedge u_q)=$$
$$=Alt( v_1 \otimes \dots \otimes v_p \otimes u_1\otimes \dots \otimes u_q) =v_1 \wedge \dots \wedge v_p  \wedge u_1\wedge \dots \wedge u_q .$$
Это показывает связь нашего определения умножения с ожидаемым определением. Ассоциативность теперь легко проверить на базисных элементах, как и градуированную коммутативность.
\endproof
\ethrm





\thrm[Симметрическая алгебра] Рассмотрим пространство $\Sym(V)=\oplus_k \Sym^k(V)$. Тогда на нём можно ввести структуру ассоциативной коммутативной алгебры задав умножение как $ f*g= S(f\otimes g)$. Более того, указанная алгебра изоморфна алгебре многочленов.
\ethrm



Изначально, внешняя алгебра была нужна для <<исчисления подпространств>> в пространстве $V$. А именно, подпространству $U\leq V$ размерности $k$ можно сопоставить прямую $\Lambda^k U$ в $\Lambda^k V$. Более того, задав на $U$ форму объёма можно выбрать на этой прямой определённую точку. Такие объекты теперь можно перемножать и складывать, хотя в общем случае может получиться и объект, не соответствующий никакому подпространству. 

Сейчас понятие внешней алгебры может служить удобным формализмом для определения интегрирования. А именно, по многообразию размерности $n$ можно проинтегрировать кососимметричное тензорное поле валентности $(n,0)$, то есть заданную в каждой точке многообразия форму объёма на касательном пространстве. В общем же виде кососимметричные тензорные поля типа $(k,0)$ называются дифференциальными формами порядка $k$. На таких дифференциальных формах задана операция взятия дифференциала, делающая из $k$-формы $k+1$-форму. Это ещё одна из канонических <<бескоординатных>> операций. Об этом вам расскажут в курсе анализа.

\subsection{Внешняя алгебра и определитель}

Покажем способ применения внешней степени для доказательства тождеств про определители. Введём обозначение.

\dfn Пусть $A$ -- матрица из $M_{m\times n}(K)$. Тогда если $\Gamma \subseteq \{1,\dots,n\}$. Тогда за $A_{\Gamma}$ обозначим матрицу состоящую из столбцов матрицы $A$ с элементами из $\Gamma$. Аналогично, если $\Gamma \subseteq \{1,\dots,m\}$ то за $A^{\Gamma}$ обозначим подматрицу $A$, из строк, чьи индексы лежат в $\Gamma$.
\edfn

Посмотрим, как устроены координаты внешнего произведения произвольного набора векторов. Пусть $e_1,\dots,e_n$ -- базис $V$, $v_1,\dots,v_k$ --  произвольный набор векторов. Пусть $A$ -- это матрица координат векторов $v_1,\dots,v_k$.
Рассмотрим $v_1 \wedge \dots \wedge v_k$. Это полилинейная кососимметрическая функция от $v_1,\dots,v_k$, а вместе с ней и координаты этого произведения. Ясно, что координаты при $e_{\Gamma}=e_{i_1}\wedge\dots\wedge e_{i_k}$ зависят только от строк матрицы $A$ с номерами из $\Gamma$. Если в этих строках стоит единичная матрица, то ясно, что координата при $e_{\Gamma}$ -- это единица. Вспоминая, что есть определитель, получаем, что в общем виде коэффициент при $e_{\Gamma}$ это определитель $A^{\Gamma}$. Итого 
$$v_1 \wedge \dots \wedge v_k= \sum_{\substack{\Gamma \subseteq \{1,\dots,n\}\\ |\Gamma|=k}} \det A^{\Gamma} e_{\Gamma}.$$

Несложно получить эту формулу непосредственно.

Пусть есть отображение $L \colon U \to V$, где $\dim U= \dim V = n$ и $A$ матрица $L$ в базисах $e_1,\dots e_n$ и $f_1,\dots,f_n$. Тогда $$\Lambda^n L(e_1\wedge \dots \wedge e_n) = \det A \,\,f_1 \wedge \dots \wedge f_n.$$
Из этого замечания уже легко получить мультипликативность определителя. Поступая аналогично можно доказать более общую теорему:



\thrm[Формула Бине-Коши] Рассмотрим две матрицы $A\in M_{m\times n}(K)$ и $B\in M_{n\times m}(K)$. Пусть $m\leq n$. Тогда
$$\det(AB)=\sum_{\substack{\Gamma \subseteq \{1,\dots,n\}\\ |\Gamma|=m}} \det A_{\Gamma} \det B^{\Gamma}.$$
\proof Рассмотрим линейные отображения, заданные матрицами $A\colon K^n \to K^m$ и  $B \colon K^m \to K^n$. Тогда $\Lambda^m (AB)$ есть оператор домножения на $\det AB$. С другой стороны $\Lambda^m(AB)=\Lambda^m(A) \Lambda^m(B)$. Вычислим матрицы этих отображений. Матрица $\Lambda^m(B)$ есть столбец высоты $C_n^m$, чьи элементы проиндексированы $\Gamma \subseteq \{1,\dots,n\}$ размера $m$. Аналогично матрица $\Lambda^m(A)$  есть строчка, чьи элементы проиндексированы аналогично.
Пусть $e_1,\dots,e_m$ -- стандартный базис $K^m$, $e=e_1\wedge \dots \wedge e_m$ единственный базисный элемент $\Lambda^m(K)$, а $f_1,\dots,f_n$ -- стандартный базис $K^n$. Тогда 
$$\Lambda^m(B)(e)=\sum_{\substack{\Gamma \subseteq \ovl{1,n} \\ |\Gamma|=m}} \det B^{\Gamma} f_{\Gamma}.$$
 С другой стороны, 
$$\Lambda^m(A)(f_{\Gamma})=\det(A_{\Gamma})e.$$
Осталось перемножить строчку на столбец.
\endproof
\ethrm

Покажем, как можно использовать формулу Бине-Коши. Вначале докажем общие факты про графы.

\dfn Определим матрицу оператора Лапласа для графа $G$ как $L(G)=D-A(G)$.
Ориентируем произвольным образом рёбра графа $G$. Тогда определим ориентированную матрицу инцидентности, как матрицу, строки которой соответствуют вершинам $G$, столбцы -- рёбрам $G$. В столбце соответствующем ребру стоит $1$ в позиции, конца этого ребра и $-1$ в позиции начала. 
\edfn

\lm Выполнено равенство $L(G)=B^\top B$ при любой ориентации рёбер графа.
\elm

Это сразу говорит нам, что матрица $L(G)$ неотрицательно определена. Однако у матрицы $L(G)$ всегда нетривиальное ядро. Действительно, столбец $(1,\dots,1)^\top$ лежит в ядре $L(G)$. Можно сказать точнее:

\lm Пусть у графа $G$ ровно $k$ компонент связности. Тогда $\dim \Ker L(G)\geq k$. На самом деле, выполнено равенство.   
\elm

\crl Если граф $G$ не связен, то алгебраическое дополнение любого элемента в матрице $L(G)$ равно $0$.
\ecrl

\lm Пусть граф $G$ -- дерево. Тогда алгебраическое дополнение любого элемента матрицы $L(G)$ равно 1.
\elm
\proof Пусть нам дан элемент с индексами $i,j$ в $L(G)$. Если $B$ -- ориентированная матрица инцидентности графа $G$, то этот определитель вычисляется как произведение  $\det B^{\ovl{i}}$ и $\det B^{\ovl{j}}$. Сведём вычисление каждого из этих определителей к ситуации, когда граф $G$ -- путь, а выкинута первая строка. Для того, чтобы разобраться с номером вершины просто поставим выкидываемую строчку на первое место. Теперь заметим, что оба определителя не меняются при элементарных преобразованиях столбцов $B$.
Теперь опишем процедуру <<выпрямления>> дерева на языке элементарных преобразований. 
\endproof

\crl[Теорема о деревьях] Количество остовных деревьев в графе $G$ есть алгебраическое дополнение любого элемента матрица $L(G)$.
\ecrl
\proof Прежде всего отметим, что если $G$ не связен, то и любой минор $L(G)$ равен $0$. $L(G)=BB^\top$, где $B$ -- ориентированная матрица смежности графа $G$. Найти какое-то алгебраическое дополнение $L$ -- это то же самое, что найти определитель $\hat{B}\hat{B}^\top$, где $\hat{B}$ -- матрица $B$ с одной выкинутой строкой. Определитель $\hat B \hat{B}^\top$ легко считается по формуле Бине-Коши. В этой формуле мы должны выбрать $\hat{B_{\Gamma}}$, то есть $n-1$ строк $\hat{B}$ и найти
$$(\det \hat{B}_{\Gamma})^2=\det(\hat{B}_{\Gamma} \hat{B}_{\Gamma}^\top)$$
Но $\det \hat{B}_{\Gamma} \hat{B}_{\Gamma}^\top$ -- это минор матрицы Лапласа графа, чьи рёбра соответствуют строкам матрицы $B$. Если этот граф  -- не дерево, то он не связен и этот определитель равен $0$. Если это дерево, то оно остовное и этот определитель равен единице. Так как по формуле Бине-Коши для вычисления определителя надо просуммировать такие слагаемые, то мы получаем ровно количество остовных деревьев.
\endproof


\crl[Формула Кэли] Количество различных помеченных деревьев на $n$ вершинах равно $n^{n-2}$.
\ecrl



